[{"authors":null,"categories":null,"content":"I’m Yuma Uchiumi (@yumaloop), a graduate student majoring in computer science. I was born on December 8, 1997 in Tokyo, Japan. My research goal is to understand and implement the computation for human thinking; I study statistical algorithms, stochastic models, and information processing systems to solve the cognitive problems with human thinking.\n  Download my resumé.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I’m Yuma Uchiumi (@yumaloop), a graduate student majoring in computer science. I was born on December 8, 1997 in Tokyo, Japan. My research goal is to understand and implement the computation for human thinking; I study statistical algorithms, stochastic models, and information processing systems to solve the cognitive problems with human thinking.","tags":null,"title":"Yuma Uchiumi","type":"authors"},{"authors":null,"categories":["Personal"],"content":"StackOverflow published its Developer Survey 2020. Click here for details.\nIn 2020, Python, Go, TypeScript, Rust are the programming language most loved by developers in the world. It means that these languages are in vogue and new learners are on the rise. While JavaScript swallows everything, Perl, PHP, and Ruby seem to be still in high demand.\nFrameworks packaging frontends, backends and database into one, such as Rails, are already not preferred. Today, dividing frontends with JavaScript (Node.js, Vue.js, React.js) and backends/database with scalable cloud services (AWS, GCP) is major approach for adaptive web applications.\nPython and Java have always been very popular and in demand as general languages that can be used for many purposes. In the future, it will be interesting to see if Go and Rust can take their place.\nFinally, I would like to introduce the cluster map of major development technologies found by the StackOverflow Developer Survey 2020. With the penetration of new technologies such as mobile, container and cloud computing, it will be more difficult to become a full-stack engineer in the near future.\n Assembly, C, C++ Raspberry Pi, Arduino Unity, UnrealEngine Hadoop, Scala, Apache Spark Python, Pandas, Torch/PyTorch Linux, Docker, Kubernetes, Bash/Shell AWS, Redis, Ansible, DynamoDB, PostgreSQL JavaScript, Node.js, React.js, Angular, TypeScript, MongoDB, PHP, MySQL, jQuery, WordPress Java, Swift, Android, iOS, Kotlin, SQLite, Firebase C#, .NET, Windows, Azure  ","date":1608422400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608422400,"objectID":"0e7eb9f6d1f17ebdc4c693cc3bea4796","permalink":"https://yumaloop.github.io/post/2020-12-20-stackoverflow-developer-survey-2020/","publishdate":"2020-12-20T00:00:00Z","relpermalink":"/post/2020-12-20-stackoverflow-developer-survey-2020/","section":"post","summary":"StackOverflow published its Developer Survey 2020. Click here for details.\nIn 2020, Python, Go, TypeScript, Rust are the programming language most loved by developers in the world. It means that these languages are in vogue and new learners are on the rise.","tags":["Tips","Python","Go","Java","React"],"title":"StackOverflow - 2020 Developer Survey","type":"post"},{"authors":null,"categories":["Finance"],"content":"幾何ブラウン運動のサンプルパスを生成することにより，株価が幾何ブラウン運動にしたがう場合のコールオプション価格をモンテカルロシミュレーションにより導出し，サンプルパスの生成回数を増やすことで理論値に収束することをプロットにより確認する．\n1. コールオプションの価格理論 ブラックショールズ式によるオプション価格の導出 満期$T$で原資産価格(株式価格)が連続時間確率過程$S = {(S_t)}_{t \\in [0,T]}$に従うオプションの時刻$t \\in [0, T]$における価格$C(t, S_t)$について考える．$S$が確率微分方程式:\n$$ \\begin{align} d S_t = \\sigma S_t dt + \\mu S_t d W_t \\end{align} $$\nの解で与えられているとする($S$は幾何ブラウン運動に従う)．時刻$t \\in [0, T]$において，オプションの原資産価格$S_t$とオプションの行使価格$K$が与えられたとき，ブラック・ショールズ式によってオプションの理論価格$C(t, S_t)$は\n$$ \\begin{align} C(t, S_t) \u0026amp;= S_t \\Phi(d_1) - K e^{-r(T-t)} \\Phi(d_2) \\\\\nwhere ~~ d_1 \u0026amp;= \\frac{\\log \\left( \\frac{S_t}{K} \\right) + \\left( r + \\frac{\\sigma^2}{2} \\right) T}{\\sigma \\sqrt{T}} \\\\\nd_2 \u0026amp;= \\frac{\\log \\left( \\frac{S_t}{K} \\right) + \\left( r - \\frac{\\sigma^2}{2} \\right) T}{\\sigma \\sqrt{T}} \\end{align} $$\nとなる．ただし，$r$は無リスク資産の利率，$\\Phi$は標準正規分布$\\mathcal{N}(0,1)の累積分布関数とする．ここで，簡単のために満期$T$を$1$とすると，現在($t=0$)のオプション価格の理論値は，\n$$ \\begin{align} C(0, S_0) \u0026amp;= S_0 \\Phi(d_1) - K e^{-r} \\Phi(d_2) \\\\\nwhere ~~ d_1 \u0026amp;= \\frac{\\log \\left( \\frac{S_0}{K} \\right) + \\left( r + \\frac{\\sigma^2}{2} \\right)}{\\sigma } \\\\\nd_2 \u0026amp;= \\frac{\\log \\left( \\frac{S_0}{K} \\right) + \\left( r - \\frac{\\sigma^2}{2} \\right)}{\\sigma} \\end{align} $$\nとなる．\nモンテカルロ法によるオプション価格の導出 ブラックショールズ式に含まれる$S_T$の期待値計算をモンテカルロ法によって近似することを考える．すなわち，幾何ブラウン運動に従うサンプルパス$S = {(S_t)}_{t \\in [0,T]}$を大量に生成することで，$S_T$の期待値を求める．\nモンテカルロシミュレーションによって生成された，満期$T$における原資産価格$S_T$の$n$個のサンプルを$(s^{(1)}{T}, \\cdots, s^{(n)}{T})$とすると，時刻$t \\in [0, T]$におけるオプションの価格の推定値$\\hat{C}(t, S_t)$は\n$$ \\begin{align} \\hat{C}(t, S_t) \u0026amp;= \\frac{1}{n} \\sum_{i=1}^{n} e^{-r(T-t)} \\cdot max(s^{(i)}_{T} - K, 0) \\end{align} $$\nと求められる．ここで，簡単のために満期$T$を$1$とすると，現在($t=0$)のオプション価格の推定値は，\n$$ \\begin{align} \\hat{C}(0, S_0) \u0026amp;= \\frac{1}{n} \\sum_{i=1}^{n} e^{-r} \\cdot max(s^{(i)}_{1} - K, 0) \\end{align} $$\nとなる．\n2. Rでモンテカルロシミュレーション YUIMAライブラリをインストール（or 読み込み）する．\ninstall.packages(\u0026quot;yuima\u0026quot;) # インストール (初回のみ) library(yuima) # ライブラリの読み込み  モンテカルロシミュレーションを実行するコード．\n# Calculation of call-option prices by Black-Sholes eq. BlackScholesCallPrice = function(S, K, r, sigma, T=1) { d1 \u0026lt;- ( log(S/K) + (r + sigma^2/2) * T)/( sigma * sqrt(T)) d2 \u0026lt;- ( log(S/K) + (r - sigma^2/2) * T)/( sigma * sqrt(T)) C0 \u0026lt;- S * pnorm(d1) - K * exp(-r * T) * pnorm(d2) return(C0) } # Calculation of call-option prices by Monte Carlo method MonteCarloCallPrice = function(S, K, r, sigma, n, T=1) { n_sample \u0026lt;- 1000 c0_list \u0026lt;- list() c0 \u0026lt;- 0 for (i in 1:n) { resultGBM \u0026lt;- GBM_sample(S, r, sigma, n_sample) sT \u0026lt;- resultGBM@data@original.data[n_sample] c0 \u0026lt;- (1/i) * exp(-1*r*T) * max(sT - K, 0) + ((i-1)/i) * c0 c0_list \u0026lt;- append(c0_list, list(c0)) } return(c0_list) } # A function which generates sample paths that follows a Geometric Brownian motion GBM_sample = function(x0, alpha, beta, n_sample, T=1) { # Step1: Define SDE # dS_t = alpha * S_t * dt + beta * S_t * dW_t mod \u0026lt;-setModel(drift=\u0026quot;alpha*x\u0026quot;, diffusion=\u0026quot;beta*x\u0026quot;) # Step2: Define samples samp \u0026lt;-setSampling(Initial=0, Terminal=T, n=n_sample) # Step3: define the statistical model smod \u0026lt;-setYuima(model=mod, sampling=samp) # Step4: Generate sample paths xinit \u0026lt;- x0 param \u0026lt;- list(alpha=alpha, beta=beta) resultGBM \u0026lt;- simulate(smod, xinit=xinit, true.parameter=param) return(resultGBM) } # Params for call-option pricing n \u0026lt;- 10000 # Num. of MonteCalro simulation K \u0026lt;- 900 # Option exercise price (at t=T) S \u0026lt;- 1000 # Curent asset price (at t=0) r \u0026lt;- 0.005 # Drift for Geometric Brownian motion sigma \u0026lt;- 0.3 # Diffusion for Geometric Brownian motion T \u0026lt;- 1 # Optional term # Main procedure: Run simulation bs_price \u0026lt;- BlackScholesCallPrice(S, K, r, sigma, T=1) mc_price \u0026lt;- MonteCarloCallPrice(S, K, r, sigma, n, T=1) print(bs_price) plot(1:n, mc_price, main=\u0026quot;Monte Carlo Simulation:\\nBlack-Scholes Option Pricing Model\u0026quot;, xlab=\u0026quot;Number of sample paths: # of ST\u0026quot;, ylab=\u0026quot;Option Price: C0\u0026quot;, cex=0.5) abline(h=bs_price, col='red', lwd=1, lty=2)  Case 1.\nモンテカルロ法によるコールオプションの推定価格が， BS式による理論価格172.7457に漸近している．\n パラメータ設定  n: 10000 # モンテカルロシミュレーションの回数 K: 900 # コールオプションの権利行使価格 (t=T) S: 1000 # 株式の現在価格 (t=0) r: 0.005 # 幾何ブラウン運動のdrift sigma: 0.3 # 幾何ブラウン運動のdiffusion T: 1 # オプションの満期    Case 2.\nモンテカルロ法によるコールオプションの推定価格が， BS式による理論価格83.1821に漸近している．\n パラメータ設定  n: 10000 # モンテカルロシミュレーションの回数 K: 1100 # コールオプションの権利行使価格 (t=T) S: 1000 # 株式の現在価格 (t=0) r: 0.005 # 幾何ブラウン運動のdrift sigma: 0.3 # 幾何ブラウン運動のdiffusion T: 1 # オプションの満期    ","date":1606262400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606262400,"objectID":"177189947e741f52fd29bb2a328c1a29","permalink":"https://yumaloop.github.io/post/2020-11-25-monte-carlo-sim-bsoption-r/","publishdate":"2020-11-25T00:00:00Z","relpermalink":"/post/2020-11-25-monte-carlo-sim-bsoption-r/","section":"post","summary":"幾何ブラウン運動のサンプルパスを生成することにより，株価が幾何ブラウン運動にしたがう場合のコールオプション価格をモンテカルロシミュレーションにより導出し，サンプルパスの生成回数を増やすことで理論値に収束することをプロットにより確認する．","tags":["Tips","R"],"title":"Monte Carlo Simulation of Black-Scholes Option Pricing Model","type":"post"},{"authors":null,"categories":["Personal"],"content":"   Simon Wood, a leading statistician in UK being famous for the text book of the GAMs introduced the lockdown reading list in his homepage. It looks interesting, so I\u0026rsquo;ll share it here. (most of them are Blackwell\u0026rsquo;s, books)\n Collapse (Jared Diamond) on how societies are destroyed, not by external forces, but by their failure to adapt their cultural norms to those forces. Thinking Fast and Slow (Daniel Kahneman) on the pitfalls of our intuitive reasoning, especially about risk and uncertainty. Mistakes were made, but not by me (Carol Tarvis and Elliot Aronson) on the psychology of sticking with bad decisions. The Parable of the Old Man and the Young by Wilfred Owen, on consequences of the above. Economics The User\u0026rsquo;s Guide (Ha-Joon Chang) on what you really need to know about economics, and how it isn\u0026rsquo;t just a scaled up version of household accounting. The Great Crash 1929 (Galbraith) a delightful disection of economic hubris (and the need for stabilizing controls that we long since did away with). The Rise and Fall of the Third Reich (William Shirer) detailing exactly how things went wrong in Germany after the Great Depression. Witch hunting in Scotland (Brian Levack) on the Scottish experience of the great European witchcraft panic (James I/VI wrote a treatise on Witchcraft). Wood and Thomas paper on the problems of prediction with disease models in the absense of direct validation data (the least impressive item here).  ","date":1593475200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593475200,"objectID":"93cba2be93672db8c2b1de0e9060b5a1","permalink":"https://yumaloop.github.io/post/2020-06-30-sw-lockdown-reading-list/","publishdate":"2020-06-30T00:00:00Z","relpermalink":"/post/2020-06-30-sw-lockdown-reading-list/","section":"post","summary":"Simon Wood, a leading statistician in UK being famous for the text book of the GAMs introduced the lockdown reading list in his homepage. It looks interesting, so I\u0026rsquo;ll share it here.","tags":["Tips","Books"],"title":"Simon N Wood's \"lockdown reading list\"","type":"post"},{"authors":null,"categories":["Finance"],"content":"   Self-discriptive system within time change.\nThe simplest example of self-discriptive systems is the exponential increase. This is because the simplest \u0026ldquo;change\u0026rdquo; of a variable $x$ is a first derivative of its time $dx/dt$, and the simplest form of the function \u0026ldquo;$f(x)$\u0026rdquo; determined by a variable $x$ is a linear one $Cx$.\n Case1: Growth is constant with the current state (state is invariant over time)  $$ \\frac{dx}{dt} = C $$\n Case2: Growth is linear with the current state (state changes exponentially)  $$ \\frac{dx}{dt} = Cx $$\n Case3: Growth is complex but relative to the current state (generalization)  $$ \\frac{dx}{dt} = f(x) $$\nActual situations 1. Money/Capital creates new money/capital\nThis is the principle of capital markets and investment. Or what is called Capital Gain. Source of motivation on the lender side.\nこれは資本市場と投資の原理．いわゆるキャピタルゲイン．貸し手の動機の源泉．\n2. Trust/Credit creates new trust/credit\nThe most obvious example is banking. Actually, there is a word of credit creation. Lending is gradually increased based on credit. This is the principle of the borrower.\nわかりやすい例は銀行業． 実際，信用創造という言葉があるくらい．信用をもとに，少しずつ貸出を増やす．これは借り手の原理．\nThe modern financial system is supported by the principle that capital and credit increase/decrease exponentially. This principle creates a dynamic phenomenon (spiral) that motivates lenders and borrowers and that \u0026ldquo;lending and borrowing exponentially increases/decreases.\u0026rdquo; This is inflation and deflation.\n現代金融システムは，「資本と信用が指数増加/減少する」という原理によって支えられる．この原理は，貸し手と借り手に動機を与え「指数増加/減少する貸金と借金」という現象（スパイラル）を生む．これがインフレとデフレ．\nA vested interest (a large corporation or a large political party) can be established as a vested interest by reproducing credit and achievement in the future by accumulating past credit and achievement.\n既得権益（大企業や大政党）は，過去の信用と実績の積み重ねにより，将来の信用と実績を再生産することで，既得権益として成立する．\nTo make matters difficult, \u0026ldquo;trust\u0026rdquo; and \u0026ldquo;achievement\u0026rdquo; are complementary. \u0026ldquo;Trust\u0026rdquo; provides an opportunity to unlock new achievements. The new “achievement” repairs and strengthens trust. In other words, vested interests are invincible unless environmental changes occur.\n厄介なことに「信頼」と「実績」は相補関係にある．「信頼」は新たな実績解除への機会を提供する．新たな「実績」は信頼を補修・強化する．つまり，既得権益は環境変化が起きない限り無敵．\nThe first step for new powers (ventures and youth) to scale up is to win trust or to make some achievements. In addition, it is even better as it causes environmental changes and innovation.\n新興勢力（ベンチャーや若者）がスケールアップするための第一歩は「信頼」を勝ち取るか，「実績」で黙らせるか． 加えて，環境変化やイノベーションを起こすとなお良い．\nAs a result, The risk-loving youths first try to walk the cycle from the trust to the achievement, but The risk-averse youths first try to walk the cycle from the achievement to the trust.\n結果として， リスク選好的な若者は，信頼から実績へというサイクルを歩もうとし， リスク回避的な若者は，実績から信頼へというサイクルを歩もうとする．\n4. Follower/Fan creates new follower/fan\nIn order for super popular corporate brands, entertainers, celebrities, and influencers to be born, their fans/followers need to create new fans/followers.\n超人気な企業ブランドや芸能人，セレブ，インフルエンサーが生まれるためには，彼らのファン/フォロワーが新たなファン/フォロワーを生む必要がある．\nNowadays, this mechanism has been strengthened in the entertainment industry where globalization by the Internet has been achieved, and in platforms where search and recommendation algorithms that strongly fit past trends are dominant.\n近年，インターネットによるグローバル化が達成されたエンタメ産業や，過去の傾向に強くfitする検索や推薦のアルゴリズムが支配的なプラットホームにおいて，このメカニズムは強化されている．\nThe more followers/fans there are, the more followers/fans there are. The services with more registrants/subscribers tend to have more registrants/subscribers.\nフォロワー/ファンが多い人ほど，フォロワー/ファンが増えやすい． 登録者/購読者の多いサービスほど，登録者/購読者が増えやすい．\n3. Popularity/Evaluation creates new popularity/evaluation\nFor products that sell well, the \u0026ldquo;selling\u0026rdquo; state itself becomes valuable.\n売れているものは，売れているということ自体が価値になる．\nThis phenomenon is often explained by René Girard\u0026rsquo;s triangular desire from the perspective of consumer sentiment, and by the network externality from the perspective of industrial organization theory.\nこの現象は，消費者心理の観点からはルネ・ジラールの欲望の三角形，産業組織論の観点からはネットワーク外部性でよく説明される．\nAdditionally, this schema is often misused for stealth marketing, hype, information products, affiliates, etc.\nまた，このスキーマは，しばしばステマ，偽客(サクラ)，誇大広告，情報商材，アフィリエイトなどで悪用される．\n","date":1591833600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591833600,"objectID":"9701ee598cf0a4fa23043c14d9af8bf3","permalink":"https://yumaloop.github.io/post/2020-06-11-recurrent-structure-in-capita/","publishdate":"2020-06-11T00:00:00Z","relpermalink":"/post/2020-06-11-recurrent-structure-in-capita/","section":"post","summary":"Self-discriptive system within time change. The simplest example of self-discriptive systems is the exponential increase. This is because the simplest \u0026ldquo;change\u0026rdquo; of a variable $x$ is a first derivative of","tags":["System","Capitalism"],"title":"Recursion of the Capital","type":"post"},{"authors":null,"categories":["Finance","Random"],"content":"中国のTencentが日本のACG(Anime, Comics, Game)産業へ大きな関心を寄せているらしい． すなわち，コンソールゲームの制作とヒットシリーズのフランチャイズに関する日本の専門知識を吸収しながら， いくつかのスタジオを買収し，潜在的な投資について交渉中とのこと．詳細は下記記事を参照．\nTencent Targets Japan Anime, Manga to Jump-Start Global Growth (Bloomberg, June 9, 2020, 5:00 PM EDT)\n","date":1591747200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591747200,"objectID":"5a7dc29a1a65dddce9f0a2b4bf0e8bcd","permalink":"https://yumaloop.github.io/post/2020-06-10-tencent-acg/","publishdate":"2020-06-10T00:00:00Z","relpermalink":"/post/2020-06-10-tencent-acg/","section":"post","summary":"中国のTencentが日本のACG(Anime, Comics, Game)産業へ大きな関心を寄せている．","tags":["Tips"],"title":"Tencent's Strategy for the ACG and the Impact on Japan","type":"post"},{"authors":null,"categories":["Finance"],"content":"2020年05月現在，コロナショックで，景気後退が予測されている． S\u0026amp;P 500 YTDをみると，2018年・2019年と比較しても低調だ．\nアメリカ株は予想に反して高い水準を維持しているが，6月以降の統計結果によっては落ち込む可能性も高い．このポストでは，アメリカのマクロ経済史を振り返り,2021年以降のアメリカ経済について考えてみたい．\nFig: S\u0026amp;P 500 Index YTD Daily Performance Source: https://www.macrotrends.net/2490/sp-500-ytd-performance\nGDP USの実質GDP年間推移(1947 - 2020)をみてみる．インフレ率を補正すると，ほぼ線形に増加していることがわかる．\nFig: Real GDP, Billions of Chained 2012 Dollars, Annual Rate (1947 - 2020) Source: https://alfred.stlouisfed.org/series?seid=GDPC1\n次に，成長率(1次微分)．USの四半期ごとのGDP成長率(YOY)を，戦後(1947-2020)に限定してみてみる．基本的にランダムウォークにみえるが，興味深いのは1985年ごろを境に，分散が小さくなっていることだ．\nFig: United States GDP Growth Rate, YOY (1947 - 2020) Source: https://tradingeconomics.com/united-states/gdp-growth\nFREDのサイトにある，年間のGDP成長率(YOY)も載せておく．\nFig: Real GDP, Percent Change from Preceding Period (1930 - 2020) Source: https://fred.stlouisfed.org/graph/?g=oM2u\nEquity Market 米国株式市場の歴史．まず事実として，USの株式市場は長期的に上昇トレンドである．また，S\u0026amp;P 500とDJIAは長期間でみると同じ挙動を示す．\nFig: S\u0026amp;P 500 Index (1928.01 - 2020.06) Source: https://www.macrotrends.net/2324/sp-500-historical-chart-data\nFig: Dow Jones Industrial Average (DJIA) (1915.02 - 2020.06) Source: https://www.macrotrends.net/2324/sp-500-historical-chart-data\nFig: NASDAQ Composite (1971.02 - 2020.06) Source: https://www.macrotrends.net/1320/nasdaq-historical-chart\nそして，狭い意味での効率的市場仮説を支持するならば，短期(年あるいは月単位)での景気循環は平滑化される．また事実として，あらゆる経済統計がせいぜい100年分しか存在しない以上，景気循環説や効率的市場仮説は正しく検証できない．\nS\u0026amp;P 500は，NYSEとNSDAQの上場銘柄から(流動性の高い大型株の)時価総額を指数化したものなので，DJIAより実体経済(企業部門の利益)を反映している(と言われている)．\nここで，S\u0026amp;P 500の過去10年間(2010 - 2020)の推移をみてみる．\nFig: S\u0026amp;P 500 Index, Daily Close (2010.06.15 - 2020.06.12) Source: https://fred.stlouisfed.org/graph/fredgraph.png?g=rtxI,\nFig: S\u0026amp;P 500 Index Change from Year Ago (2011.06.13 - 2020.06.12) Source: https://fred.stlouisfed.org/graph/fredgraph.png?g=rtyH\nS\u0026amp;P 500の日別の変化量(YOY)．マイナスとなった期間は2015/08~2016/07，2018/11~2019/05，2020/03~05の3回である．\nFig: S\u0026amp;P 500 Index Change, Daily Close (2010.06.15 - 2020.06.12) Source: https://fred.stlouisfed.org/graph/fredgraph.png?g=rtyj\nS\u0026amp;P 500の日別の変化量(DOD)．\nEconomic Policy 株価(cf. S\u0026amp;P 500)は，マクロ経済(GDP)に対する先行指標であると同時に，政府が経済対策を決定するための原因となる（結果ではない）．S\u0026amp;P 500の歴史と，その結果として時の米国政府がどのような経済政策を実施したか，その政策は具体的にどのようなマクロ経済理論によって裏付け(アドバイス)されたか，を考えてみる．\nS\u0026amp;P 500の前年比変動率（1929-2020/03）をみてみよう．\nFig: S\u0026amp;P 500 Historical Annual Returns (1928 - 2020) Source: https://www.macrotrends.net/2526/sp-500-historical-annual-returns\nS\u0026amp;P 500の統計が開始されてから，1929-2019年のちょうど100年間で，年換算で前年比マイナスとなった年は22回しかない．そして，2020年は23番目の年になるかもしれない．全22回をリストアップしてみる．\n 統計上の留意点：S\u0026amp;P 500指数の発行元であるS\u0026amp;P Global社の沿革:\n1941年，スタンダード統計社(Standard Statistics Bureau)とプアー出版社(H.V. and H.W. Poor Co.)が合併してS\u0026amp;P社(Standard\u0026amp;Poor\u0026rsquo;s)が誕生．1957年，S\u0026amp;P 500が誕生．1966年，マグロウヒル社がS\u0026amp;P社を買収し，現在の運営体制 (S\u0026amp;P Global Inc.; 1995年までの旧名:The McGraw-Hill Companies)となった．\n Great Depression (1929-1936, 7 years) ケインズ，ニューディール 政策 有効需要\n 1929: (-11.91% YoY) 世界恐慌 1930: (-28.48% YoY) 世界恐慌 1931: (-47.07% YoY) 世界恐慌 1932: (-15.15% YoY) 世界恐慌 1934: (-05.94% YoY) 世界恐慌  World War Ⅱ (1937-1945, 8 years)  1937: (-38.59% YoY) 第二次世界大戦 1939: (-05.45% YoY) 第二次世界大戦 1940: (-15.29% YoY) 第二次世界大戦 1941: (-17.86% YoY) 第二次世界大戦  Post-war Prosperity (1945-1973, 28 years)  60s後半-70s前半のスタグフレーションの結果として，1972,73: ニクソンショック(ブレトン・ウッズ協定崩壊)\n  1946: (-11.87% YoY) 戦後 1953: (-06.62% YoY) 1957: (-14.31% YoY) 1960: (-02.97% YoY) 1962: (-11.81% YoY) 1966: (-13.09% YoY) 1969: (-11.36% YoY)  Reaganomics (1974-1990, 16 years)  新自由主義，エネルギー規制の緩和，レーガノミクス(所得減税)，プラザ合意，双子の赤字(財政赤字と貿易赤字の拡大)\n  1973: (-17.37% YoY) 第一次オイルショック 1974: (-29.72% YoY) 第一次オイルショック 1977: (-11.50% YoY) 1981: (-09.73% YoY) 第二次オイルショック  New Economy (1990-2000, 10 years)  情報通信業の牽引，ドットコムバブル，90sのUSGDPは69%増，S\u0026amp;P 500は3倍に上昇\n  1990: (-06.56% YoY) 通貨危機 2000: (-10.14% YoY) ドットコムバブル崩壊  Financial Crisis (2001-2009, 8 years)  2001年の同時多発テロ，2001-2007の住宅バブルとサブプライムローンによる金融危機，ミンスキーの金融不安定仮説，\n  2001: (-13.04% YoY) ドットコムバブル崩壊 2002: (-23.37% YoY) ドットコムバブル崩壊 2009: (-38.49% YoY) リーマンショック  Tech boom (2010-2019, 10years)  西海岸テック株の牽引，中国経済の台頭\n  2018: (-06.42% YoY) 上海危機  New Normal? (2020-????)  2020: (-3.34% YTD) コロナショック  References \u0026amp; Source  www.macrotrends.net - S\u0026amp;P 500, historical annual returns www.macrotrends.net - DJIA, 100 years historical chart us.spindices.com - S\u0026amp;P 500 us.spindices.com - Dow Jones Industrial Average tradingeconomics.com - United States GDP Growth Rate tradingeconomics.com - S\u0026amp;P 500 tradingeconomics.com - Dow Jones Industrial Average fred.stlouisfed.org - Real Gross Domestic Product fred.stlouisfed.org - S\u0026amp;P 500 Index fred.stlouisfed.org - Dow Jones Industrial Average Wikipedia - Economic history of the United States Wikipedia - List of recessions in the United States Wikipedia - List of economic expansions in the United States  ","date":1590710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590710400,"objectID":"46be1c3180b33e0ee552ff09d82056d4","permalink":"https://yumaloop.github.io/post/2020-05-29-history_of_us_macro_economy/","publishdate":"2020-05-29T00:00:00Z","relpermalink":"/post/2020-05-29-history_of_us_macro_economy/","section":"post","summary":"アメリカのマクロ経済史を振り返り,2021年以降のアメリカ経済について考えてみる．","tags":["Tips"],"title":"United States Macroeconomic History","type":"post"},{"authors":null,"categories":["StatML"],"content":"システムと定常状態 多くの動的モデル（Dynamic Model-System）は，定常状態（time-steady state）に至ることを目的として設計される．ここで，定常状態とは，「ある変数$X$に作用する，何らかの時変量（parameter $\\theta_t$）や関数$L_t(x; \\theta)$）が一定値に収束すること」と定義しておく．\nそして，着目している動的モデルが定常状態に至るプロセスは，システム同定（System identification）と呼ばれ，定常状態を示した概念として，均衡（equilibrium）や平衡（balance）と呼ばれる用語が使われる．\n 化学反応においては、可逆反応の生成物の変化量と出発物質の変化量が合致した状態を指す。化学平衡を参照。 力学においては、物体に加わっている全ての力の合力と力のモーメントの和がともに 0 である状態を平衡と呼ぶ。力学的平衡（英語: Mechanical equilibrium）を参照。 熱力学においては通常、熱平衡、力学的平衡、化学平衡の三つを合わせて、熱力学的平衡とよぶ。 統計力学においては、系のエネルギー分布が、ボルツマン分布に従うことである。熱力学的平衡を参照。 物理化学においては、複数の物質相から構成される系において、相間の物質の出入りが合い等しい状態を指す。相平衡を参照。 電気工学においては、信号源と負荷の間のインピーダンスが合致していることを指す。インピーダンス平衡を参照。 電気回路においては、信号回路の双方が接地点に接続されていないことを指す。平衡接続を参照。 情報工学においては、データ木構造の任意の節においてその配下の節点の数が等しい状態を指す。 生態学においては、生物群集間の分布と個体数の変化が無い状態を指す。 生理学においては、水平であることを認知することを指す。平衡感覚を参照。 経済学においては、需要と供給が釣り合って価格が不動になることなどを指す。均衡を参照。  連立(微分)方程式で記述できるため．\nなぜ線形モデルが有用なのか？ 答えはシンプルで，Taylor展開\n$$ \\begin{align} \\frac{d x_1}{d t} \u0026amp;= f_1(x_1, \\dots, x_n; \\theta_1) \\\\\n\\frac{d x_2}{d t} \u0026amp;= f_2(x_1, \\dots, x_n; \\theta_1) \\\\\n\u0026amp;\\vdots \\\\\n\\frac{d x_n}{d t} \u0026amp;= f_n(x_1, \\dots, x_n; \\theta_1) \\end{align} $$\nこのモデルが，定常状態にいる場合，\n$$ \\begin{align} f_1 = f_2 = \\cdots = f_n = 0 \\end{align} $$\nが成り立つから，$n$個の変数${\\bf x} = (x_1, x_2, \\dots, x_n)$に対して，$n$個の方程式が得られる．この解がシステムの定常化となる．これを${\\bf x}^* = (x_1^*, x_2^*, \\dots, x_n^*)$とおくと，$f_1, f_2, \\dots, f_n$に対して，点${\\bf x}^*$の近傍でTaylor展開が可能になる．\n$$ \\begin{align} \\frac{d x_1}{d t} = f_1(x_1, \\dots, x_n; \\theta_1) =\u0026amp; a_{11}(x_1 - x_1^*) + a_{12} {(x_2 - x_2^*)} + \\cdots a_{1n} {(x_n - x_n^*)} + \\\\\n\u0026amp; a_{111}{(x_1 - x_1^*)}^2 + a_{112}(x_1 - x_1^*)(x_2 - x_2^*) + \\cdots + a_{11n}{(x_1 - x_1^*)}^2 + \\\\\n\u0026amp; ~~ \\vdots \\\\\n\u0026amp; a_{11\\cdots1}{(x_1 - x_1^*)}^n + \\cdots \\\\\n\\frac{d x_2}{d t} = f_2(x_1, \\dots, x_n; \\theta_1) =\u0026amp; a_{21}(x_1 - x_1^*) + a_{22} {(x_2 - x_2^*)} + \\cdots a_{2n} {(x_n - x_n^*)} + \\\\\n\u0026amp; a_{211}{(x_1 - x_1^*)}^2 + a_{212}(x_1 - x_1^*)(x_2 - x_2^*) + \\cdots + a_{21n}{(x_1 - x_1^*)}^2 + \\\\\n\u0026amp; ~~ \\vdots \\\\\n\u0026amp; a_{21\\cdots1}{(x_1 - x_1^*)}^n + \\cdots \\\\ \\\\\n\u0026amp; ~~ \\vdots \\\\ \\\\\n\\frac{d x_n}{d t} = f_n(x_1, \\dots, x_n; \\theta_1) =\u0026amp; a_{n1}(x_1 - x_1^*) + a_{n2} {(x_2 - x_2^*)} + \\cdots a_{nn} {(x_n - x_n^*)} + \\\\\n\u0026amp; a_{n11}{(x_1 - x_1^*)}^2 + a_{n12}(x_1 - x_1^*)(x_2 - x_2^*) + \\cdots + a_{n1n}{(x_1 - x_1^*)}^2 + \\\\\n\u0026amp; ~~ \\vdots \\\\\n\u0026amp; a_{n1\\cdots1}{(x_1 - x_1^*)}^n + \\cdots \\\\\n\\end{align} $$\nつまり，任意の微分可能関数$f_1, f_2, \\dots, f_n$によって表現されたダイナミクスをもつ動的モデルは，（定常解の近傍では）任意のn次多項式によって近似できる．これにより，線形システムの妥当性が保証される．一般解は，\n$$ \\begin{align} x_1 =\u0026amp; ~ x_1^* + C_{11}e^{\\lambda_1 t} + C_{12}e^{\\lambda_2 t} + \\cdots C_{1n}e^{\\lambda_n t} + \\\\\n\u0026amp; ~~~~~~~~~~ C_{111}e^{2\\lambda_1 t} + C_{112}e^{2\\lambda_2 t} + \\cdots + C_{11n}e^{2\\lambda_n t} + \\\\\n\u0026amp; ~~~~~~~~~~ ~~ \\vdots \\\\\n\u0026amp; ~~~~~~~~~~ C_{11\\cdots1}e^{n\\lambda_1 t} + C_{11\\cdots2}e^{n\\lambda_2 t} + \\cdots + C_{11\\cdots n}e^{n\\lambda_n t} \\\\\nx_2 =\u0026amp; ~ x_2^* + C_{21}e^{\\lambda_1 t} + C_{22}e^{\\lambda_2 t} + \\cdots C_{2n}e^{\\lambda_n t} + \\\\\n\u0026amp; ~~~~~~~~~~ C_{211}e^{2\\lambda_1 t} + C_{212}e^{2\\lambda_2 t} + \\cdots + C_{11n}e^{2\\lambda_n t} + \\\\\n\u0026amp; ~~~~~~~~~~ ~~ \\vdots \\\\\n\u0026amp; ~~~~~~~~~~ C_{21\\cdots1}e^{n\\lambda_1 t} + C_{21\\cdots2}e^{n\\lambda_2 t} + \\cdots + C_{21\\cdots n}e^{n\\lambda_n t} \\\\ \\\\\n\u0026amp; ~~~~~~~~~~ ~~ \\vdots \\ \\\nx_n =\u0026amp; ~ x_n^* + C_{n1}e^{\\lambda_1 t} + C_{n2}e^{\\lambda_2 t} + \\cdots C_{nn}e^{\\lambda_n t} + \\\\\n\u0026amp; ~~~~~~~~~~ C_{n11}e^{2\\lambda_1 t} + C_{n12}e^{2\\lambda_2 t} + \\cdots + C_{n1n}e^{2\\lambda_n t} + \\\\\n\u0026amp; ~~~~~~~~~~ ~~ \\vdots \\\\\n\u0026amp; ~~~~~~~~~~ C_{n1\\cdots1}e^{n\\lambda_1 t} + C_{n1\\cdots2}e^{n\\lambda_2 t} + \\cdots + C_{n1\\cdots n}e^{n\\lambda_n t} \\\\\n\\end{align} $$\nとなる．\nモデルとは何か？ つまり，多くの分野において数理モデルとか計量モデルとか呼ばれるものは，以下の手続きを必要とする．\n 定常状態に至ることを目的とする動的モデルを定義する モデルの状態変化を最適化問題(過程)として定式化する． 定常状態への収束が保証された最適化アルゴリズムを考える  移動平均  カルマンフィルタ Adamに置けるモメンタム 強化学習の報酬 ゲーム理論におけるFicticious Play 株価におけるテクニカル分析移動平均（ARMA） (金融工学)バリュエーションにおけるDCF法  移動平均とは何か？\n$$ \\begin{align} m_t \u0026amp;= \\gamma \\cdot m_t − 1+η \\cdot \\frac{\\partial L(w_t)}{\\partial w} \\\\\nw_{t+1} \u0026amp;= w_t - m_t \\end{align} $$\n$$ m_t = g_t + \\gamma \\cdot g_{t-1} + \\gamma^2 \\cdot g_{t-2} \\cdots + \\gamma^t \\cdot g_{0} $$\n","date":1587686400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587686400,"objectID":"21fbd7d1785d6723151bf2b3dc3e018b","permalink":"https://yumaloop.github.io/post/2020-04-24-time-steady-state-on-system/","publishdate":"2020-04-24T00:00:00Z","relpermalink":"/post/2020-04-24-time-steady-state-on-system/","section":"post","summary":"動的システムの定常状態，線形性，移動平均について．","tags":["Tips","System"],"title":"Time-steady States on Systems","type":"post"},{"authors":null,"categories":["StatML"],"content":"   State Space Model (SSM)\nState Space Model(SSM) is widely used in the field requiring the sequential estimation or online learning. This model is effective if you consider a system having two different variables; one completely represents the actual state but cannot be observed and the other partially represents the actual state but can be observed. Here, I call the former $x$ (state variable) and the latter $y$ (observation variable).\nIn SSM, we intruduce the following equations $F, H$ (or $f, h$) and identify them by observed data sample $[y_1, \\dots, y_t]$.\n  Equation of each state $x_t$ :\n$$ \\begin{aligned} x_{t+1} \u0026amp;= F(x_t) ~~ (\\text{Deterministic process}) \\\\\nx_{t+1} \u0026amp;\\sim f(\\cdot\\vert x_t) ~~ (\\text{Stochastic process}) \\end{aligned} $$\n  Equation of each observation $y_t$ :\n$$ \\begin{aligned} y_t \u0026amp;= H(x_t) ~~ (\\text{Deterministic process}) \\\\\ny_t \u0026amp;\\sim h(\\cdot \\vert x_t) ~~ (\\text{Stochastic process}) \\end{aligned} $$\n  Perticle filter\n  For each $i$ in $[1 \\dots M]$\n  (Prediction)\nDerive prediction distribution $f(x_t \\vert \\cdot)$ depends on particles $\\hat{x}_{t-1}$.\nSample $x^{i}_{t \\vert t-1} ~~~ (i = 1, \\dots, M)$ following $f(x_t \\vert \\cdot)$.\n$$ \\begin{align} x^{i}_{t \\vert t-1} \\sim f(x_t \\vert \\hat{x}_{t-1}) \\end{align} $$\n    (Likelihood)\nDerive the likelihood of $x^i_{t \\vert t-1}$ from given sample data $y_t$ based on $h(\\cdot)$\n$$ w^i_t \\sim h(y_t \\vert x^i_{t \\vert t-1}) $$\n  (Resampling)\nResampe $\\hat{x}^i_{t \\vert t-1}$ based on the likelihood $w^i_t ~~~ (i=1,\\dots,M)$ .\nDerive the filter distribution $p(x_t \\vert y_{1:t})$ for any $x_t$: $$ \\begin{aligned} p(x_t \\vert y_{1:t}) \u0026amp;\\approx \\frac{1}{M} \\sum_{i=1}^{M} \\delta(x_t - \\hat{x}^i_{t \\vert t-1}) \\\\\n\u0026amp;\\approx \\sum_{i=1}^{M} \\frac{}{\\sum_{i=1}^{M} } \\delta(x_t - \\hat{x}^i_{t \\vert t-1}) \\end{aligned} $$\n  ","date":1584489600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584489600,"objectID":"f552b8fa81a0587f981381f6b2396798","permalink":"https://yumaloop.github.io/post/2020-03-18-particle_filter/","publishdate":"2020-03-18T00:00:00Z","relpermalink":"/post/2020-03-18-particle_filter/","section":"post","summary":"State Space Model (SSM)\nState Space Model(SSM) is widely used in the field requiring the sequential estimation or online learning. This model is effective if you consider a system having two different variables; one completely represents the actual state but cannot be observed and the other partially represents the actual state but can be observed.","tags":["Bayes","System"],"title":"State Space Model \u0026 Particle Filter","type":"post"},{"authors":null,"categories":["StatML"],"content":"平均場近似と自由エネルギー ある変数$X$のとりうるすべての状態(実現値)$x$に対して，何らかのエネルギー関数$\\phi(x)$が与えられたとする．このとき，変数$X$のGibbs分布（Boltzmann分布）:\n$$ \\begin{aligned} p(x) \u0026amp;= \\frac{\\exp (- \\beta \\phi(x))}{\\int_X \\exp (- \\beta \\phi(x))} = \\frac{\\exp (- \\beta \\phi(x))}{Z^{\\phi}(\\beta)} \\end{aligned} $$\nを考える．このとき，Gibbs分布$p(x)$と任意の近似分布$q(x)$とのKL-divergence:\n$$ \\begin{aligned} D_{KL}(q \\vert\\vert p) := \\int_{X} q(x) \\log \\frac{q(x)}{p(x)} \\end{aligned} $$\nは以下のように分解できる．\n$$ \\begin{aligned} D_{KL}(q \\vert\\vert p) \u0026amp;= \\beta \\int_X q(x)\\phi(x) - \\left\\{ - \\int_X q(x)\\log q(x) \\right\\} + \\log \\int_X \\exp(-\\beta \\phi(x)) \\\\\n\u0026amp;= \\beta~ \\mathbb{E}_{x \\sim q}[\\phi(x)] - H_q(X) + \\log Z^{\\phi}(\\beta) \\\\\n\u0026amp;= \\beta~ (\\text{Internal energy}) - (\\text{Entropy}) + (\\text{Const.}) \\\\\n\\end{aligned} $$\nいま，近似分布$q(x)$に対する汎関数として，自由エネルギー:\n$$ F^{\\phi}(q) := \\mathbb{E}_{x \\sim q}[\\phi(x)] - \\frac{1}{\\beta}H_q(X) ~~~ (\\text{Free energy}) $$\nを定義すれば，\n$$ D_{KL}(q \\vert\\vert p) = \\beta~ F^{\\phi}(q) + \\log Z^{\\phi}(\\beta) $$\nとなるから，$q(x)$による$p(x)$の近似問題は次式で表現できる．\n$$ \\begin{aligned} \\underset{q}{\\rm min} ~ D_{KL}(p \\vert\\vert q) \u0026amp;= \\underset{q}{\\rm min} ~ F^{\\phi}(q) \\end{aligned} $$\nまた．自由エネルギー$F^{\\phi}(q)$の最小値は，\n$$ {F^{\\phi}}^{*}(q) = - \\frac{1}{\\beta} \\log \\int_X \\exp (-\\beta \\phi(x)) = - \\frac{1}{\\beta} \\log Z^{\\phi}(\\beta) $$\nとなる．すなわち，\n$$ \\begin{aligned} {F^{\\phi}}(q) = - \\frac{1}{\\beta} \\log Z^{\\phi}(\\beta) ~~ \\Leftrightarrow ~~ D_{KL}(p \\vert\\vert q) = 0 ~~ \\Leftrightarrow ~~ p(\\cdot) \\equiv\tq(\\cdot) \\end{aligned} $$\nとなる．\n熱力学(統計力学)との関係 温度$T$，内部エネルギー$U$，エントロピー$S$に対して，Helmholtzの自由エネルギー$F$は以下のように定義される．\n$$ F = U - TS $$\n$F^{\\phi}(q)$の定義式で，$F = F^{\\phi}(q)$，$U = \\mathbb{E}_{x \\sim q}[\\phi(x)]$，$S = H_q(X)$とおけば，\n$$ \\begin{aligned} \\beta F \u0026amp;= \\beta U - S \\\\\nF \u0026amp;= U - \\frac{1}{\\beta} S \\end{aligned} $$\nとなるから，汎関数$F^{\\phi}(q)$は，熱力学におけるHelmholtzの自由エネルギー$F$と類似した形式を持っていることがわかる．なお，Bayes理論において定数$\\beta$は「逆温度」と呼ばれるが，これは温度$T$に由来する．\nBayes脳やFEPとの関係 神経科学の分野でK.Fristonによって提唱された自由エネルギー原理(Free energy principle, FEP)は，上にある汎関数$F^{\\phi}(q)$を変分推論を組み合わせたものである（と解釈できる）．ここでは，ELBOとの関係にのみ触れておく．\nELBOの定義式:\n$$ \\begin{align} (\\text{Evidence}) \u0026amp;= \\log p(y) \\\\\n\u0026amp;\\geq \\mathbb{E}_{\\theta \\sim q}\\left[ \\log p(y, \\theta) \\right] - \\mathbb{E}_{\\theta \\sim q} \\left[ \\log q(\\theta) \\right] \\\\\n\u0026amp;= \\mathcal{L}_{ELBO}(q) \\\\\n\u0026amp;= (\\text{Evidence Lower Bound}) \\end{align} $$\nとFristonのCell論文(2009)にある自由エネルギーの定義式\n$$ F(y) = - \\mathbb{E}_{\\theta \\sim q}[\\log p(y,\\theta)] + \\mathbb{E}_{\\theta \\sim q}[\\log q(\\theta)] $$\nを比べると，以下の関係が得られる．\n$$ \\begin{aligned} (\\text{Surprise}) \u0026amp;= - \\log p(y) \\\\\n\u0026amp;\\leq - \\mathbb{E}_{\\theta \\sim q}[\\log p(y,\\theta)] + \\mathbb{E}_{\\theta \\sim q}[\\log q(\\theta)] \\\\\n\u0026amp;= -\\mathcal{L}_{ELBO}(q) \\\\\n\u0026amp;= F(y) \\\\\n\u0026amp;= (\\text{Free energy}) \\end{aligned} $$\nつまり，Fristonの自由エネルギー$F(y)$は「脳の外部環境$Y$に対する観測データ${\\{y_t\\}}_{t=1}^{n}$の対数尤度下限(ELBO)に$-1$をかけたもの」である．なお，Bayes推論では,対数尤度$\\log p(y)$をエビデンス(Evidence)といい，情報理論では負の対数尤度$-\\log p(y)$をサプライズ(Surprise)という．\nFristonのCell論文(2009)にあるエージェントの行動$\\alpha$や脳の内部状態$\\mu$の更新式:\n$$ \\begin{align} \\alpha^{*} \u0026amp;= \\underset{\\alpha}{\\rm argmin} ~ F(y) \\\\\n\\mu^{*} \u0026amp;= \\underset{\\mu}{\\rm argmin} ~ F(y) \\end{align} $$\nにおける$F(y)$の最小化は，「脳の外部環境$Y$に対する観測データ${\\{y_t\\}}_{t=1}^{n}$の対数尤度(Evidence)」を最大化する過程を表している．ELBOとFEPの関係をまとめると以下の表のようになる．\n    原理 Jensenの不等式 Bayes推論     ELBO Evidence: $ \\log p(y)$ の最大化 $\\text{Evidence} \\geq \\mathcal{L}_{ELBO}$ 下限$\\mathcal{L}_{ELBO}$を最大化   FEP Surprise: $- \\log p(y)$ の最小化 $\\text{Surprise} \\leq F$ 上限$F$を最小化    執筆時の個人的な理解としては，FEPにおける各変数$\\theta, \\mu, y, \\alpha$の更新規則は，「観測データを用いた最尤推定」そのものだと思っている．論文で提唱されている自由エネルギー$F(y)$最小化は，Variational BayesにおけるELBO最大化と同じであるから，むしろ4つの変数間のループ構造（グラフ表現）の方が重要なのだろう．\nFristonのNature論文(2010)では，自由エネルギー$F(y)$の定義がより複雑化しており，よく理解していない．\n","date":1583798400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583798400,"objectID":"44558e929078445c7ddf4f8e3db92205","permalink":"https://yumaloop.github.io/post/2020-03-10-free_energy_on_bayes_inference/","publishdate":"2020-03-10T00:00:00Z","relpermalink":"/post/2020-03-10-free_energy_on_bayes_inference/","section":"post","summary":"Fristonの自由エネルギー原理とELBOの等価性について","tags":["Bayes"],"title":"Free energy and Bayes inference","type":"post"},{"authors":null,"categories":["Tips"],"content":"   In Git, commit messages are very imoportant to avoid confused commit log in your branch. But in some small projects that you develope alone, thinking about every commit message might be dull.\nI usualy use the following script to send local data to the remote repository. Please try it.\n#!/bin/bash echo -e \u0026quot;\\033[0;32mDeploying updates to GitHub...\\033[0m\u0026quot; # Go To .git root directory cd ~/workspace/{project_name} # Add all changes to git. git add . # Commit changes. msg=\u0026quot;update repo `date`\u0026quot; if [ $# -eq 1 ] then msg=\u0026quot;$1\u0026quot; fi git commit -m \u0026quot;$msg\u0026quot; # Push source and build repos. git push origin master  ","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"813e3de4b4d1b414e68a4ba3b298a8c2","permalink":"https://yumaloop.github.io/post/2020-03-01-bash_script_for_git_push/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/post/2020-03-01-bash_script_for_git_push/","section":"post","summary":"In Git, commit messages are very imoportant to avoid confused commit log in your branch. But in some small projects that you develope alone, thinking about every commit message might be dull.","tags":["Git","Shell"],"title":"Bash script for git push","type":"post"},{"authors":null,"categories":["StatML"],"content":"   Evidence Lower Bound (ELBO) is widely used in variational inference. Recently, according to the massive success of DeepLearning and related models, variational inference (and its technic) gains exposure in the filed of representation learning. For instance, stochastic generative models such as VAE and GAN are famous for their variational aspects.\nELBO Evidence Lower Bound (ELBO) is a lower bound of Log likelihood of $X$ (Evidence) in the model. The below inequality holds based on Cauchy-Schwartz inequality because of the convexity of log function.\n$$ \\begin{aligned} (\\text{Evidence}) \u0026amp;= \\log p(x) \\\\\n\u0026amp;= \\log \\int_{Z} p(x,z) \\\\\n\u0026amp;= \\log \\int_{Z} p(x,z) \\frac{q(z)}{q(z)} \\\\\n\u0026amp;= \\log \\int_{Z} q(z) \\frac{p(x,z)}{q(z)} \\\\\n\u0026amp;= \\log \\mathbb{E}_{z \\sim q} \\left[ \\frac{p(x,z)}{q(z)} \\right] \\\\\n\u0026amp;\\geq \\mathbb{E}_{z \\sim q} \\left[ \\log \\frac{p(x,z)}{q(z)} \\right] \\\\\n\u0026amp;= \\mathbb{E}_{z \\sim q} \\left[ \\log p(x,z) \\right] + H_q(Z) \\\\\n\u0026amp;= ELBO(q) ~~~ (\\text{Evidence Lower Bound, ELBO}) \\end{aligned} $$\nSo that, we can obtain the optimization formula below.\n$$ \\begin{aligned} \\underset{\\theta}{\\rm max} ~ \\log p_{\\theta}(x) \u0026amp;= \\underset{q}{\\rm max} ~ ELBO(q) \\end{aligned} $$\nKL-divergence and ELBO $$ \\begin{aligned} D_{KL}( q(z) \\vert\\vert p(z \\vert x) ) \u0026amp;= \\int_{Z} q(z) \\frac{q(z)}{p(z \\vert x)} \\\\\n\u0026amp;= - H_q(Z) - \\mathbb{E}_{z \\sim q} \\left[ \\log p(z|x) \\right] \\end{aligned} $$\nELBO is considered as the difference between Log likelihood $\\log p(x)$ and KL-divergence $D_{KL}( q(z) \\vert\\vert p(z \\vert x) )$ as below.\n$$ \\begin{align} ELBO \u0026amp;= \\mathbb{E}_{z \\sim q} \\left[ \\log p(x,z) \\right] + H_q(Z) \\\\\n\u0026amp;= \\mathbb{E}_{z \\sim q} \\left[ \\log p(x) + \\log p(z|x) \\right] + H_q(Z) \\\\\n\u0026amp;= \\mathbb{E}_{z \\sim q} \\left[ \\log p(x) \\right] + \\mathbb{E}_{z \\sim q} \\left[ \\log p(z|x) \\right] + H_q(Z) \\\\\n\u0026amp;= \\log p(x) + H_q(Z) + \\mathbb{E}_{z \\sim q} \\left[ \\log p(z \\vert x) \\right] \\\\\n\u0026amp;= \\log p(x) - D_{KL}( q(z) \\vert\\vert p(z \\vert x) ) \\end{align} $$\nSo that, we can obtain the below relation.\n$$ \\begin{align} \\underset{\\theta}{\\rm max} ~ \\log p_{\\theta}(x) \u0026amp;= \\underset{q}{\\rm max} ~ ELBO(q) \\\\\n\u0026amp;= \\underset{q}{\\rm min} ~ D_{KL}( q(z) \\vert\\vert p(z|x) ) \\end{align} $$ \\\n","date":1582502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582502400,"objectID":"4c6e4097764feef99ecf700b24345c9b","permalink":"https://yumaloop.github.io/post/2020-02-24-deriving-elbo/","publishdate":"2020-02-24T00:00:00Z","relpermalink":"/post/2020-02-24-deriving-elbo/","section":"post","summary":"Evidence Lower Bound (ELBO) is widely used in variational inference. Recently, according to the massive success of DeepLearning and related models, variational inference (and its technic) gains exposure in the filed of representation learning.","tags":["Bayes"],"title":"Deriving ELBO","type":"post"},{"authors":null,"categories":["StatML"],"content":"In this post, I introduce you the Counterfactual Regret Minimization (CFR Algorithm). It is mainly used for the algorithm to figure out the optimal strategy of a extensive-form game with incomplete information such as Poker and Mahjong.\nExtensive-form Game  Set, variables  $N: $ set of players  $i \\in N$: player   $A :$ set of actions  $a \\in A: $ action   $H: $set of sequences  $h \\in H: $ sequences (= possible history of actions, $h = (a_1, \\dots, a_t$) $Z \\subseteq H: $ set of terminal histories. $Z = {z \\in H \\vert \\forall h \\in H, z \\notin h }$ $z \\in Z$: sea     Function, relations  $u_i: Z \\to \\mathbb{R}: $ utility function of player $i$ $\\sigma_i: A \\to [0,1]$ a strategy of player $i$, probability distribution on action set $A$. $\\sigma~: A^N \\to [0,1]$ a strategy profile, $\\sigma := (\\sigma_1, \\dots, \\sigma_N)$ $\\pi^{\\sigma}i: H \\to [0,1]: $ probability of history $h$ under a strategy $$\\sigma$ of player $i$ $\\pi^{\\sigma}: H^N \\to [0,1]: $ probability of history $h$ under a strategy profile $\\sigma$    Then, you can also interplate $u_i$ as the function mapping a storategy profile $\\sigma$ to its utility.\n$$ \\begin{align} u_i(\\sigma) \u0026amp;= \\sum_{h \\in Z} u_i(h) \\pi^{\\sigma}(h) \\\\\n\u0026amp;= \\sum_{h \\in Z} u_i(h) \\prod_{i \\in N} \\pi^{\\sigma}_i(h) \\end{align} $$ Nash equilibrium Definition: $(\\text{Nash equilibrium})$\nIn $N$-player extensive game, a strategy profile $\\acute{\\sigma} := (\\acute{\\sigma_1}, \\dots, \\acute{\\sigma_N})$ is the Nash equilibrium if and only if the followings holds.\n$$ \\begin{aligned} u_1(\\acute{\\sigma_1}, \\dots, \\acute{\\sigma_N}) \u0026amp;\\geq \\underset{\\sigma_1}{\\rm max} ~ u_1(\\sigma_1, \\acute{\\sigma_{-1}}) \\\\\nu_2(\\acute{\\sigma_1}, \\dots, \\acute{\\sigma_N}) \u0026amp;\\geq \\underset{\\sigma_2}{\\rm max} ~ u_2(\\sigma_2, \\acute{\\sigma_{-2}}) \\\\\n\u0026amp;~ \\vdots \\\\\nu_N(\\acute{\\sigma_1}, \\dots, \\acute{\\sigma_N}) \u0026amp;\\geq \\underset{\\sigma_N}{\\rm max} ~ u_N(\\sigma_N, \\acute{\\sigma_{-N}}) \\end{aligned} $$\nDefinition: $\\text{(}\\varepsilon\\text{-Nash equilibrium)}$\nIn $N$-player extensive game, a strategy profile $\\acute{\\sigma} := (\\acute{\\sigma_1}, \\dots, \\acute{\\sigma_N})$ is the $\\varepsilon$-Nash equilibrium if and only if the followings holds when $\\forall \\varepsilon \\geq 0$ is given.\n$$ \\begin{aligned} u_1(\\acute{\\sigma_1}, \\dots, \\acute{\\sigma_N}) + \\varepsilon \u0026amp;\\geq \\underset{\\sigma_1}{\\rm max} ~ u_1(\\sigma_1, \\acute{\\sigma_{-1}}) \\\\\nu_2(\\acute{\\sigma_1}, \\dots, \\acute{\\sigma_N}) + \\varepsilon \u0026amp;\\geq \\underset{\\sigma_2}{\\rm max} ~ u_2(\\sigma_2, \\acute{\\sigma_{-2}}) \\\\\n\u0026amp;~ \\vdots \\\\\nu_N(\\acute{\\sigma_1}, \\dots, \\acute{\\sigma_N}) + \\varepsilon \u0026amp;\\geq \\underset{\\sigma_N}{\\rm max} ~ u_N(\\sigma_N, \\acute{\\sigma_{-N}}) \\end{aligned} $$\nRegret matching  Average overall regret of player $i$ at time $T$：  $$ R_i^T := \\underset{\\sigma_i^*}{\\rm max} ~ \\frac{1}{T} \\sum_{t=1}^{T} \\left( u_i(\\sigma_i^*, \\sigma_{-i}^{t}) - u_i(\\sigma_i^t, \\sigma_{-i}^{t}) \\right) $$\n Average strategy for player $i$ from time $1$ to $T$：  $$ \\begin{align} \\overline{\\sigma}_i^t(I)(a) \u0026amp;:= \\frac{\\sum_{t=1}^{T} \\pi_i^{\\sigma^t}(I) \\cdot \\sigma^t(I)(a)}{\\sum_{t=1}^{T} \\pi_i^{\\sigma^t}(I)} \\\\\n\u0026amp;= \\frac{\\sum_{t=1}^{T} \\sum_{h \\in I} \\pi_i^{\\sigma^t}(h) \\cdot \\sigma^t(h)(a)}{\\sum_{t=1}^{T} \\sum_{h \\in I} \\pi_i^{\\sigma^t}(h)} \\end{align} $$\nIf the average overall regret holds $R_i^T \\leq \\varepsilon$, the average strategy $\\overline{\\sigma}_i^t(I)(a) $ is $2 \\varepsilon$-Nash equilibrium for player $i$ in time $t$. So that, in order to derive Nash equilibrium, we should minimize the average overall regret $R_i^T$ or its upper bound $\\varepsilon$ according to $R_i^T \\to 0 ~~ (\\varepsilon \\to 0)$.\nCFR Algorithm  Counterfactual utility：  $$ \\begin{align} u_i(\\sigma, I) = \\frac{\\sum_{h \\in H, h' \\in Z} \\pi_{-i}^{\\sigma}(h)\\pi^{\\sigma}(h,h')u_i(h) }{\\pi_{-i}^{\\sigma}(I)} \\end{align} $$\n immediate counteractual regret of action $a$ in Information set $I$:  $$ \\begin{aligned} R_{i,imm}^{T}(I, a) := \\frac{1}{T} \\sum_{t=1}^{T} \\pi_{-i}^{\\sigma^t}(I) \\left( u_i(\\sigma^t_{I \\to a}, I) - u_i(\\sigma^t, I) \\right) \\end{aligned} $$\n Immediate counterfactual regret of Information set $I$：  $$ \\begin{aligned} R_{i,imm}^{T}(I) \u0026amp;:= \\underset{a \\in A(I)}{\\rm max} ~ \\frac{1}{T} \\sum_{t=1}^{T} \\pi_{-i}^{\\sigma^t}(I) \\left( u_i(\\sigma^t_{I \\to a}, I) - u_i(\\sigma^t, I) \\right) \\end{aligned} $$\nThe following inequality holds for the average overall regret $R_i^T $ and the immediate counterfactual regret $R_{i,imm}^{T}(I)$:\n$$ \\begin{aligned} R_i^T \\leq \\sum_{I \\in \\mathcal{I}_i} \u0026amp;R_{i,imm}^{T,+}(I) \\\\\nwhere ~~~ \u0026amp;R_{i,imm}^{T, +}(I) := max(R_{i,imm}^{T}(I), 0) \\end{aligned} $$\nSo that, we obtain the sufficient condition of $R_{i,imm}^{T}(I)$ for the average strategy $\\overline{\\sigma}_i^t(I)(a)$ to become a Nash equilibrium strategy as below.\n$$ \\sum_{I \\in \\mathcal{I}_i} R_{i,imm}^{T,+}(I) \\to 0 ~~~ \\Rightarrow ~~~ R_i^T \\to 0 ~~~ \\Rightarrow ~~~ \\varepsilon \\to 0. $$\nNow all we need is to minimize the immediate counterfactual regret $R_{i,imm}^{T}(I)$.\nIn addition, as can be seen from the above formula, the computational complexity of the CFR algorithm depends on the number of information sets $I$. Also, to avoid the complete search of game tree (searching all information sets $I$), subsequent algorithms such as CFR + propose an abstraction of the game state.\nPython code to run CFR algorithm for Kuhn Poker import numpy as np # Number of actions a player can take at a decision node. _N_ACTIONS = 2 _N_CARDS = 3 def main(): \u0026quot;\u0026quot;\u0026quot; Run iterations of counterfactual regret minimization algorithm. \u0026quot;\u0026quot;\u0026quot; i_map = {} # map of information sets n_iterations = 10000 expected_game_value = 0 for _ in range(n_iterations): expected_game_value += cfr(i_map) for _, v in i_map.items(): v.next_strategy() expected_game_value /= n_iterations display_results(expected_game_value, i_map) def cfr(i_map, history=\u0026quot;\u0026quot;, card_1=-1, card_2=-1, pr_1=1, pr_2=1, pr_c=1): \u0026quot;\u0026quot;\u0026quot; Counterfactual regret minimization algorithm. Parameters ---------- i_map: dict Dictionary of all information sets. history : [{'r', 'c', 'b'}], str A string representation of the game tree path we have taken. Each character of the string represents a single action: 'r': random chance action 'c': check action 'b': bet action card_1 : (0, 2), int player A's card card_2 : (0, 2), int player B's card pr_1 : (0, 1.0), float The probability that player A reaches `history`. pr_2 : (0, 1.0), float The probability that player B reaches `history`. pr_c: (0, 1.0), float The probability contribution of chance events to reach `history`. \u0026quot;\u0026quot;\u0026quot; if is_chance_node(history): return chance_util(i_map) if is_terminal(history): return terminal_util(history, card_1, card_2) n = len(history) is_player_1 = n % 2 == 0 info_set = get_info_set(i_map, card_1 if is_player_1 else card_2, history) strategy = info_set.strategy if is_player_1: info_set.reach_pr += pr_1 else: info_set.reach_pr += pr_2 # Counterfactual utility per action. action_utils = np.zeros(_N_ACTIONS) for i, action in enumerate([\u0026quot;c\u0026quot;, \u0026quot;b\u0026quot;]): next_history = history + action if is_player_1: action_utils[i] = -1 * cfr(i_map, next_history, card_1, card_2, pr_1 * strategy[i], pr_2, pr_c) else: action_utils[i] = -1 * cfr(i_map, next_history, card_1, card_2, pr_1, pr_2 * strategy[i], pr_c) # Utility of information set. util = sum(action_utils * strategy) regrets = action_utils - util if is_player_1: info_set.regret_sum += pr_2 * pr_c * regrets else: info_set.regret_sum += pr_1 * pr_c * regrets return util def is_chance_node(history): \u0026quot;\u0026quot;\u0026quot; Determine if we are at a chance node based on tree history. \u0026quot;\u0026quot;\u0026quot; return history == \u0026quot;\u0026quot; def chance_util(i_map): expected_value = 0 n_possibilities = 6 for i in range(_N_CARDS): for j in range(_N_CARDS): if i != j: expected_value += cfr(i_map, \u0026quot;rr\u0026quot;, i, j, 1, 1, 1/n_possibilities) return expected_value/n_possibilities def is_terminal(history): possibilities = { \u0026quot;rrcc\u0026quot;: True, \u0026quot;rrcbc\u0026quot;: True, \u0026quot;rrcbb\u0026quot;: True, \u0026quot;rrbc\u0026quot;: True, \u0026quot;rrbb\u0026quot;: True} return history in possibilities def terminal_util(history, card_1, card_2): n = len(history) card_player = card_1 if n % 2 == 0 else card_2 card_opponent = card_2 if n % 2 == 0 else card_1 if history == \u0026quot;rrcbc\u0026quot; or history == \u0026quot;rrbc\u0026quot;: # Last player folded. The current player wins. return 1 elif history == \u0026quot;rrcc\u0026quot;: # Showdown with no bets return 1 if card_player \u0026gt; card_opponent else -1 # Showdown with 1 bet assert(history == \u0026quot;rrcbb\u0026quot; or history == \u0026quot;rrbb\u0026quot;) return 2 if card_player \u0026gt; card_opponent else -2 def card_str(card): if card == 0: return \u0026quot;J\u0026quot; elif card == 1: return \u0026quot;Q\u0026quot; elif card == 2: return \u0026quot;K\u0026quot; def get_info_set(i_map, card, history): \u0026quot;\u0026quot;\u0026quot; Retrieve information set from dictionary. \u0026quot;\u0026quot;\u0026quot; key = card_str(card) + \u0026quot; \u0026quot; + history info_set = None if key not in i_map: info_set = InformationSet(key) i_map[key] = info_set return info_set return i_map[key] class InformationSet(): def __init__(self, key): self.key = key self.regret_sum = np.zeros(_N_ACTIONS) self.strategy_sum = np.zeros(_N_ACTIONS) self.strategy = np.repeat(1/_N_ACTIONS, _N_ACTIONS) self.reach_pr = 0 self.reach_pr_sum = 0 def next_strategy(self): self.strategy_sum += self.reach_pr * self.strategy self.strategy = self.calc_strategy() self.reach_pr_sum += self.reach_pr self.reach_pr = 0 def calc_strategy(self): \u0026quot;\u0026quot;\u0026quot; Calculate current strategy from the sum of regret. \u0026quot;\u0026quot;\u0026quot; strategy = self.make_positive(self.regret_sum) total = sum(strategy) if total \u0026gt; 0: strategy = strategy / total else: n = _N_ACTIONS strategy = np.repeat(1/n, n) return strategy def get_average_strategy(self): \u0026quot;\u0026quot;\u0026quot; Calculate average strategy over all iterations. This is the Nash equilibrium strategy. \u0026quot;\u0026quot;\u0026quot; strategy = self.strategy_sum / self.reach_pr_sum # Purify to remove actions that are likely a mistake strategy = np.where(strategy \u0026lt; 0.001, 0, strategy) # Re-normalize total = sum(strategy) strategy /= total return strategy def make_positive(self, x): return np.where(x \u0026gt; 0, x, 0) def __str__(self): strategies = ['{:03.2f}'.format(x) for x in self.get_average_strategy()] return '{} {}'.format(self.key.ljust(6), strategies) def display_results(ev, i_map): print('player 1 expected value: {}'.format(ev)) print('player 2 expected value: {}'.format(-1 * ev)) print() print('player 1 strategies:') sorted_items = sorted(i_map.items(), key=lambda x: x[0]) for _, v in filter(lambda x: len(x[0]) % 2 == 0, sorted_items): print(v) print() print('player 2 strategies:') for _, v in filter(lambda x: len(x[0]) % 2 == 1, sorted_items): print(v) if __name__ == \u0026quot;__main__\u0026quot;: main()  ","date":1581292800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581292800,"objectID":"b0463a81c533ce0008fd98312178c36b","permalink":"https://yumaloop.github.io/post/2020-02-10-counterfactual-regret-minimization/","publishdate":"2020-02-10T00:00:00Z","relpermalink":"/post/2020-02-10-counterfactual-regret-minimization/","section":"post","summary":"In this post, I introduce you the Counterfactual Regret Minimization (CFR Algorithm). It is mainly used for the algorithm to figure out the optimal strategy of a extensive-form game with incomplete information such as Poker and Mahjong.","tags":["RL","Game Theory","Regret","Python"],"title":"Counterfactual Regret Minimization","type":"post"},{"authors":null,"categories":["Random"],"content":"   In this note, I describe how to install NVIDIA GPU and set up CUDA/cuDNN on Ubuntu 16.04LTS machine that has been clean booted. Also, I write down some linux commands used in debugging, since knowing your machine in detail would lead to resolving some errors related to the machine environment. This article could be updated from time to time.\n Example: My Ubuntu GPU machine (2020/01/10)\n OS :  Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-145-generic x86_64)   RAM(16GB) :  Memory: Kingston 8GB 288-Pin DDR4 SDRAM DDR4 2133 (PC4 17000) x2   ROM(250GB):  SSD: Samsung SSD 750 EVO 250GB (/dev/sda) HDD: Seagate Barracuda ST2000DM001 Desktop SATA (/dev/sdb)   CPU(x8) :  Intel Core i7-6700 CPU @ 3.40GHz   GPU(x1) :  NVIDIA Geforce GTX 1080  NVIDIA CUDA : 10.0.130 (/usr/local/cuda-10.0/) NVIDIA cuDNN : 7.4.2.24 (/usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.2)  Python3 : 3.6.9 (/usr/bin/python3.6) Python2 : 2.7.12 (/usr/bin/python) tensorflow 1.13.1 ($HOME/.local/lib/python3.6/site-packages) tensorflow-gpu 1.13.1 ($HOME/.local/lib/python3.6/site-packages) keras 2.2.4 ($HOME/.local/lib/python3.6/site-packages) pytorch 1.2.0 ($HOME/.local/lib/python3.6/site-packages)          Table of contents\n Operating System  Checking Linux OS Checking Linux distribution Checking Linux kernel   Storage (ROM)  Checking ROM devices Checking the number of files Checking disk space   Memory (RAM)  Checking RAM devices Checking memory space   CPU  Checking CPU devices   GPU  Checking GPU devices NVIDIA driver \u0026amp; CUDA/cuDNN  Installing NVIDIA driver Installing NVIDIA CUDA Installing NVIDIA cuDNN     I/O  Checking X11 display manager (DM)     Operating System Checking Linux OS uname command shows 1.OS Name, 2.Hostname, 3.Release, 4.Version, 5,Hardware Architecture, 6,CPU type, 7.Platform, 8.OS Name, respectively,\n$ uname -a Linux XXXX 4.4.0-145-generic #171-Ubuntu SMP Tue Mar 26 12:43:40 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux  Checking Linux distribution /etc/issue contains information about Linux distribution.\n$ cat /etc/issue Ubuntu 16.04.6 LTS \\n \\l  /etc/lsb-release contains the same information.\n$ cat /etc/lsb-release DISTRIB_ID=Ubuntu DISTRIB_RELEASE=16.04 DISTRIB_CODENAME=xenial DISTRIB_DESCRIPTION=\u0026quot;Ubuntu 16.04.6 LTS\u0026quot;  /etc/os-release contains the same information.\n$ cat /etc/os-release NAME=“Ubuntu” VERSION=“16.04.6 LTS (Xenial Xerus)” ID=ubuntu ID_LIKE=debian PRETTY_NAME=“Ubuntu 16.04.6 LTS” VERSION_ID=“16.04\u0026quot; HOME_URL=“http://www.ubuntu.com/” SUPPORT_URL=“http://help.ubuntu.com/” BUG_REPORT_URL=“http://bugs.launchpad.net/ubuntu/” VERSION_CODENAME=xenial UBUNTU_CODENAME=xenial  Checking Linux kernel /proc/version contains information about Linux kernel.\n$ cat /proc/version Linux version 4.4.0-159-generic (buildd@lgw01-amd64-042) (gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10) ) #187-Ubuntu SMP Thu Aug 1 16:28:06 UTC 2019  Storage (ROM) Storage device (HDD, SSD) and file systems.\nChecking ROM devices df commad shows information about ROM (HDD) devices\n$ df -h Filesystem Size Used Avail Use% Mounted on udev 7.8G 0 7.8G 0% /dev tmpfs 1.6G 46M 1.6G 3% /run /dev/sda1 214G 165G 39G 81% / tmpfs 7.9G 208K 7.9G 1% /dev/shm tmpfs 5.0M 4.0K 5.0M 1% /run/lock tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup /dev/loop3 384K 384K 0 100% /snap/patchelf/93 /dev/loop1 384K 384K 0 100% /snap/patchelf/87 none 7.9G 2.5M 7.9G 1% /tmp/guest-qyuodw tmpfs 1.6G 64K 1.6G 1% /run/user/998 /dev/loop4 90M 90M 0 100% /snap/core/8213 /dev/loop0 90M 90M 0 100% /snap/core/8268 tmpfs 1.6G 0 1.6G 0% /run/user/1001  Checking the number of files wc command shows the number of files under the current dir.\n$ du -hsc * 689M\tResearch 4.0K\tbuild 106M\tdataset 4.0K\tdocker 9.3M\tgym 50M\tkaggle 2.6M\tlatent.gif 2.0G\topencv 122G\tworkspace 4.0K\tダウンロード 4.0K\tテンプレート 4.0K\tデスクトップ 4.0K\tドキュメント 4.0K\tビデオ 4.0K\tピクチャ 4.0K\tミュージック 4.0K\t公開 125G\t合計  ファイルのディスク使用量を確認したい df -hコマンドを使う\nカレントディレクトリ直下にあるファイルおよびディレクトリのディスク使用量とその合計を表示する\n$ du -hsc * 689M\tResearch 4.0K\tbuild 106M\tdataset 4.0K\tdocker 9.3M\tgym 50M\tkaggle 2.6M\tlatent.gif 2.0G\topencv 122G\tworkspace 4.0K\tダウンロード 4.0K\tテンプレート 4.0K\tデスクトップ 4.0K\tドキュメント 4.0K\tビデオ 4.0K\tピクチャ 4.0K\tミュージック 4.0K\t公開  メモリ（RAM） メモリデバイスを確認したい /proc/meminfoをみる\nメモリの詳細情報が表示される\n$ cat /proc/meminfo MemTotal: 16377200 kB MemFree: 3077848 kB MemAvailable: 15767804 kB Buffers: 363052 kB Cached: 12274992 kB SwapCached: 66936 kB Active: 8048088 kB Inactive: 4689560 kB Active(anon): 25860 kB Inactive(anon): 86584 kB ... HugePages_Total: 0 HugePages_Free: 0 HugePages_Rsvd: 0 HugePages_Surp: 0 Hugepagesize: 2048 kB DirectMap4k: 1907316 kB DirectMap2M: 14815232 kB DirectMap1G: 0 kB  メモリの空き容量を確認したい freeコマンドを使う\n$ free total used free shared buff/cache available Mem: 16377148 2470228 314496 17140 13592424 13460232 Swap: 16720892 431568 16289324  vmstatコマンドを使う\n$ vmstat procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 431584 267696 944212 12638044 0 2 389 15 0 0 6 2 91 0 0  topコマンドを使う\n$ top top - 15:55:05 up 64 days, 23:12, 5 users, load average: 1.00, 1.04, 1.07 Tasks: 232 total, 2 running, 230 sleeping, 0 stopped, 0 zombie %Cpu(s): 9.1 us, 3.5 sy, 0.0 ni, 86.9 id, 0.5 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 16377148 total, 271964 free, 2527528 used, 13577656 buff/cache KiB Swap: 16720892 total, 16289228 free, 431664 used. 13403420 avail Mem ...  CPU CPUデバイスを確認したい /proc/cpuinfoをみる\nCPUのコアごとに詳細情報が表示される\n $ cat /proc/cpuinfo processor : 0 vendor_id : GenuineIntel cpu family : 6 model : 94 model name : Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz stepping : 3 microcode : 0xc6 cpu MHz : 800.062 cache size : 8192 KB physical id : 0 siblings : 8 ... processor : 1 vendor_id : GenuineIntel cpu family : 6 model : 94 model name : Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz ...  GPU GPUデバイスの確認 lswsコマンドを使う\n$ sudo lshw -C display *-display 詳細: VGA compatible controller 製品: GP104 [GeForce GTX 1080] ベンダー: NVIDIA Corporation 物理ID: 0 バス情報: pci@0000:01:00.0 バージョン: a1 幅: 64 bits クロック: 33MHz 性能: pm msi pciexpress vga_controller bus_master cap_list rom 設定: driver=nvidia latency=0 リソース: irq:317 メモリー:de000000-deffffff メモリー:c0000000-cfffffff メモリー:d0000000-d1ffffff IOポート:e000(サイズ=128) メモリー:df000000-df07ffff  lspciコマンドを使う\nLinuxに搭載されているPCIバスの情報を表示する．\n$ lspci | grep -i nvidia 01:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1080] (rev a1) 01:00.1 Audio device: NVIDIA Corporation GP104 High Definition Audio Controller (rev a1)  NVIDIAドライバとCUDA/cuDNNの導入 NVIDIAドライバのインストール 1.下記リンクから，自分のGPUにあうドライバを検索してダウンロードする．\nhttps://www.nvidia.co.jp/Download/index.aspx?lang=jp\nたとえば，GPU「NVIDIA GeForce 1080」に対応したドライバは以下のようになる．\n．\n新しくGPUドライバ（NVIDIAドライバ）をインストールする前に，既にインストールされているGPUドライバを確認する．  aptにNVIDIAドライバを提供しているxorg-edgersレポジトリを追加する．\naptでNVIDIAドライバ「nvidia-396」をインストールして，マシンを再起動．\nCUDAのインストール （注意）CUDA・cuDNN・tensorFlow-gpuのバージョンを合わせる必要がある．\n  CUDAの公式ドキュメントをよく読む．\nCUDA Toolkit Documentation https://docs.nvidia.com/cuda/index.html\n  下記リンクから，NVIDIAドライバに対応するCUDAのバージョンを確認する\nCUDA Toolkit Documentation \u0026gt; Release Notes \u0026gt; 1. CUDA Toolkit Major Components \u0026gt; CUDA Driver https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html\n   下記リンクから，tensorflow-gpuに対応するcuDNN/CUDAのバージョンを確認する\nTensorFlow (Linux) - テスト済みのビルド設定\nhttps://www.tensorflow.org/install/source#linux\n  CUDA・cuDNN・tensorFlow-gpuのバージョン確認を終えた．\n今回は，以下で環境構築をする．\n  Python 3.6.9 tensorflow-gpu 1.13.1 CUDA 10.0 cuDNN 7.4    下記リンクから，自分の環境にあった「CUDA Toolkitパッケージ」を確認し，マシンへダウンロードする．\nCUDA Toolkit Archive https://developer.nvidia.com/cuda-toolkit-archive\n  今回は，CUDA10.0で，マシンの環境として，以下を選択．\n  Operating System: Linux Architecture: x86_64 Distribution: Ubuntu Version: 16.04 Installer Type: deb [network]   （注意）https://developer.nvidia.com/cuda-downloadsは，最新バージョンのダウンロードリンクなので，ここから安易にCUDAをダウンロードしてはいけない．特に，tensorflow-gpuは，最新のCUDA Toolkitに対応していないので注意する．CUDAとTensorflow-gpuのバージョンがあっていないと，たとえばImportError: libcublas.so.10.0が発生する．\n対応するCUDA Toolkit（CUDA 10.0）の.debファイル(network)は「cuda-repo-ubuntu1604_10.0.130-1_amd64.deb」となる． この.debファイルをwgetコマンドを使って，マシンへダウンロードする．\nダウンロードしたCUDA Toolkitパッケージ(.deb)を，マシンへインストールする  dpkgコマンドでCUDA Toolkitパッケージ(.deb)をcudaパッケージとして保存します．さらに，aptコマンドでcudaパッケージをインストールします． 注意：公式に書かれているsudo apt-get install cudaを実行すると自動的に最新版のCUDAがインストールされる．\nこれでCUDA Toolkit（CUDA 10.0）のインストールは完了．\n次に，環境変数（PATH）を設定する．\ncuDNNのインストール PATHチェック ディスプレイ X11ディスプレイマネージャ(DM)を確認 /etc/X11/default-display-managerをみる\n","date":1579132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579132800,"objectID":"1269f2e0e4402c1d9168bd4342744f90","permalink":"https://yumaloop.github.io/post/2020-01-16-setup-gpu-machine-for-ml/","publishdate":"2020-01-16T00:00:00Z","relpermalink":"/post/2020-01-16-setup-gpu-machine-for-ml/","section":"post","summary":"In this note, I describe how to install NVIDIA GPU and set up CUDA/cuDNN on Ubuntu 16.04LTS machine that has been clean booted. Also, I write down some linux commands","tags":["Env"],"title":"Setting up a GPU machine for Machine Learning","type":"post"},{"authors":null,"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://yumaloop.github.io/about/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"About Me","tags":null,"title":"About","type":"widget_page"},{"authors":null,"categories":["Random"],"content":"Prof. George M. Whitesides is a top-level researcher on chemistry and also known as one of the highest h-index researchers in the world. He explains his unique writing techniques called Outline Method in the article, \u0026ldquo;Whitesides' Group: Writing a Paper\u0026rdquo;. I\u0026rsquo;ve translated that into Japanese and publish it here.\nOriginal paper This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n  Japanese translation This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n  ","date":1574640000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574640000,"objectID":"981967c8ac223875067a00abfb3c335b","permalink":"https://yumaloop.github.io/post/2019-11-25-whitesides_outline_method/","publishdate":"2019-11-25T00:00:00Z","relpermalink":"/post/2019-11-25-whitesides_outline_method/","section":"post","summary":"Prof. George M. Whitesides is a top-level researcher on chemistry and also known as one of the highest h-index researchers in the world. He explains his unique writing techniques called Outline Method in the article, \u0026ldquo;Whitesides' Group: Writing a Paper\u0026rdquo;.","tags":["Tips","Book"],"title":"Whiteside's Outline Method","type":"post"},{"authors":null,"categories":["StatML"],"content":"   ​\tEM algorithm is an algorithm for deriving the maximum likelihood estimator (MLE), which is generally applied to statistical methods for incomplete data. Originally, the concept of “incomplete data and complete data” was established to handle missing data, but by extending the definition, it can be applied to cut data, censored data, mixed distribution models, Robust distribution models, and latent data. It can also be applied to variable models, and Bayesian modeling.\n​\tAlso, a number of statistical approach for clustering and unsupervised learning (eg, k-means, Gaussian mixture models) can be generalized as EM algorithms when focusing on the computational process. In addition, researches on analyzing the EM algorithm from the viewpoint of information geometry has been active, and applying EM algorithm to the stochastic model including an exponential family can be summarized in the form of e-projection / m-projection.\n1. Statistical inference  Objectives: To find out the probability distribution $q(x)$ that a certain variable $x \\in X$ follows.\n Namely, when considering a stochastic model $p(x \\vert \\theta)$ determined by the parameter $\\theta \\in \\Theta$ and detecting the optimal parameter $\\theta^{*} \\in \\Theta$ from dataset $ \\mathcal{D} := {\\{x_i\\}}_{i=1}^{n}$, the follwing Approximation holds.\n$$ \\begin{align} x \\sim q(x) \\approx p(x|\\theta) \\end{align} $$\nThis is called a statistical inference (or statistical estimation).\n2. Maximum likelihood estimation The most basic algorithm for statistical inference is maximum likelihood estimation (MLE). A log likelihood function of the stochastic model $p(x \\vert \\theta)$ is defined as\n$$ \\begin{align} \\ell(\\theta | x) := \\log p(x | \\theta) \\end{align} $$\nand an empirical objective function of $\\theta$:\n$$ \\begin{align} J(\\theta) := \\frac{1}{n} \\sum_{i=1}^{n} \\ell(\\theta | x_i) \\end{align} $$\nthat depends on dataset $ \\mathcal{D} := {{x_i}}_{i=1}^{n}$ can be obtained, MLE of parameter $\\theta$ is derived as follows.\n$$ \\begin{align} \\hat{\\theta}_{MLE} = \\underset{\\theta \\in \\Theta}{\\rm argmax} ~ J(\\theta) \\end{align} $$\n3. EM algorithm Let\u0026rsquo;s define the following data categories.\n Complete data $Y \\in \\mathcal{Y}$ :  not observable but completely follows the true distribution $p(y)$   Imcomplete data $X \\in \\mathcal{X}$ :  observable but not completely follows the true distribution $p(x)$    In general, the relationship complete data $y$ and incomplete data $x$ is a one-to-many relationship. but here, as a convenient assumption, I introduce a latent variable $z \\in Z$ to express this constraint, that is assume $y = [x, z]$ holds. Considering the stochastic model for the complete data $x$,\n$$ \\begin{align} p(y | \\theta) = p(x,z | \\theta) \\end{align} $$\n Complete data $\\{X,Z\\} \\in \\mathcal{X \\times Z}$ :  not observable but completely follows the true distribution $p(x,z)$   Imcomplete data $X \\in \\mathcal{X}$ :  observable but not completely follows the true distribution $p(x)$    data sample $x_i$ cannot be observed and its likelihood $p(x_i \\vert \\theta)$ cannot be calculated. However, for pair data sample $\\{x_i, z_i\\}$ can be observed and its likelihood $p(x_i, z_i \\vert \\theta)$ can be calculated.\n$$ \\begin{align} p(x_i | \\theta) \u0026amp;= \\int_{Z} p(x_i, z_i | \\theta) ~ dz \\\n\\end{align} $$\nBy using this formula, the estimated value of $\\hat{\\theta}_{MLE}$ can be obtained by approximating $p(x,z \\vert \\theta)$, the likelihood function of complete data $\\{x, z\\}$. The procedure to derive the estimated value of $\\hat{\\theta}_{MLE}$ is called EM algorithm because it is an iterative method that repeats E-step and M-step alternately.\n EM algorithm\n  Initialize $\\theta$ with $\\theta^{0}$.\n  For each step $t$:\nE Step: Update the expectation value $Q$.\n$$ \\begin{aligned} Q(\\theta | \\theta^{(t)}) \u0026amp;= \\mathbb{E}_{z \\sim p(z \\vert x, \\theta^{(t)})} \\left[ \\log p(x, z \\vert \\theta) \\right] \\\\ \u0026amp;\\simeq \\sum_{i=1}^{n} p(z_i \\vert x_i, \\theta^{(t)}) \\log p(x_i, z_i \\vert \\theta) \\\\ \u0026amp;= \\sum_{i=1}^{n} p(z_i \\vert x_i, \\theta^{(t)}) \\log p(z_i \\vert x_i, \\theta) + Const. \\end{aligned}$$\nM Step: Derive the optimal parameter ${\\theta}^{(t+1)}$ that maximize $Q$ value.\n$$\\begin{aligned} {\\theta}^{(t+1)} \u0026amp;= \\underset{\\theta \\in \\Theta}{\\rm argmax} ~ Q(\\theta \\vert {\\theta}^{(t)}) \\end{aligned}$$\n  Consider the convergence value $\\theta^{(\\infty)}$ as the algorithm output $\\hat{\\theta}_{EM}$.\n   As a result, the estimated value of $\\hat{\\theta}_{MLE}$ is derived as $\\hat{\\theta}_{EM}$ and the following holds.\n$$ \\begin{align} \\hat{\\theta}_{MLE} \\approx \\hat{\\theta}_{EM}, ~~~ p(x|\\hat{\\theta}_{MLE} ) \\approx p(x|\\hat{\\theta}_{EM} ) \\end{align} $$\nAlso, the summarized formula of calculations in E step and M step is as follows.\n  For each step $t$:\nEM Step:\n$$\\begin{align} \\theta^{(t+1)} \u0026amp;= \\underset{\\theta \\in \\Theta}{\\rm argmax} ~ \\mathbb{E}_{z \\sim p(z \\vert x, \\theta^{(t)})} \\left[ \\log p(x, z \\vert \\theta) \\right] \\\\ \u0026amp;= \\underset{\\theta \\in \\Theta}{\\rm argmax} ~ \\sum_{i=1}^{n} p(z_i \\vert x_i, \\theta^{(t)}) \\log p(x_i, z_i \\vert \\theta) \\end{align} $$\n   References  PRML Chapter 9: Mixture models and EM 9章 EMアルゴリズム - 「21 世紀の統計科学」第 III 巻 日本統計学会, 2008 解説 EMアルゴリズムの幾何学 - 赤穂昭太郎, 電子技術総合研究所 EMアルゴリズムと神経回路網, 2000, 統計数理研究所  ","date":1569369600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569369600,"objectID":"69426175cd5772c606e672386dab8632","permalink":"https://yumaloop.github.io/post/2019-09-25-em-algorithm/","publishdate":"2019-09-25T00:00:00Z","relpermalink":"/post/2019-09-25-em-algorithm/","section":"post","summary":"​ EM algorithm is an algorithm for deriving the maximum likelihood estimator (MLE), which is generally applied to statistical methods for incomplete data. Originally, the concept of “in","tags":["Bayes"],"title":"EM Algorithm","type":"post"},{"authors":null,"categories":["Finance"],"content":"   1. 資本主義の原理 資本主義 - Capitalism\n資本主義とは何か？を考える際には，資本主義ではないものは何か？を考え，その差分をまとめ直せば良い．資本主義の対義語は，共産主義（社会主義）になる．これらを排他的に比較すれば，その定義（政治的概念である\u0026quot;自由\u0026quot;や\u0026quot;民主主義\u0026quot;については除外する）は，次のようにまとめられると思う．\n 資本主義：個人が資本を所有できる 共産主義：個人が資本を所有できない  非常にシンプルな定義だが，僕は気に入っているし，この定義を使って文意解釈に苦難したことはない．要するに，何らかの文脈で「資本主義」というワードが使われた場合，そこでは私的財産の所有を認めるか・認めないかが言及されているのだ．\nNote:\n原理としての「資本主義/共産主義」2元論は非常にシンプルだが，実際の人類史では，これらを現実社会へ制度・システムとして導入し，この原理を駆動させるために，様々な法的・政治的なしくみが提案されている．しかし，再現性がない現象(=史実)については黙るのが科学のルールなので，ここでは立ち入らないこととする．\n資本 - Capital\n資本とは，富を生産するための構成要素を指す抽象概念である．\n 資本：生産要素  資本は，広義では生産要素を指す用語であり，以降で扱うヒト・モノ・カネは全て資本とみなすこともできる．しかし，実際には，その社会的役割や歴史的意味に注意して，これらを分けて扱うことが多い．これは経済学が，自然科学ではなく社会科学たる所以である．すなわち，経済学が扱う範疇である「人間社会」の制度・システムには，法や政治が設計理念として深く関わっており，実際的・実用的な分析を行う際には，これらは分離不可能であるということである．\nNote:\n資本によって，我々が生産しているもの(=富)とは何か？という疑問が残る．結論から言うと，僕はまだ理解できていない（詳しい方に教えて欲しい）．以下に現時点での一応の解釈をまとめておく\n アダム・スミスの時代に，当時の理論家は（抽象的な概念としての）「富」を理解するために思考を重ねた．その結果，国家における「富」を説明する際に，観測可能なもののうち，もっとも妥当な指標は，資本(=生産要素)だった．そのため「国家の富∝国家の生産力」と仮定し，その国の資本に注目した．\n なお，「観測できない事実」については問題の範疇外とするか，あるいは妥当な仮定をおいて探求を進めるのが経験科学の作法なので，ここでは立ち入らないこととする．\n取引と市場 - Trading \u0026amp; markets\n我々が，経済学的視点で社会をみるとき，それはすなわち，我々が「取引」に着目することを意味する．「取引」とは「資本(=生産要素)の所有権の交換」であり，取引の傾向を分析することで，経済のミクロな状態を推定することができる．さらに，取引の集合に対して「市場」という概念を与え，取引の統計的傾向を分析することで，経済のマクロな状態を推定することができる．\n2. 資本主義社会に対する個人の適合戦略 ここからは，資本主義の原理を踏まえた上で，われわれ個人が取るべき戦略について考えてみようと思う．ここでいう「戦略」とは，「行動選択」と同義である．なお，実際の社会で考慮すべき個別具体的な条件は考えず，あくまで資本主義の\u0026quot;原理\u0026quot;を表現するためのモデルとして，ゲームを導入していることに注意してほしい．\n資本主義ゲーム - The capitalist\u0026rsquo;s game\n個人にとって重要な問題は，ミクロな社会状況に対して，科学的な視点をもち，合理的に意思決定・行動選択を行うことである．ここでは，資本主義の原理を踏まえた以下のようなゲームを考えて，個人の適合戦略を考えてみる．\n  社会：ゲーム世界\n  個人：プレイヤー\n 資本：プレイヤーの所有アイテム    資本主義経済：ゲームルール\n 取引：資本の所有権の交換 市場：取引の集合 価値：市場から統計的に推定される各資本の評価値    このゲームにおける各プレイヤーの目標は，「市場価値」が常に最大となるように所有する資本を選択し続けることとなる．ここで，「市場価値」とは，ゲーム参加者によって行われる取引（資本の所有権の交換）データ全体から，リアルタイムで統計的に推定された，各資本の評価値である．\nNote:\n資本価値の定義に「評価値」という表現を使ったが，これは一体何なのか？各資本の評価値は，取引が行われる市場によって異なるが，その算定メカニズムは「需要」と「供給」という抽象概念で説明される．例として，株式市場では，企業の所有権（株式）に対して評価値（株価）を与えているが，これは需要（買い注文）と供給（売り注文）によって算定される．\n代表的な資本: ヒト・モノ・カネ\n資本の分類については，法的分類や会計学的分類など種々である．ここでは，資本をヒト・モノ・カネに分けて，その具体例を考えてみる．\n＊代表的な資本（=私的財産）\n  ヒト（個人）：法的制約を根拠とした価値を持つ．\n​\tcf) 経済取引における行動主体　cf) 法人\n  モノ（消費財，生産財）：物理的制約を根拠とした価値をもつ．\n​\tcf) サービスも含む　ex）金・石油・土地　  カネ（現金，有価証券）：モノ・ヒトとの交換可能性により価値を持つ．\n  ＊代表的な市場（=価値を測る評価関数）\n ヒト：労働市場/雇用市場 モノ：財市場　 カネ：資本市場/金融市場  ＊変換規則\n  ヒト→カネ（労働）\n  カネ→モノ（消費）\n  モノ→ヒト（教育）：ここでいうモノ/サービスとは，知識やスキルを指す\n  モノ→カネ（売却）：ほとんどのモノは，評価されない（中古品市場）\n  →適切な行動選択\n＊市場に根ざした保有資本のポジショニング\n  ヒト（就職，転職，副業）\n  モノ（せどり，車や家，遺産相続）\n  カネ（ポートフォリオ設計）\n  →適切な現状分析\n＊生産力(=価値)は，経過時間に対して成長/減衰する\n  ヒトの価値：UP/DOWN（知識と技術，管理，人間関係の質/量）\n  モノの価値：UP/DOWN（骨董品や古酒，経年劣化）\n  カネの価値：UP/DOWN（物価と金利，経済成長と株価）\n  →適切な未来予測\n＊なぜ，金融市場に着目するべきか？\n カネは，法的・物理的制約の影響がもっとも小さい．  ヒトは法的制約を受ける：Ex.)労働時間の上限，勤務地の選択 モノは物理的制約を受ける：Ex.)石油埋蔵量の上限，人口上限    →カネの市場価値は，定常状態に収束しないため，もっとも変化が激しい．\n","date":1561075200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561075200,"objectID":"202c2d8d41279ef86362d59a029b53ec","permalink":"https://yumaloop.github.io/post/2020-04-05-adaptation-strategies-for-capitalism/","publishdate":"2019-06-21T00:00:00Z","relpermalink":"/post/2020-04-05-adaptation-strategies-for-capitalism/","section":"post","summary":"Thinking of adaptation strategies for the capitalism","tags":["Tips","System","Capitalism"],"title":"A Capitalist's Game","type":"post"},{"authors":null,"categories":["Random"],"content":"Have you ever want to login to keio.jp automatically? Don\u0026rsquo;t you think it is cool? At least I think so and I write down the way to achieve that with Python.\nYour browser does not support the video tag.\n  In order to login to keio.jp (Keio Single Sign-On System), it is necessary to satisfy the page transition as below.\n  https://auth.keio.jp (SSO login page)\n  https://gslbs.adst.keio.ac.jp/student/index.html (Syllabus page)\n  https://www.edu.keio.jp/ess2/login? (Class support page)\n  So, this time, a static web-scraping library like BeautifulSoup is not enough, because it doesn\u0026rsquo;t support the dynamic site with Javascript or page redirection. Then I use Selenium and ChromeDriver in python.\nExample 1 : Auto login to keio.jp If you are the student in Keio University, you can login to keio.jp automatically. All you need is to assign your email address and password in the below script and run it in command line.\nfrom selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC from selenium.common.exceptions import TimeoutException ID = \u0026quot;*****\u0026quot; # your email in keio.jp (ex. example@keio.jp) # PW = \u0026quot;*****\u0026quot; # your password in keio.jp # # Optional settings of chrome driver options = webdriver.ChromeOptions() options.add_argument('--headless') # Boot chrome driver driver = webdriver.Chrome(\u0026quot;/usr/local/bin/chromedriver\u0026quot;, options=options) driver.set_page_load_timeout(15) # Time out 15 sec # GET (HTML Page) driver.get(\u0026quot;https://auth.keio.jp\u0026quot;) # Find elements and POST (send keys to the input tag) id_element = driver.find_element_by_name(\u0026quot;j_username\u0026quot;) id_element.send_keys(ID) pw_element = driver.find_element_by_name(\u0026quot;j_password\u0026quot;) pw_element.send_keys(PW) # Click login button login_button = driver.find_element_by_name(\u0026quot;_eventId_proceed\u0026quot;) login_button.click() # GET (HTML Page) driver.get(\u0026quot;https://gslbs.adst.keio.ac.jp/student/index.html\u0026quot;) # GET (HTML Page) driver.get(\u0026quot;https://www.edu.keio.jp/ess2/login?\u0026quot;) # Close chrome driver driver.quit()  Example 2 : Auto login to twitter.com If you have twitter account, you can also login to twitter.com automatically. All you need is to assign your username (@********) and password (********) in the below script and run it in command line.\nfrom selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC from selenium.common.exceptions import TimeoutException USERNAME=\u0026quot;*****\u0026quot; # your username in twitter # PASSWORD=\u0026quot;*****\u0026quot; # your password in twitter # # Optional settings of chrome driver options = webdriver.ChromeOptions() options.add_argument('--headless') # Boot chrome driver driver = webdriver.Chrome(\u0026quot;/usr/local/bin/chromedriver\u0026quot;, options=options) driver.set_page_load_timeout(15) # Time out 15 sec # GET (HTML Page) driver.get(\u0026quot;https://twitter.com/login\u0026quot;) # Find elements and POST (send keys to the input tag) username_element = driver.find_element_by_class_name('js-username-field') username_element.send_keys(USERNAME) password_element = driver.find_element_by_class_name('js-password-field') password_element.send_keys(PASSWORD) # Click login button login_button = driver.find_element_by_css_selector('button.submit.EdgeButton.EdgeButton--primary.EdgeButtom--medium') login_button.click() # Close chrome driver driver.quit()  Example 3 : Auto search in google.com If you refer to Selenium Getting started guide, you can aquire the search result with the keyword \u0026ldquo;cheese\u0026rdquo; at google.com with Selenium on Python. (This requires Selenium WebDriver 3.13 or newer.)\nfrom selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support.expected_conditions import presence_of_element_located # Open web driver (Google Chrome) driver = webdriver.Firefox() wait = WebDriverWait(driver, 10) # GET HTML page source of google.com driver.get(\u0026quot;https://google.com/ncr\u0026quot;) # GET # POST the keyword \u0026quot;cheese\u0026quot; in \u0026quot;q\u0026quot; element in google.com driver.find_element_by_name(\u0026quot;q\u0026quot;).send_keys(\u0026quot;cheese\u0026quot; + Keys.RETURN) # POST first_result = wait.until(presence_of_element_located((By.CSS_SELECTOR, \u0026quot;h3\u0026gt;div\u0026quot;))) # Search result as text print(first_result.get_attribute(\u0026quot;textContent\u0026quot;)) # Close web driver (Google Chrome) driver.quit()  References   The Selenium Browser Automation Project \u0026gt; Getting started \u0026gt; Quick tour\nhttps://selenium.dev/documentation/en/getting_started/quick/#webdriver\n  ","date":1561075200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561075200,"objectID":"ca9f10c0f3d78d0df17137a403cb2bff","permalink":"https://yumaloop.github.io/post/2019-06-21-keiojp-auto-login/","publishdate":"2019-06-21T00:00:00Z","relpermalink":"/post/2019-06-21-keiojp-auto-login/","section":"post","summary":"Have you ever want to login to keio.jp automatically? Don\u0026rsquo;t you think it is cool? At least I think so and I write down the way to achieve that with Python.","tags":["Tips","Python"],"title":"Automatically Login to keio.jp using Selenium on Python","type":"post"},{"authors":null,"categories":["StatML"],"content":"KL-divergence frequently appears in many fields such as statistics and information theory. It is defined as the expected value of logarithmic transformation of likelihood ratio. Note that:\n expected value: weighted integration with probability density. logarithmic transformation: conversion multiplication to linear combination that is suitable for convex optimization and function analysis. likelihood ratio: a measure of likelihood comparison  1. What is KL-divergence? 1.1 Definition 　For any probability distributions $P$ and $Q$, KL-divergence (Kullback-Leibler divergence)1 is defined as follows, using their probability density function $p(x)$ and $q(x)$.\n\\begin{align} D_{KL}( Q \\mid\\mid P ) \u0026amp;:= \\int q(x) \\log \\frac{q(x)}{p(x)} ~dx \\end{align}\n1.2 Basic properties 　KL-divergence has the following properties.\n （non-negative）It has a non-negative range.  \\begin{align} 0 \\leq D_{KL}( Q \\mid\\mid P ) \u0026amp;\\leq \\infty \\end{align}\n （completeness）When it equals to $0$, $P$ and $Q$ are equivalent.  \\begin{align} D_{KL}( Q \\mid\\mid P ) \u0026amp;= 0 ~~ \\Leftrightarrow ~~ P = Q \\end{align}\n （assymmetry）It is not symmetric about $P$ and $Q$.  \\begin{align} D_{KL}( Q \\mid\\mid P ) \u0026amp;\\neq D_{KL}( P \\mid\\mid Q ) \\end{align}\n （absolute continuity）Unless it diverges, $Q$ is absolutely continuous with respect to $P$.  \\begin{align} D_{KL}( Q \\mid\\mid P ) \u0026amp;\\lt \\infty ~~ \\Rightarrow ~~ P \\gg Q \\end{align}\n​\tFor example, calculating KL-divergence2 between two Gaussian distributions gives the following results: It can be seen that the more the shapes between two distributions do not match, the more KL-divergence increases.\n1.3 Is KL-divergence a metrics? ​\tKL-divergence is so important measurement when considering probability and information that it is called by various names depending on the field and context.\n  \u0026ldquo;KL-divergence\u0026rdquo; \u0026ldquo;KL-metrics\u0026rdquo; \u0026ldquo;KL-information\u0026rdquo; \u0026ldquo;Information divergence\u0026rdquo; \u0026ldquo;Information gain\u0026rdquo; \u0026ldquo;Relative entropy\u0026rdquo;   Since KL-divergence is always non-negative, it might be interpreted as the metrics in the space where the probability distributions $P$ and $Q$ exist. However, KL-divergence is not strictly a metric because it only satisfies \u0026ldquo;non-negativity\u0026rdquo; and \u0026ldquo;completeness\u0026rdquo; among the following axioms of metrics.\n Axioms of metrics $d(~)$:\n  non-negativity $d(x, ~ y) \\geq 0$\n  completeness $d(x, ~ y) = 0 ~~ \\Leftrightarrow ~~ x = y$\n  symmetry $d(x, ~ y) = d(y, ~ x)$\n  The triangle inequality $d(x, ~ y) + d(y, ~ z) \\geq d(x, ~ z)$\n   Note that $d()$ is called the distance function or simply distance\nFor example, Euclidean distance, squared distance, Mahalanobis distance, and Hamming distance satisfy these conditions, and can be clearly considered as metrics. On the other hand, KL-divergence is a divergence, not metrics. In mathematics, \u0026ldquo;divergence\u0026rdquo; is an extended concept of \u0026ldquo;metrics\u0026rdquo; that satisfies only non-negativity and completeness among axioms of metrics. By introducing \u0026ldquo;divergence\u0026rdquo;, you can reduce the constraints of axioms of metrics and have a high level of abstraction.\nThe word \u0026ldquo;divergence\u0026rdquo; is generally interpreted as the process or state of diverging; for example, in physics it appears as a vector operator div. There is no Japanese words that corresponds to the meaning of divergence, but it seems that \u0026ldquo;相違度\u0026rdquo;, \u0026ldquo;分離度\u0026rdquo;, \u0026ldquo;逸脱度\u0026rdquo;, \u0026ldquo;乖離度\u0026rdquo; etc. might be used.\nAs an example, let\u0026rsquo;s measure the KL-divergence between two Gaussian distributions $ N (0, 1) $ (blue) and $ N (1, 2) $ (red). In the figure, the left shows KL-divergence from red one as seen from blue one, and the right shows KL-divergence from blue one as seen from red one. Their value are surely different.\nNote that given two Gaussian distribution $p_1,p_2$ as\n$$ \\begin{align} p_1(x) \u0026amp;= \\mathcal{N}(\\mu_1, \\sigma_1^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma_1^2}} \\exp \\left\\{ - \\frac{ {(x - \\mu_1)}^2}{2 \\sigma_1^2} \\right\\} \\\\\np_2(x) \u0026amp;= \\mathcal{N}(\\mu_2, \\sigma_2^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma_2^2}} \\exp \\left\\{ - \\frac{ {(x - \\mu_2)}^2}{2 \\sigma_2^2} \\right\\} \\end{align} $$\nthe following holds.\n$$ \\begin{align} {D}_{KL}(p_1 \\mid\\mid p_2) \u0026amp;= \\int_{-\\infty}^{\\infty} p_1(x) \\log \\frac{p_1(x)}{p_2(x)} dx \\\\\n\u0026amp;= \\log \\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + {( \\mu_1 - \\mu_2 )}^2}{2 \\sigma_2^2} - \\frac{1}{2} \\end{align} $$\nIncidentally, in addition to the KL-divergence, the following is known as a measure of the proximity (or closeness) between two probability distributions.\n The metrics to measure closeness between $q(x)$ and $p(x)$\n $ {\\chi}^2(q ; p) := \\sum_{i=1}^{k} \\frac{ { { p_i - q_i } }^{2} }{p_i}$ ($\\chi^2$-statistics) $ L_1(q ; p) := \\int \\vert q(x) - p(x) \\vert ~ dx$ ($L_1$-norm) $ L_2(q ; p) := \\int { { q(x) - p(x) } }^{2} ~ dx$ ($L_2$-norm) $ I_K(q ; p) := \\int { \\{ \\sqrt{ q(x) } - \\sqrt{ p(x) } \\} }^{2} ~ dx $ (Herringer distance) $ \\mathbb{D}(q ; p) := \\int f \\left( {\\large \\frac{q(x)}{p(x)} } \\right) q(x) ~ dx$ ($f$-divergence) $ I_{\\lambda}(q ; p) := \\int \\left\\{ { \\left( {\\large \\frac{q(x)}{p(x)} } \\right) }^{\\lambda} - 1 \\right\\} q(x) ~ dx$ (Generalized information) $ {D}_{KL}(q ; p) := \\int \\log \\left( {\\large \\frac{q(x)}{p(x)} } \\right) q(x) ~ dx$ (KL-divergence) $ JSD(q \\mid\\mid p) := \\frac{1}{2} {D}_{KL}(q \\mid\\mid \\frac{q+p}{2}) + \\frac{1}{2} {D}_{KL}(p \\mid\\mid \\frac{q+p}{2})$ (JS-divergence)   2. Relatinoship to other measurements 2.1 KL-divergence vs Mutual information 　In information theory, entropy $H(X)$, join entropy $H(X,Y)$, conditional entropy $H(X \\vert Y)$, mutual information $MI(X,Y)$ are defined as follows by using probability density $Pr()$3.\n\\begin{align} H(X) \u0026amp;:= - \\int Pr(x) \\log Pr(x) ~dx \\\\\nH(X,Y) \u0026amp;:= - \\int Pr(x,y) \\log Pr(x,y) ~dy~dx \\\\\nH(X|Y) \u0026amp;:= - \\int Pr(x,y) \\log Pr(x|y) ~dx~dy \\\\\nMI(X,Y) \u0026amp;:= \\int \\int Pr(x,y) \\log \\frac{Pr(x,y)}{Pr(x)Pr(y)} ~dxdy \\end{align}\nFor any two random variable $X$ and $Y$, mutual information $MI(X, Y)$ specifies the mutual (symmetric) dependence between them.\n\\begin{align} MI(X,Y) \u0026amp;= H(X) - H(X|Y) \\\\\n\u0026amp;= H(Y) - H(Y|X) \\\\\n\u0026amp;= H(X) + H(Y) - H(X,Y) \\end{align}\nHere, the following relationship holds between KL-divergence and mutual information.\n\\begin{align} MI(X, Y) \u0026amp;= D_{KL} \\bigl( Pr(x, y) \\mid\\mid Pr(x)Pr(y) \\bigr) \\\\\n\u0026amp;= \\mathbb{E}_{Y} \\bigl[ D_{KL} \\bigl( Pr(x|y) \\mid\\mid Pr(x) \\bigr) \\bigr] \\\\\n\u0026amp;= \\mathbb{E}_{X} \\bigl[ D_{KL} \\bigl( Pr(y|x) \\mid\\mid Pr(y) \\bigr) \\bigr] \\end{align}\nSo that, mutual information $MI (X, Y)$ is interpreted as the degree of difference (average degree of deviation) between the joint distribution $Pr (x, y)$ when the $X$ and $Y$ are not independent and the joint distribution $Pr (x) Pr (y)$ when $X$ and $Y$ are independent.\n（cf.）Formula transformation of mutual information:\n\\begin{align} MI(X,Y) \u0026amp;= \\int \\int Pr(x,y) \\log \\frac{Pr(x,y)}{Pr(x)Pr(y)} ~dxdy \\\\\n\u0026amp;= \\int \\int Pr(x|y)Pr(y) \\log \\frac{Pr(x|y)Pr(y)}{Pr(x)Pr(y)} ~dxdy \\\\\n\u0026amp;= \\int Pr(y) \\int Pr(x|y) \\log \\frac{Pr(x|y)}{Pr(x)} ~dx~dy \\\\\n\u0026amp;= \\int Pr(y) \\cdot D_{KL} \\bigl( Pr(x|y) \\mid\\mid Pr(x) \\bigr) ~dy \\\\\n\u0026amp;= \\mathbb{E}_{Y} \\bigl[ D_{KL} \\bigl( Pr(x|y) \\mid\\mid Pr(x) \\bigr) \\bigr] \\end{align}\n2.2 KL-divergence vs Log likelihood ratio In the field of Bayes inference and statistical modeling, you often face the problem of estimating the true distribution $q(x)$ by $p_{\\hat{\\theta}}(x)$ (that is the combination of stochastic model $p_{\\theta}(x)$ and estimated parameter $\\hat{\\theta}$ ) . Therefore, KL-divergence is used when you want to measure the difference between two distributions, or when you want to incorporate the estimation error into the loss function or risk function in order to solve the optimization problem for the parameter $\\theta$.\nAlso, KL-divergence is related to the log likelihood ratio so much that it has a deep connection to the model selection method 4 such as likelihood ratio test, Bayes factor, and AIC (Akaike\u0026rsquo;s information criterion).\n KL-divergence of estimated distribution $p_{\\theta}(x)$ for the true distribution $q(x)$ : $D_{KL}(q \\mid\\mid p_{\\theta})$ is considerd as the expected value of the log likelihood ratio $q(x)/p_{\\theta}(x)$ for tue true distribution $q(x)$.  \\begin{align} \\left( \\text{Log likelihood ratio} \\right) \u0026amp;= \\log \\frac{q(x)}{p_{\\theta}(x)} \\\\\nD_{KL}( q \\mid\\mid p_{\\theta} ) \u0026amp;:= \\int q(x) \\log \\frac{q(x)}{p_{\\theta}(x)} ~dx \\\\\n\u0026amp;= \\mathbb{E}_{X} \\left[ \\log \\frac{q(x)}{p_{\\theta}(x)} \\right] \\left(\\text{Expected log likelihood ratio} \\right) \\end{align}\nWhen using KL-divergence as the evaluation/loss value in model selection/comparison, it is equivalent that minimizing KL-divergence: $D_{KL}( q \\mid\\mid p )$ and maximizing the log likelihood: $\\log p(x)$ as follows.\n\\begin{align} D_{KL}( q \\mid\\mid p_{\\theta} ) \u0026amp;= \\mathbb{E}_{X} \\bigl[ \\log q(x) \\bigr] - \\mathbb{E}_{X} \\bigl[ \\log p_{\\theta}(x) \\bigr] \\\\\n\u0026amp;\\propto - \\mathbb{E}_{X} \\bigl[ \\log p_{\\theta}(x) \\bigr] \\left(-1 \\cdot \\text{ Expected log likelihood} \\right) \\end{align}\n  For any parametric stochastic model $f(x \\vert \\theta)$ (such as a linear regression model) which represents the estimated distribution as\n\\begin{align} p_{\\theta}(x) = f(x|\\theta) \\end{align}\n, if a certain loss function $L(\\theta)$ is given, the optimal parameter $\\theta^*$ exists as it satisfy the following.\n\\begin{align} q(x) \u0026amp;= f(x|\\theta^*) \\end{align}\nThen, for any estimated parameter $\\hat{\\theta}$ ,the estimated loss of the model $f(x \\vert \\hat{\\theta})$ is represented by KL-divergence. (Note that $\\ell( \\cdot \\vert x)$ means the log likelihood function.)\n  \\begin{align} \\left( \\text{Log likelihood ratio} \\right) \u0026amp;= \\log \\frac{f(x|\\theta^{*})}{f(x|\\hat{\\theta})} \\end{align}\n\\begin{align} \\hat{\\theta} \u0026amp;:= \\underset{\\theta \\in \\Theta}{\\rm argmin} ~ L(\\theta) \\tag{7} \\\\\nD_{KL}( q \\mid\\mid p_{\\hat{\\theta}} ) \u0026amp;= D_{KL}( p_{\\theta^{*}} \\mid\\mid p_{\\hat{\\theta}} ) \\\\\n\u0026amp;= D_{KL}( f_{\\theta^{*}} \\mid\\mid f_{\\hat{\\theta}} ) \\\\\n\u0026amp;= \\int f(x|\\theta^{*}) \\log \\frac{f(x|\\theta^{*})}{ f(x|\\hat{\\theta})} dx \\\\\n\u0026amp;= \\mathbb{E}_{X} \\left[ \\log \\frac{ f(x|\\theta^{*}) }{ f(x|\\hat{\\theta}) } \\right] \\\\\n\u0026amp;= \\mathbb{E}_{X} \\bigl[ \\ell( \\theta_{0}|x ) \\bigr] - \\mathbb{E}_{X} \\bigl[ \\ell( \\hat{\\theta} | x ) \\bigr] \\end{align}\n2.3 KL-divergence vs Fisher information Given a certain stochastic model $f(\\cdot \\vert \\theta)$, Fisher information $I(\\theta)$ for the parameter $\\theta$ is defined as follows. (Note that $ \\ell( \\cdot \\vert x) $ means the log likelihood function.)\n\\begin{align} I(\\theta) \u0026amp;:= \\mathbb{E}_{X} \\left[ { \\left\\{ \\frac{d}{dx} \\ell(\\theta \\vert x) \\right\\} }^{3} \\right] \\\\\n\u0026amp;= \\mathbb{E}_{X} \\left[ { \\left\\{ \\frac{d}{dx} \\log f(x|\\theta) \\right\\} }^{2} \\right] \\end{align}\nAlso, between KL-divergence and Fisher information, the following holds.\n\\begin{align} \\lim_{h \\to 0} \\frac{1}{h^{2}} D_{KL} \\bigl( f(x|\\theta) \\mid\\mid f(x|\\theta+h) \\bigr) \u0026amp;= \\frac{1}{2} I(\\theta)\n\\end{align}\n(cf.) The following equation holds by using Taylor expansion of $\\ell( \\cdot \\vert x)$.\n\\begin{align} \\ell(\\theta + h) - \\ell(\\theta) \u0026amp;= {\\ell}^{'}(\\theta)h + \\frac{1}{2} {\\ell}^{''}(\\theta) h^{2} + O(h^{3}) \\end{align}\nThis formula indicates that in parameter space $\\Theta$, for all point $ \\theta \\in \\Theta $ ant its neighborring point $ \\theta + h $, their KL-divergence：$ D_{KL} ( f(x \\vert \\theta) \\mid\\mid f(x \\vert \\theta+h) )$ is **directly proportional to** Fisher information $I(\\theta)$. After all, Fisher information $ I(\\theta)$ measures **the local information** that the stochastic model $f(\\cdot \\vert \\theta)$ has at the point $\\theta$.\n3. References    \n  Also, f-divergence is defined as its generalized class. \u0026#x21a9;\u0026#xfe0e;\n I used scipy.stats.entropy(). \u0026#x21a9;\u0026#xfe0e;\n Although thermodynamic entropy is originated in Boltzmann, the historical background of Shannon information is mentioned below link. There seems to be a reference flow: Hartley → Nyquist → Shannon. http://www.ieice.org/jpn/books/kaishikiji/200112/200112-9.html \u0026#x21a9;\u0026#xfe0e;\n Article on gneralized information criterion(GIC): https://www.ism.ac.jp/editsec/toukei/pdf/47-2-375.pdf \u0026#x21a9;\u0026#xfe0e;\n   ","date":1524096000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524096000,"objectID":"27a281007a11aade3bc5bd85329a41fd","permalink":"https://yumaloop.github.io/post/2018-04-19-kl-divergence/","publishdate":"2018-04-19T00:00:00Z","relpermalink":"/post/2018-04-19-kl-divergence/","section":"post","summary":"This is a basic notebook for KL-divergence which frequently appears in many fields such as statistics and information theory.","tags":["Bayes"],"title":"Kullback-Leibler Divergence","type":"post"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://yumaloop.github.io/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"}]