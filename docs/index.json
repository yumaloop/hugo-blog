[{"authors":null,"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n  Download my resumÃ©.\n","date":1461110400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1607817600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Nelson Bighetti","type":"authors"},{"authors":null,"categories":null,"content":"Iâ€™m Yuma Uchiumi, a graduate student majoring in computer science. I was born on December 8, 1997 in Tokyo, Japan. see CV My research goal is to understand and implement the computation for human thinking; I study statistical algorithms, stochastic models, and information processing systems to solve the cognitive problems with human thinking. ğŸ‘» ğŸ­ ğŸ¦„ ğŸŒˆ â›… âœ¨\n  Download my resumÃ©.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a9a3490cc16efb06535c8144a9c51396","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Iâ€™m Yuma Uchiumi, a graduate student majoring in computer science. I was born on December 8, 1997 in Tokyo, Japan. see CV My research goal is to understand and implement the computation for human thinking; I study statistical algorithms, stochastic models, and information processing systems to solve the cognitive problems with human thinking.","tags":null,"title":"Yuma Uchiumi","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://yumaloop.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":null,"categories":["Personal"],"content":"StackOverflow published its Developer Survey 2020. Click here for details.\nIn 2020, Python, Go, TypeScript, Rust are the programming language most loved by developers in the world. It means that these languages are in vogue and new learners are on the rise. While JavaScript swallows everything, Perl, PHP, and Ruby seem to be still in high demand.\nFrameworks packaging frontends, backends and database into one, such as Rails, are already not preferred. Today, dividing frontends with JavaScript (Node.js, Vue.js, React.js) and backends/database with scalable cloud services (AWS, GCP) is major approach for adaptive web applications.\nPython and Java have always been very popular and in demand as general languages that can be used for many purposes. In the future, it will be interesting to see if Go and Rust can take their place.\nFinally, I would like to introduce the cluster map of major development technologies found by the StackOverflow Developer Survey 2020. With the penetration of new technologies such as mobile, container and cloud computing, it will be more difficult to become a full-stack engineer in the near future.\n Assembly, C, C++ Raspberry Pi, Arduino Unity, UnrealEngine Hadoop, Scala, Apache Spark Python, Pandas, Torch/PyTorch Linux, Docker, Kubernetes, Bash/Shell AWS, Redis, Ansible, DynamoDB, PostgreSQL JavaScript, Node.js, React.js, Angular, TypeScript, MongoDB, PHP, MySQL, jQuery, WordPress Java, Swift, Android, iOS, Kotlin, SQLite, Firebase C#, .NET, Windows, Azure  ","date":1608422400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608422400,"objectID":"67cbf9f559aa5d077627a7dcf5a3a6ab","permalink":"https://yumaloop.github.io/post/2020-12-20-stackoverflow-developer-survey-2020/","publishdate":"2020-12-20T00:00:00Z","relpermalink":"/post/2020-12-20-stackoverflow-developer-survey-2020/","section":"post","summary":"StackOverflow published its Developer Survey 2020. Click here for details.\nIn 2020, Python, Go, TypeScript, Rust are the programming language most loved by developers in the world. It means that these languages are in vogue and new learners are on the rise.","tags":["Tips","Python","Go","Java","React"],"title":"StackOverflow - 2020 Developer Survey","type":"post"},{"authors":null,"categories":["Finance"],"content":"å¹¾ä½•ãƒ–ãƒ©ã‚¦ãƒ³é‹å‹•ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‘ã‚¹ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šï¼Œæ ªä¾¡ãŒå¹¾ä½•ãƒ–ãƒ©ã‚¦ãƒ³é‹å‹•ã«ã—ãŸãŒã†å ´åˆã®ã‚³ãƒ¼ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä¾¡æ ¼ã‚’ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚Šå°å‡ºã—ï¼Œã‚µãƒ³ãƒ—ãƒ«ãƒ‘ã‚¹ã®ç”Ÿæˆå›æ•°ã‚’å¢—ã‚„ã™ã“ã¨ã§ç†è«–å€¤ã«åæŸã™ã‚‹ã“ã¨ã‚’ãƒ—ãƒ­ãƒƒãƒˆã«ã‚ˆã‚Šç¢ºèªã™ã‚‹ï¼\n1. ã‚³ãƒ¼ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ä¾¡æ ¼ç†è«– ãƒ–ãƒ©ãƒƒã‚¯ã‚·ãƒ§ãƒ¼ãƒ«ã‚ºå¼ã«ã‚ˆã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä¾¡æ ¼ã®å°å‡º æº€æœŸ$T$ã§åŸè³‡ç”£ä¾¡æ ¼(æ ªå¼ä¾¡æ ¼)ãŒé€£ç¶šæ™‚é–“ç¢ºç‡éç¨‹$S = {(S_t)}_{t \\in [0,T]}$ã«å¾“ã†ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®æ™‚åˆ»$t \\in [0, T]$ã«ãŠã‘ã‚‹ä¾¡æ ¼$C(t, S_t)$ã«ã¤ã„ã¦è€ƒãˆã‚‹ï¼$S$ãŒç¢ºç‡å¾®åˆ†æ–¹ç¨‹å¼:\n$$ \\begin{align} d S_t = \\sigma S_t dt + \\mu S_t d W_t \\end{align} $$\nã®è§£ã§ä¸ãˆã‚‰ã‚Œã¦ã„ã‚‹ã¨ã™ã‚‹($S$ã¯å¹¾ä½•ãƒ–ãƒ©ã‚¦ãƒ³é‹å‹•ã«å¾“ã†)ï¼æ™‚åˆ»$t \\in [0, T]$ã«ãŠã„ã¦ï¼Œã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®åŸè³‡ç”£ä¾¡æ ¼$S_t$ã¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®è¡Œä½¿ä¾¡æ ¼$K$ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãï¼Œãƒ–ãƒ©ãƒƒã‚¯ãƒ»ã‚·ãƒ§ãƒ¼ãƒ«ã‚ºå¼ã«ã‚ˆã£ã¦ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ç†è«–ä¾¡æ ¼$C(t, S_t)$ã¯\n$$ \\begin{align} C(t, S_t) \u0026amp;= S_t \\Phi(d_1) - K e^{-r(T-t)} \\Phi(d_2) \\\\\nwhere ~~ d_1 \u0026amp;= \\frac{\\log \\left( \\frac{S_t}{K} \\right) + \\left( r + \\frac{\\sigma^2}{2} \\right) T}{\\sigma \\sqrt{T}} \\\\\nd_2 \u0026amp;= \\frac{\\log \\left( \\frac{S_t}{K} \\right) + \\left( r - \\frac{\\sigma^2}{2} \\right) T}{\\sigma \\sqrt{T}} \\end{align} $$\nã¨ãªã‚‹ï¼ãŸã ã—ï¼Œ$r$ã¯ç„¡ãƒªã‚¹ã‚¯è³‡ç”£ã®åˆ©ç‡ï¼Œ$\\Phi$ã¯æ¨™æº–æ­£è¦åˆ†å¸ƒ$\\mathcal{N}(0,1)ã®ç´¯ç©åˆ†å¸ƒé–¢æ•°ã¨ã™ã‚‹ï¼ã“ã“ã§ï¼Œç°¡å˜ã®ãŸã‚ã«æº€æœŸ$T$ã‚’$1$ã¨ã™ã‚‹ã¨ï¼Œç¾åœ¨($t=0$)ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä¾¡æ ¼ã®ç†è«–å€¤ã¯ï¼Œ\n$$ \\begin{align} C(0, S_0) \u0026amp;= S_0 \\Phi(d_1) - K e^{-r} \\Phi(d_2) \\\\\nwhere ~~ d_1 \u0026amp;= \\frac{\\log \\left( \\frac{S_0}{K} \\right) + \\left( r + \\frac{\\sigma^2}{2} \\right)}{\\sigma } \\\\\nd_2 \u0026amp;= \\frac{\\log \\left( \\frac{S_0}{K} \\right) + \\left( r - \\frac{\\sigma^2}{2} \\right)}{\\sigma} \\end{align} $$\nã¨ãªã‚‹ï¼\nãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã«ã‚ˆã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä¾¡æ ¼ã®å°å‡º ãƒ–ãƒ©ãƒƒã‚¯ã‚·ãƒ§ãƒ¼ãƒ«ã‚ºå¼ã«å«ã¾ã‚Œã‚‹$S_T$ã®æœŸå¾…å€¤è¨ˆç®—ã‚’ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã«ã‚ˆã£ã¦è¿‘ä¼¼ã™ã‚‹ã“ã¨ã‚’è€ƒãˆã‚‹ï¼ã™ãªã‚ã¡ï¼Œå¹¾ä½•ãƒ–ãƒ©ã‚¦ãƒ³é‹å‹•ã«å¾“ã†ã‚µãƒ³ãƒ—ãƒ«ãƒ‘ã‚¹$S = {(S_t)}_{t \\in [0,T]}$ã‚’å¤§é‡ã«ç”Ÿæˆã™ã‚‹ã“ã¨ã§ï¼Œ$S_T$ã®æœŸå¾…å€¤ã‚’æ±‚ã‚ã‚‹ï¼\nãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚ŒãŸï¼Œæº€æœŸ$T$ã«ãŠã‘ã‚‹åŸè³‡ç”£ä¾¡æ ¼$S_T$ã®$n$å€‹ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’$(s^{(1)}{T}, \\cdots, s^{(n)}{T})$ã¨ã™ã‚‹ã¨ï¼Œæ™‚åˆ»$t \\in [0, T]$ã«ãŠã‘ã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ä¾¡æ ¼ã®æ¨å®šå€¤$\\hat{C}(t, S_t)$ã¯\n$$ \\begin{align} \\hat{C}(t, S_t) \u0026amp;= \\frac{1}{n} \\sum_{i=1}^{n} e^{-r(T-t)} \\cdot max(s^{(i)}_{T} - K, 0) \\end{align} $$\nã¨æ±‚ã‚ã‚‰ã‚Œã‚‹ï¼ã“ã“ã§ï¼Œç°¡å˜ã®ãŸã‚ã«æº€æœŸ$T$ã‚’$1$ã¨ã™ã‚‹ã¨ï¼Œç¾åœ¨($t=0$)ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä¾¡æ ¼ã®æ¨å®šå€¤ã¯ï¼Œ\n$$ \\begin{align} \\hat{C}(0, S_0) \u0026amp;= \\frac{1}{n} \\sum_{i=1}^{n} e^{-r} \\cdot max(s^{(i)}_{1} - K, 0) \\end{align} $$\nã¨ãªã‚‹ï¼\n2. Rã§ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ YUIMAãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆor èª­ã¿è¾¼ã¿ï¼‰ã™ã‚‹ï¼\ninstall.packages(\u0026quot;yuima\u0026quot;) # ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« (åˆå›ã®ã¿) library(yuima) # ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿  ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã™ã‚‹ã‚³ãƒ¼ãƒ‰ï¼\n# Calculation of call-option prices by Black-Sholes eq. BlackScholesCallPrice = function(S, K, r, sigma, T=1) { d1 \u0026lt;- ( log(S/K) + (r + sigma^2/2) * T)/( sigma * sqrt(T)) d2 \u0026lt;- ( log(S/K) + (r - sigma^2/2) * T)/( sigma * sqrt(T)) C0 \u0026lt;- S * pnorm(d1) - K * exp(-r * T) * pnorm(d2) return(C0) } # Calculation of call-option prices by Monte Carlo method MonteCarloCallPrice = function(S, K, r, sigma, n, T=1) { n_sample \u0026lt;- 1000 c0_list \u0026lt;- list() c0 \u0026lt;- 0 for (i in 1:n) { resultGBM \u0026lt;- GBM_sample(S, r, sigma, n_sample) sT \u0026lt;- resultGBM@data@original.data[n_sample] c0 \u0026lt;- (1/i) * exp(-1*r*T) * max(sT - K, 0) + ((i-1)/i) * c0 c0_list \u0026lt;- append(c0_list, list(c0)) } return(c0_list) } # A function which generates sample paths that follows a Geometric Brownian motion GBM_sample = function(x0, alpha, beta, n_sample, T=1) { # Step1: Define SDE # dS_t = alpha * S_t * dt + beta * S_t * dW_t mod \u0026lt;-setModel(drift=\u0026quot;alpha*x\u0026quot;, diffusion=\u0026quot;beta*x\u0026quot;) # Step2: Define samples samp \u0026lt;-setSampling(Initial=0, Terminal=T, n=n_sample) # Step3: define the statistical model smod \u0026lt;-setYuima(model=mod, sampling=samp) # Step4: Generate sample paths xinit \u0026lt;- x0 param \u0026lt;- list(alpha=alpha, beta=beta) resultGBM \u0026lt;- simulate(smod, xinit=xinit, true.parameter=param) return(resultGBM) } # Params for call-option pricing n \u0026lt;- 10000 # Num. of MonteCalro simulation K \u0026lt;- 900 # Option exercise price (at t=T) S \u0026lt;- 1000 # Curent asset price (at t=0) r \u0026lt;- 0.005 # Drift for Geometric Brownian motion sigma \u0026lt;- 0.3 # Diffusion for Geometric Brownian motion T \u0026lt;- 1 # Optional term # Main procedure: Run simulation bs_price \u0026lt;- BlackScholesCallPrice(S, K, r, sigma, T=1) mc_price \u0026lt;- MonteCarloCallPrice(S, K, r, sigma, n, T=1) print(bs_price) plot(1:n, mc_price, main=\u0026quot;Monte Carlo Simulation:\\nBlack-Scholes Option Pricing Model\u0026quot;, xlab=\u0026quot;Number of sample paths: # of ST\u0026quot;, ylab=\u0026quot;Option Price: C0\u0026quot;, cex=0.5) abline(h=bs_price, col='red', lwd=1, lty=2)  Case 1.\nãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã«ã‚ˆã‚‹ã‚³ãƒ¼ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®æ¨å®šä¾¡æ ¼ãŒï¼Œ BSå¼ã«ã‚ˆã‚‹ç†è«–ä¾¡æ ¼172.7457ã«æ¼¸è¿‘ã—ã¦ã„ã‚‹ï¼\n ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š  n: 10000 # ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®å›æ•° K: 900 # ã‚³ãƒ¼ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®æ¨©åˆ©è¡Œä½¿ä¾¡æ ¼ (t=T) S: 1000 # æ ªå¼ã®ç¾åœ¨ä¾¡æ ¼ (t=0) r: 0.005 # å¹¾ä½•ãƒ–ãƒ©ã‚¦ãƒ³é‹å‹•ã®drift sigma: 0.3 # å¹¾ä½•ãƒ–ãƒ©ã‚¦ãƒ³é‹å‹•ã®diffusion T: 1 # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®æº€æœŸ    Case 2.\nãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã«ã‚ˆã‚‹ã‚³ãƒ¼ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®æ¨å®šä¾¡æ ¼ãŒï¼Œ BSå¼ã«ã‚ˆã‚‹ç†è«–ä¾¡æ ¼83.1821ã«æ¼¸è¿‘ã—ã¦ã„ã‚‹ï¼\n ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š  n: 10000 # ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®å›æ•° K: 1100 # ã‚³ãƒ¼ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®æ¨©åˆ©è¡Œä½¿ä¾¡æ ¼ (t=T) S: 1000 # æ ªå¼ã®ç¾åœ¨ä¾¡æ ¼ (t=0) r: 0.005 # å¹¾ä½•ãƒ–ãƒ©ã‚¦ãƒ³é‹å‹•ã®drift sigma: 0.3 # å¹¾ä½•ãƒ–ãƒ©ã‚¦ãƒ³é‹å‹•ã®diffusion T: 1 # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®æº€æœŸ    ","date":1606262400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606262400,"objectID":"43daa3b26ad2dd6a975fc56f9cabb805","permalink":"https://yumaloop.github.io/post/2020-11-25-monte-carlo-sim-bsoption-r/","publishdate":"2020-11-25T00:00:00Z","relpermalink":"/post/2020-11-25-monte-carlo-sim-bsoption-r/","section":"post","summary":"å¹¾ä½•ãƒ–ãƒ©ã‚¦ãƒ³é‹å‹•ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‘ã‚¹ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šï¼Œæ ªä¾¡ãŒå¹¾ä½•ãƒ–ãƒ©ã‚¦ãƒ³é‹å‹•ã«ã—ãŸãŒã†å ´åˆã®ã‚³ãƒ¼ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä¾¡æ ¼ã‚’ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚Šå°å‡ºã—ï¼Œã‚µãƒ³ãƒ—ãƒ«ãƒ‘ã‚¹ã®ç”Ÿæˆå›æ•°ã‚’å¢—ã‚„ã™ã“ã¨ã§ç†è«–å€¤ã«åæŸã™ã‚‹ã“ã¨ã‚’ãƒ—ãƒ­ãƒƒãƒˆã«ã‚ˆã‚Šç¢ºèªã™ã‚‹ï¼","tags":["Tips","R"],"title":"Monte Carlo Simulation of Black-Scholes Option Pricing Model","type":"post"},{"authors":null,"categories":["Personal"],"content":"   Simon Wood, a leading statistician in UK being famous for the text book of the GAMs introduced the lockdown reading list in his homepage. It looks interesting, so I\u0026rsquo;ll share it here. (most of them are Blackwell\u0026rsquo;s, books)\n Collapse (Jared Diamond) on how societies are destroyed, not by external forces, but by their failure to adapt their cultural norms to those forces. Thinking Fast and Slow (Daniel Kahneman) on the pitfalls of our intuitive reasoning, especially about risk and uncertainty. Mistakes were made, but not by me (Carol Tarvis and Elliot Aronson) on the psychology of sticking with bad decisions. The Parable of the Old Man and the Young by Wilfred Owen, on consequences of the above. Economics The User\u0026rsquo;s Guide (Ha-Joon Chang) on what you really need to know about economics, and how it isn\u0026rsquo;t just a scaled up version of household accounting. The Great Crash 1929 (Galbraith) a delightful disection of economic hubris (and the need for stabilizing controls that we long since did away with). The Rise and Fall of the Third Reich (William Shirer) detailing exactly how things went wrong in Germany after the Great Depression. Witch hunting in Scotland (Brian Levack) on the Scottish experience of the great European witchcraft panic (James I/VI wrote a treatise on Witchcraft). Wood and Thomas paper on the problems of prediction with disease models in the absense of direct validation data (the least impressive item here).  ","date":1593475200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593475200,"objectID":"93cba2be93672db8c2b1de0e9060b5a1","permalink":"https://yumaloop.github.io/post/2020-06-30-sw-lockdown-reading-list/","publishdate":"2020-06-30T00:00:00Z","relpermalink":"/post/2020-06-30-sw-lockdown-reading-list/","section":"post","summary":"Simon Wood, a leading statistician in UK being famous for the text book of the GAMs introduced the lockdown reading list in his homepage. It looks interesting, so I\u0026rsquo;ll share it here.","tags":["Tips","Books"],"title":"Simon N Wood's \"lockdown reading list\"","type":"post"},{"authors":null,"categories":["Finance"],"content":"   Self-discriptive system within time change.\nThe simplest example of self-discriptive systems is the exponential increase. This is because the simplest \u0026ldquo;change\u0026rdquo; of a variable $x$ is a first derivative of its time $dx/dt$, and the simplest form of the function \u0026ldquo;$f(x)$\u0026rdquo; determined by a variable $x$ is a linear one $Cx$.\n Case1: Growth is constant with the current state (state is invariant over time)  $$ \\frac{dx}{dt} = C $$\n Case2: Growth is linear with the current state (state changes exponentially)  $$ \\frac{dx}{dt} = Cx $$\n Case3: Growth is complex but relative to the current state (generalization)  $$ \\frac{dx}{dt} = f(x) $$\nActual situations 1. Money/Capital creates new money/capital\nThis is the principle of capital markets and investment. Or what is called Capital Gain. Source of motivation on the lender side.\nã“ã‚Œã¯è³‡æœ¬å¸‚å ´ã¨æŠ•è³‡ã®åŸç†ï¼ã„ã‚ã‚†ã‚‹ã‚­ãƒ£ãƒ”ã‚¿ãƒ«ã‚²ã‚¤ãƒ³ï¼è²¸ã—æ‰‹ã®å‹•æ©Ÿã®æºæ³‰ï¼\n2. Trust/Credit creates new trust/credit\nThe most obvious example is banking. Actually, there is a word of credit creation. Lending is gradually increased based on credit. This is the principle of the borrower.\nã‚ã‹ã‚Šã‚„ã™ã„ä¾‹ã¯éŠ€è¡Œæ¥­ï¼ å®Ÿéš›ï¼Œä¿¡ç”¨å‰µé€ ã¨ã„ã†è¨€è‘‰ãŒã‚ã‚‹ãã‚‰ã„ï¼ä¿¡ç”¨ã‚’ã‚‚ã¨ã«ï¼Œå°‘ã—ãšã¤è²¸å‡ºã‚’å¢—ã‚„ã™ï¼ã“ã‚Œã¯å€Ÿã‚Šæ‰‹ã®åŸç†ï¼\nThe modern financial system is supported by the principle that capital and credit increase/decrease exponentially. This principle creates a dynamic phenomenon (spiral) that motivates lenders and borrowers and that \u0026ldquo;lending and borrowing exponentially increases/decreases.\u0026rdquo; This is inflation and deflation.\nç¾ä»£é‡‘èã‚·ã‚¹ãƒ†ãƒ ã¯ï¼Œã€Œè³‡æœ¬ã¨ä¿¡ç”¨ãŒæŒ‡æ•°å¢—åŠ /æ¸›å°‘ã™ã‚‹ã€ã¨ã„ã†åŸç†ã«ã‚ˆã£ã¦æ”¯ãˆã‚‰ã‚Œã‚‹ï¼ã“ã®åŸç†ã¯ï¼Œè²¸ã—æ‰‹ã¨å€Ÿã‚Šæ‰‹ã«å‹•æ©Ÿã‚’ä¸ãˆã€ŒæŒ‡æ•°å¢—åŠ /æ¸›å°‘ã™ã‚‹è²¸é‡‘ã¨å€Ÿé‡‘ã€ã¨ã„ã†ç¾è±¡ï¼ˆã‚¹ãƒ‘ã‚¤ãƒ©ãƒ«ï¼‰ã‚’ç”Ÿã‚€ï¼ã“ã‚ŒãŒã‚¤ãƒ³ãƒ•ãƒ¬ã¨ãƒ‡ãƒ•ãƒ¬ï¼\nA vested interest (a large corporation or a large political party) can be established as a vested interest by reproducing credit and achievement in the future by accumulating past credit and achievement.\næ—¢å¾—æ¨©ç›Šï¼ˆå¤§ä¼æ¥­ã‚„å¤§æ”¿å…šï¼‰ã¯ï¼Œéå»ã®ä¿¡ç”¨ã¨å®Ÿç¸¾ã®ç©ã¿é‡ã­ã«ã‚ˆã‚Šï¼Œå°†æ¥ã®ä¿¡ç”¨ã¨å®Ÿç¸¾ã‚’å†ç”Ÿç”£ã™ã‚‹ã“ã¨ã§ï¼Œæ—¢å¾—æ¨©ç›Šã¨ã—ã¦æˆç«‹ã™ã‚‹ï¼\nTo make matters difficult, \u0026ldquo;trust\u0026rdquo; and \u0026ldquo;achievement\u0026rdquo; are complementary. \u0026ldquo;Trust\u0026rdquo; provides an opportunity to unlock new achievements. The new â€œachievementâ€ repairs and strengthens trust. In other words, vested interests are invincible unless environmental changes occur.\nå„ä»‹ãªã“ã¨ã«ã€Œä¿¡é ¼ã€ã¨ã€Œå®Ÿç¸¾ã€ã¯ç›¸è£œé–¢ä¿‚ã«ã‚ã‚‹ï¼ã€Œä¿¡é ¼ã€ã¯æ–°ãŸãªå®Ÿç¸¾è§£é™¤ã¸ã®æ©Ÿä¼šã‚’æä¾›ã™ã‚‹ï¼æ–°ãŸãªã€Œå®Ÿç¸¾ã€ã¯ä¿¡é ¼ã‚’è£œä¿®ãƒ»å¼·åŒ–ã™ã‚‹ï¼ã¤ã¾ã‚Šï¼Œæ—¢å¾—æ¨©ç›Šã¯ç’°å¢ƒå¤‰åŒ–ãŒèµ·ããªã„é™ã‚Šç„¡æ•µï¼\nThe first step for new powers (ventures and youth) to scale up is to win trust or to make some achievements. In addition, it is even better as it causes environmental changes and innovation.\næ–°èˆˆå‹¢åŠ›ï¼ˆãƒ™ãƒ³ãƒãƒ£ãƒ¼ã‚„è‹¥è€…ï¼‰ãŒã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ã™ã‚‹ãŸã‚ã®ç¬¬ä¸€æ­©ã¯ã€Œä¿¡é ¼ã€ã‚’å‹ã¡å–ã‚‹ã‹ï¼Œã€Œå®Ÿç¸¾ã€ã§é»™ã‚‰ã›ã‚‹ã‹ï¼ åŠ ãˆã¦ï¼Œç’°å¢ƒå¤‰åŒ–ã‚„ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã‚’èµ·ã“ã™ã¨ãªãŠè‰¯ã„ï¼\nAs a result, The risk-loving youths first try to walk the cycle from the trust to the achievement, but The risk-averse youths first try to walk the cycle from the achievement to the trust.\nçµæœã¨ã—ã¦ï¼Œ ãƒªã‚¹ã‚¯é¸å¥½çš„ãªè‹¥è€…ã¯ï¼Œä¿¡é ¼ã‹ã‚‰å®Ÿç¸¾ã¸ã¨ã„ã†ã‚µã‚¤ã‚¯ãƒ«ã‚’æ­©ã‚‚ã†ã¨ã—ï¼Œ ãƒªã‚¹ã‚¯å›é¿çš„ãªè‹¥è€…ã¯ï¼Œå®Ÿç¸¾ã‹ã‚‰ä¿¡é ¼ã¸ã¨ã„ã†ã‚µã‚¤ã‚¯ãƒ«ã‚’æ­©ã‚‚ã†ã¨ã™ã‚‹ï¼\n4. Follower/Fan creates new follower/fan\nIn order for super popular corporate brands, entertainers, celebrities, and influencers to be born, their fans/followers need to create new fans/followers.\nè¶…äººæ°—ãªä¼æ¥­ãƒ–ãƒ©ãƒ³ãƒ‰ã‚„èŠ¸èƒ½äººï¼Œã‚»ãƒ¬ãƒ–ï¼Œã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ãŒç”Ÿã¾ã‚Œã‚‹ãŸã‚ã«ã¯ï¼Œå½¼ã‚‰ã®ãƒ•ã‚¡ãƒ³/ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ãŒæ–°ãŸãªãƒ•ã‚¡ãƒ³/ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ã‚’ç”Ÿã‚€å¿…è¦ãŒã‚ã‚‹ï¼\nNowadays, this mechanism has been strengthened in the entertainment industry where globalization by the Internet has been achieved, and in platforms where search and recommendation algorithms that strongly fit past trends are dominant.\nè¿‘å¹´ï¼Œã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã«ã‚ˆã‚‹ã‚°ãƒ­ãƒ¼ãƒãƒ«åŒ–ãŒé”æˆã•ã‚ŒãŸã‚¨ãƒ³ã‚¿ãƒ¡ç”£æ¥­ã‚„ï¼Œéå»ã®å‚¾å‘ã«å¼·ãfitã™ã‚‹æ¤œç´¢ã‚„æ¨è–¦ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒæ”¯é…çš„ãªãƒ—ãƒ©ãƒƒãƒˆãƒ›ãƒ¼ãƒ ã«ãŠã„ã¦ï¼Œã“ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã¯å¼·åŒ–ã•ã‚Œã¦ã„ã‚‹ï¼\nThe more followers/fans there are, the more followers/fans there are. The services with more registrants/subscribers tend to have more registrants/subscribers.\nãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼/ãƒ•ã‚¡ãƒ³ãŒå¤šã„äººã»ã©ï¼Œãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼/ãƒ•ã‚¡ãƒ³ãŒå¢—ãˆã‚„ã™ã„ï¼ ç™»éŒ²è€…/è³¼èª­è€…ã®å¤šã„ã‚µãƒ¼ãƒ“ã‚¹ã»ã©ï¼Œç™»éŒ²è€…/è³¼èª­è€…ãŒå¢—ãˆã‚„ã™ã„ï¼\n3. Popularity/Evaluation creates new popularity/evaluation\nFor products that sell well, the \u0026ldquo;selling\u0026rdquo; state itself becomes valuable.\nå£²ã‚Œã¦ã„ã‚‹ã‚‚ã®ã¯ï¼Œå£²ã‚Œã¦ã„ã‚‹ã¨ã„ã†ã“ã¨è‡ªä½“ãŒä¾¡å€¤ã«ãªã‚‹ï¼\nThis phenomenon is often explained by RenÃ© Girard\u0026rsquo;s triangular desire from the perspective of consumer sentiment, and by the network externality from the perspective of industrial organization theory.\nã“ã®ç¾è±¡ã¯ï¼Œæ¶ˆè²»è€…å¿ƒç†ã®è¦³ç‚¹ã‹ã‚‰ã¯ãƒ«ãƒãƒ»ã‚¸ãƒ©ãƒ¼ãƒ«ã®æ¬²æœ›ã®ä¸‰è§’å½¢ï¼Œç”£æ¥­çµ„ç¹”è«–ã®è¦³ç‚¹ã‹ã‚‰ã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¤–éƒ¨æ€§ã§ã‚ˆãèª¬æ˜ã•ã‚Œã‚‹ï¼\nAdditionally, this schema is often misused for stealth marketing, hype, information products, affiliates, etc.\nã¾ãŸï¼Œã“ã®ã‚¹ã‚­ãƒ¼ãƒã¯ï¼Œã—ã°ã—ã°ã‚¹ãƒ†ãƒï¼Œå½å®¢(ã‚µã‚¯ãƒ©)ï¼Œèª‡å¤§åºƒå‘Šï¼Œæƒ…å ±å•†æï¼Œã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆãªã©ã§æ‚ªç”¨ã•ã‚Œã‚‹ï¼\n","date":1591833600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591833600,"objectID":"9701ee598cf0a4fa23043c14d9af8bf3","permalink":"https://yumaloop.github.io/post/2020-06-11-recurrent-structure-in-capita/","publishdate":"2020-06-11T00:00:00Z","relpermalink":"/post/2020-06-11-recurrent-structure-in-capita/","section":"post","summary":"Self-discriptive system within time change.\nThe simplest example of self-discriptive systems is the exponential increase. This is because the simplest \u0026ldquo;change\u0026rdquo; of a variable $x$ is a first derivative of its time $dx/dt$, and the simplest form of the function \u0026ldquo;$f(x)$\u0026rdquo; determined by a variable $x$ is a linear one $Cx$.","tags":["System","Capitalism"],"title":"Recursion of the Capital","type":"post"},{"authors":null,"categories":["Finance","Random"],"content":"ä¸­å›½ã®TencentãŒæ—¥æœ¬ã®ACG(Anime, Comics, Game)ç”£æ¥­ã¸å¤§ããªé–¢å¿ƒã‚’å¯„ã›ã¦ã„ã‚‹ã‚‰ã—ã„ï¼ ã™ãªã‚ã¡ï¼Œã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã‚²ãƒ¼ãƒ ã®åˆ¶ä½œã¨ãƒ’ãƒƒãƒˆã‚·ãƒªãƒ¼ã‚ºã®ãƒ•ãƒ©ãƒ³ãƒãƒ£ã‚¤ã‚ºã«é–¢ã™ã‚‹æ—¥æœ¬ã®å°‚é–€çŸ¥è­˜ã‚’å¸åã—ãªãŒã‚‰ï¼Œ ã„ãã¤ã‹ã®ã‚¹ã‚¿ã‚¸ã‚ªã‚’è²·åã—ï¼Œæ½œåœ¨çš„ãªæŠ•è³‡ã«ã¤ã„ã¦äº¤æ¸‰ä¸­ã¨ã®ã“ã¨ï¼è©³ç´°ã¯ä¸‹è¨˜è¨˜äº‹ã‚’å‚ç…§ï¼\nTencent Targets Japan Anime, Manga to Jump-Start Global Growth (Bloomberg, June 9, 2020, 5:00 PM EDT)\n","date":1591747200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591747200,"objectID":"5a7dc29a1a65dddce9f0a2b4bf0e8bcd","permalink":"https://yumaloop.github.io/post/2020-06-10-tencent-acg/","publishdate":"2020-06-10T00:00:00Z","relpermalink":"/post/2020-06-10-tencent-acg/","section":"post","summary":"ä¸­å›½ã®TencentãŒæ—¥æœ¬ã®ACG(Anime, Comics, Game)ç”£æ¥­ã¸å¤§ããªé–¢å¿ƒã‚’å¯„ã›ã¦ã„ã‚‹ï¼","tags":["Tips"],"title":"Tencent's Strategy for the ACG and the Impact on Japan","type":"post"},{"authors":null,"categories":["Finance"],"content":"2020å¹´05æœˆç¾åœ¨ï¼Œã‚³ãƒ­ãƒŠã‚·ãƒ§ãƒƒã‚¯ã§ï¼Œæ™¯æ°—å¾Œé€€ãŒäºˆæ¸¬ã•ã‚Œã¦ã„ã‚‹ï¼ S\u0026amp;P 500 YTDã‚’ã¿ã‚‹ã¨ï¼Œ2018å¹´ãƒ»2019å¹´ã¨æ¯”è¼ƒã—ã¦ã‚‚ä½èª¿ã ï¼\nã‚¢ãƒ¡ãƒªã‚«æ ªã¯äºˆæƒ³ã«åã—ã¦é«˜ã„æ°´æº–ã‚’ç¶­æŒã—ã¦ã„ã‚‹ãŒï¼Œ6æœˆä»¥é™ã®çµ±è¨ˆçµæœã«ã‚ˆã£ã¦ã¯è½ã¡è¾¼ã‚€å¯èƒ½æ€§ã‚‚é«˜ã„ï¼ã“ã®ãƒã‚¹ãƒˆã§ã¯ï¼Œã‚¢ãƒ¡ãƒªã‚«ã®ãƒã‚¯ãƒ­çµŒæ¸ˆå²ã‚’æŒ¯ã‚Šè¿”ã‚Š,2021å¹´ä»¥é™ã®ã‚¢ãƒ¡ãƒªã‚«çµŒæ¸ˆã«ã¤ã„ã¦è€ƒãˆã¦ã¿ãŸã„ï¼\nFig: S\u0026amp;P 500 Index YTD Daily Performance Source: https://www.macrotrends.net/2490/sp-500-ytd-performance\nGDP USã®å®Ÿè³ªGDPå¹´é–“æ¨ç§»(1947 - 2020)ã‚’ã¿ã¦ã¿ã‚‹ï¼ã‚¤ãƒ³ãƒ•ãƒ¬ç‡ã‚’è£œæ­£ã™ã‚‹ã¨ï¼Œã»ã¼ç·šå½¢ã«å¢—åŠ ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ï¼\nFig: Real GDP, Billions of Chained 2012 Dollars, Annual Rate (1947 - 2020) Source: https://alfred.stlouisfed.org/series?seid=GDPC1\næ¬¡ã«ï¼Œæˆé•·ç‡(1æ¬¡å¾®åˆ†)ï¼USã®å››åŠæœŸã”ã¨ã®GDPæˆé•·ç‡(YOY)ã‚’ï¼Œæˆ¦å¾Œ(1947-2020)ã«é™å®šã—ã¦ã¿ã¦ã¿ã‚‹ï¼åŸºæœ¬çš„ã«ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã«ã¿ãˆã‚‹ãŒï¼Œèˆˆå‘³æ·±ã„ã®ã¯1985å¹´ã”ã‚ã‚’å¢ƒã«ï¼Œåˆ†æ•£ãŒå°ã•ããªã£ã¦ã„ã‚‹ã“ã¨ã ï¼\nFig: United States GDP Growth Rate, YOY (1947 - 2020) Source: https://tradingeconomics.com/united-states/gdp-growth\nFREDã®ã‚µã‚¤ãƒˆã«ã‚ã‚‹ï¼Œå¹´é–“ã®GDPæˆé•·ç‡(YOY)ã‚‚è¼‰ã›ã¦ãŠãï¼\nFig: Real GDP, Percent Change from Preceding Period (1930 - 2020) Source: https://fred.stlouisfed.org/graph/?g=oM2u\nEquity Market ç±³å›½æ ªå¼å¸‚å ´ã®æ­´å²ï¼ã¾ãšäº‹å®Ÿã¨ã—ã¦ï¼ŒUSã®æ ªå¼å¸‚å ´ã¯é•·æœŸçš„ã«ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰ã§ã‚ã‚‹ï¼ã¾ãŸï¼ŒS\u0026amp;P 500ã¨DJIAã¯é•·æœŸé–“ã§ã¿ã‚‹ã¨åŒã˜æŒ™å‹•ã‚’ç¤ºã™ï¼\nFig: S\u0026amp;P 500 Index (1928.01 - 2020.06) Source: https://www.macrotrends.net/2324/sp-500-historical-chart-data\nFig: Dow Jones Industrial Average (DJIA) (1915.02 - 2020.06) Source: https://www.macrotrends.net/2324/sp-500-historical-chart-data\nFig: NASDAQ Composite (1971.02 - 2020.06) Source: https://www.macrotrends.net/1320/nasdaq-historical-chart\nãã—ã¦ï¼Œç‹­ã„æ„å‘³ã§ã®åŠ¹ç‡çš„å¸‚å ´ä»®èª¬ã‚’æ”¯æŒã™ã‚‹ãªã‚‰ã°ï¼ŒçŸ­æœŸ(å¹´ã‚ã‚‹ã„ã¯æœˆå˜ä½)ã§ã®æ™¯æ°—å¾ªç’°ã¯å¹³æ»‘åŒ–ã•ã‚Œã‚‹ï¼ã¾ãŸäº‹å®Ÿã¨ã—ã¦ï¼Œã‚ã‚‰ã‚†ã‚‹çµŒæ¸ˆçµ±è¨ˆãŒã›ã„ãœã„100å¹´åˆ†ã—ã‹å­˜åœ¨ã—ãªã„ä»¥ä¸Šï¼Œæ™¯æ°—å¾ªç’°èª¬ã‚„åŠ¹ç‡çš„å¸‚å ´ä»®èª¬ã¯æ­£ã—ãæ¤œè¨¼ã§ããªã„ï¼\nS\u0026amp;P 500ã¯ï¼ŒNYSEã¨NSDAQã®ä¸Šå ´éŠ˜æŸ„ã‹ã‚‰(æµå‹•æ€§ã®é«˜ã„å¤§å‹æ ªã®)æ™‚ä¾¡ç·é¡ã‚’æŒ‡æ•°åŒ–ã—ãŸã‚‚ã®ãªã®ã§ï¼ŒDJIAã‚ˆã‚Šå®Ÿä½“çµŒæ¸ˆ(ä¼æ¥­éƒ¨é–€ã®åˆ©ç›Š)ã‚’åæ˜ ã—ã¦ã„ã‚‹(ã¨è¨€ã‚ã‚Œã¦ã„ã‚‹)ï¼\nã“ã“ã§ï¼ŒS\u0026amp;P 500ã®éå»10å¹´é–“(2010 - 2020)ã®æ¨ç§»ã‚’ã¿ã¦ã¿ã‚‹ï¼\nFig: S\u0026amp;P 500 Index, Daily Close (2010.06.15 - 2020.06.12) Source: https://fred.stlouisfed.org/graph/fredgraph.png?g=rtxI,\nFig: S\u0026amp;P 500 Index Change from Year Ago (2011.06.13 - 2020.06.12) Source: https://fred.stlouisfed.org/graph/fredgraph.png?g=rtyH\nS\u0026amp;P 500ã®æ—¥åˆ¥ã®å¤‰åŒ–é‡(YOY)ï¼ãƒã‚¤ãƒŠã‚¹ã¨ãªã£ãŸæœŸé–“ã¯2015/08~2016/07ï¼Œ2018/11~2019/05ï¼Œ2020/03~05ã®3å›ã§ã‚ã‚‹ï¼\nFig: S\u0026amp;P 500 Index Change, Daily Close (2010.06.15 - 2020.06.12) Source: https://fred.stlouisfed.org/graph/fredgraph.png?g=rtyj\nS\u0026amp;P 500ã®æ—¥åˆ¥ã®å¤‰åŒ–é‡(DOD)ï¼\nEconomic Policy æ ªä¾¡(cf. S\u0026amp;P 500)ã¯ï¼Œãƒã‚¯ãƒ­çµŒæ¸ˆ(GDP)ã«å¯¾ã™ã‚‹å…ˆè¡ŒæŒ‡æ¨™ã§ã‚ã‚‹ã¨åŒæ™‚ã«ï¼Œæ”¿åºœãŒçµŒæ¸ˆå¯¾ç­–ã‚’æ±ºå®šã™ã‚‹ãŸã‚ã®åŸå› ã¨ãªã‚‹ï¼ˆçµæœã§ã¯ãªã„ï¼‰ï¼S\u0026amp;P 500ã®æ­´å²ã¨ï¼Œãã®çµæœã¨ã—ã¦æ™‚ã®ç±³å›½æ”¿åºœãŒã©ã®ã‚ˆã†ãªçµŒæ¸ˆæ”¿ç­–ã‚’å®Ÿæ–½ã—ãŸã‹ï¼Œãã®æ”¿ç­–ã¯å…·ä½“çš„ã«ã©ã®ã‚ˆã†ãªãƒã‚¯ãƒ­çµŒæ¸ˆç†è«–ã«ã‚ˆã£ã¦è£ä»˜ã‘(ã‚¢ãƒ‰ãƒã‚¤ã‚¹)ã•ã‚ŒãŸã‹ï¼Œã‚’è€ƒãˆã¦ã¿ã‚‹ï¼\nS\u0026amp;P 500ã®å‰å¹´æ¯”å¤‰å‹•ç‡ï¼ˆ1929-2020/03ï¼‰ã‚’ã¿ã¦ã¿ã‚ˆã†ï¼\nFig: S\u0026amp;P 500 Historical Annual Returns (1928 - 2020) Source: https://www.macrotrends.net/2526/sp-500-historical-annual-returns\nS\u0026amp;P 500ã®çµ±è¨ˆãŒé–‹å§‹ã•ã‚Œã¦ã‹ã‚‰ï¼Œ1929-2019å¹´ã®ã¡ã‚‡ã†ã©100å¹´é–“ã§ï¼Œå¹´æ›ç®—ã§å‰å¹´æ¯”ãƒã‚¤ãƒŠã‚¹ã¨ãªã£ãŸå¹´ã¯22å›ã—ã‹ãªã„ï¼ãã—ã¦ï¼Œ2020å¹´ã¯23ç•ªç›®ã®å¹´ã«ãªã‚‹ã‹ã‚‚ã—ã‚Œãªã„ï¼å…¨22å›ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ã¿ã‚‹ï¼\n çµ±è¨ˆä¸Šã®ç•™æ„ç‚¹ï¼šS\u0026amp;P 500æŒ‡æ•°ã®ç™ºè¡Œå…ƒã§ã‚ã‚‹S\u0026amp;P Globalç¤¾ã®æ²¿é©:\n1941å¹´ï¼Œã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰çµ±è¨ˆç¤¾(Standard Statistics Bureau)ã¨ãƒ—ã‚¢ãƒ¼å‡ºç‰ˆç¤¾(H.V. and H.W. Poor Co.)ãŒåˆä½µã—ã¦S\u0026amp;Pç¤¾(Standard\u0026amp;Poor\u0026rsquo;s)ãŒèª•ç”Ÿï¼1957å¹´ï¼ŒS\u0026amp;P 500ãŒèª•ç”Ÿï¼1966å¹´ï¼Œãƒã‚°ãƒ­ã‚¦ãƒ’ãƒ«ç¤¾ãŒS\u0026amp;Pç¤¾ã‚’è²·åã—ï¼Œç¾åœ¨ã®é‹å–¶ä½“åˆ¶ (S\u0026amp;P Global Inc.; 1995å¹´ã¾ã§ã®æ—§å:The McGraw-Hill Companies)ã¨ãªã£ãŸï¼\n Great Depression (1929-1936, 7 years) ã‚±ã‚¤ãƒ³ã‚ºï¼Œãƒ‹ãƒ¥ãƒ¼ãƒ‡ã‚£ãƒ¼ãƒ« æ”¿ç­– æœ‰åŠ¹éœ€è¦\n 1929: (-11.91% YoY) ä¸–ç•Œææ…Œ 1930: (-28.48% YoY) ä¸–ç•Œææ…Œ 1931: (-47.07% YoY) ä¸–ç•Œææ…Œ 1932: (-15.15% YoY) ä¸–ç•Œææ…Œ 1934: (-05.94% YoY) ä¸–ç•Œææ…Œ  World War â…¡ (1937-1945, 8 years)  1937: (-38.59% YoY) ç¬¬äºŒæ¬¡ä¸–ç•Œå¤§æˆ¦ 1939: (-05.45% YoY) ç¬¬äºŒæ¬¡ä¸–ç•Œå¤§æˆ¦ 1940: (-15.29% YoY) ç¬¬äºŒæ¬¡ä¸–ç•Œå¤§æˆ¦ 1941: (-17.86% YoY) ç¬¬äºŒæ¬¡ä¸–ç•Œå¤§æˆ¦  Post-war Prosperity (1945-1973, 28 years)  60så¾ŒåŠ-70så‰åŠã®ã‚¹ã‚¿ã‚°ãƒ•ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®çµæœã¨ã—ã¦ï¼Œ1972,73: ãƒ‹ã‚¯ã‚½ãƒ³ã‚·ãƒ§ãƒƒã‚¯(ãƒ–ãƒ¬ãƒˆãƒ³ãƒ»ã‚¦ãƒƒã‚ºå”å®šå´©å£Š)\n  1946: (-11.87% YoY) æˆ¦å¾Œ 1953: (-06.62% YoY) 1957: (-14.31% YoY) 1960: (-02.97% YoY) 1962: (-11.81% YoY) 1966: (-13.09% YoY) 1969: (-11.36% YoY)  Reaganomics (1974-1990, 16 years)  æ–°è‡ªç”±ä¸»ç¾©ï¼Œã‚¨ãƒãƒ«ã‚®ãƒ¼è¦åˆ¶ã®ç·©å’Œï¼Œãƒ¬ãƒ¼ã‚¬ãƒãƒŸã‚¯ã‚¹(æ‰€å¾—æ¸›ç¨)ï¼Œãƒ—ãƒ©ã‚¶åˆæ„ï¼ŒåŒå­ã®èµ¤å­—(è²¡æ”¿èµ¤å­—ã¨è²¿æ˜“èµ¤å­—ã®æ‹¡å¤§)\n  1973: (-17.37% YoY) ç¬¬ä¸€æ¬¡ã‚ªã‚¤ãƒ«ã‚·ãƒ§ãƒƒã‚¯ 1974: (-29.72% YoY) ç¬¬ä¸€æ¬¡ã‚ªã‚¤ãƒ«ã‚·ãƒ§ãƒƒã‚¯ 1977: (-11.50% YoY) 1981: (-09.73% YoY) ç¬¬äºŒæ¬¡ã‚ªã‚¤ãƒ«ã‚·ãƒ§ãƒƒã‚¯  New Economy (1990-2000, 10 years)  æƒ…å ±é€šä¿¡æ¥­ã®ç‰½å¼•ï¼Œãƒ‰ãƒƒãƒˆã‚³ãƒ ãƒãƒ–ãƒ«ï¼Œ90sã®USGDPã¯69%å¢—ï¼ŒS\u0026amp;P 500ã¯3å€ã«ä¸Šæ˜‡\n  1990: (-06.56% YoY) é€šè²¨å±æ©Ÿ 2000: (-10.14% YoY) ãƒ‰ãƒƒãƒˆã‚³ãƒ ãƒãƒ–ãƒ«å´©å£Š  Financial Crisis (2001-2009, 8 years)  2001å¹´ã®åŒæ™‚å¤šç™ºãƒ†ãƒ­ï¼Œ2001-2007ã®ä½å®…ãƒãƒ–ãƒ«ã¨ã‚µãƒ–ãƒ—ãƒ©ã‚¤ãƒ ãƒ­ãƒ¼ãƒ³ã«ã‚ˆã‚‹é‡‘èå±æ©Ÿï¼ŒãƒŸãƒ³ã‚¹ã‚­ãƒ¼ã®é‡‘èä¸å®‰å®šä»®èª¬ï¼Œ\n  2001: (-13.04% YoY) ãƒ‰ãƒƒãƒˆã‚³ãƒ ãƒãƒ–ãƒ«å´©å£Š 2002: (-23.37% YoY) ãƒ‰ãƒƒãƒˆã‚³ãƒ ãƒãƒ–ãƒ«å´©å£Š 2009: (-38.49% YoY) ãƒªãƒ¼ãƒãƒ³ã‚·ãƒ§ãƒƒã‚¯  Tech boom (2010-2019, 10years)  è¥¿æµ·å²¸ãƒ†ãƒƒã‚¯æ ªã®ç‰½å¼•ï¼Œä¸­å›½çµŒæ¸ˆã®å°é ­\n  2018: (-06.42% YoY) ä¸Šæµ·å±æ©Ÿ  New Normal? (2020-????)  2020: (-3.34% YTD) ã‚³ãƒ­ãƒŠã‚·ãƒ§ãƒƒã‚¯  References \u0026amp; Source  www.macrotrends.net - S\u0026amp;P 500, historical annual returns www.macrotrends.net - DJIA, 100 years historical chart us.spindices.com - S\u0026amp;P 500 us.spindices.com - Dow Jones Industrial Average tradingeconomics.com - United States GDP Growth Rate tradingeconomics.com - S\u0026amp;P 500 tradingeconomics.com - Dow Jones Industrial Average fred.stlouisfed.org - Real Gross Domestic Product fred.stlouisfed.org - S\u0026amp;P 500 Index fred.stlouisfed.org - Dow Jones Industrial Average Wikipedia - Economic history of the United States Wikipedia - List of recessions in the United States Wikipedia - List of economic expansions in the United States  ","date":1590710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590710400,"objectID":"46be1c3180b33e0ee552ff09d82056d4","permalink":"https://yumaloop.github.io/post/2020-05-29-history_of_us_macro_economy/","publishdate":"2020-05-29T00:00:00Z","relpermalink":"/post/2020-05-29-history_of_us_macro_economy/","section":"post","summary":"ã‚¢ãƒ¡ãƒªã‚«ã®ãƒã‚¯ãƒ­çµŒæ¸ˆå²ã‚’æŒ¯ã‚Šè¿”ã‚Š,2021å¹´ä»¥é™ã®ã‚¢ãƒ¡ãƒªã‚«çµŒæ¸ˆã«ã¤ã„ã¦è€ƒãˆã¦ã¿ã‚‹ï¼","tags":["Tips"],"title":"United States Macroeconomic History","type":"post"},{"authors":null,"categories":["StatML"],"content":"ã‚·ã‚¹ãƒ†ãƒ ã¨å®šå¸¸çŠ¶æ…‹ å¤šãã®å‹•çš„ãƒ¢ãƒ‡ãƒ«ï¼ˆDynamic Model-Systemï¼‰ã¯ï¼Œå®šå¸¸çŠ¶æ…‹ï¼ˆtime-steady stateï¼‰ã«è‡³ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦è¨­è¨ˆã•ã‚Œã‚‹ï¼ã“ã“ã§ï¼Œå®šå¸¸çŠ¶æ…‹ã¨ã¯ï¼Œã€Œã‚ã‚‹å¤‰æ•°$X$ã«ä½œç”¨ã™ã‚‹ï¼Œä½•ã‚‰ã‹ã®æ™‚å¤‰é‡ï¼ˆparameter $\\theta_t$ï¼‰ã‚„é–¢æ•°$L_t(x; \\theta)$ï¼‰ãŒä¸€å®šå€¤ã«åæŸã™ã‚‹ã“ã¨ã€ã¨å®šç¾©ã—ã¦ãŠãï¼\nãã—ã¦ï¼Œç€ç›®ã—ã¦ã„ã‚‹å‹•çš„ãƒ¢ãƒ‡ãƒ«ãŒå®šå¸¸çŠ¶æ…‹ã«è‡³ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã¯ï¼Œã‚·ã‚¹ãƒ†ãƒ åŒå®šï¼ˆSystem identificationï¼‰ã¨å‘¼ã°ã‚Œï¼Œå®šå¸¸çŠ¶æ…‹ã‚’ç¤ºã—ãŸæ¦‚å¿µã¨ã—ã¦ï¼Œå‡è¡¡ï¼ˆequilibriumï¼‰ã‚„å¹³è¡¡ï¼ˆbalanceï¼‰ã¨å‘¼ã°ã‚Œã‚‹ç”¨èªãŒä½¿ã‚ã‚Œã‚‹ï¼\n åŒ–å­¦åå¿œã«ãŠã„ã¦ã¯ã€å¯é€†åå¿œã®ç”Ÿæˆç‰©ã®å¤‰åŒ–é‡ã¨å‡ºç™ºç‰©è³ªã®å¤‰åŒ–é‡ãŒåˆè‡´ã—ãŸçŠ¶æ…‹ã‚’æŒ‡ã™ã€‚åŒ–å­¦å¹³è¡¡ã‚’å‚ç…§ã€‚ åŠ›å­¦ã«ãŠã„ã¦ã¯ã€ç‰©ä½“ã«åŠ ã‚ã£ã¦ã„ã‚‹å…¨ã¦ã®åŠ›ã®åˆåŠ›ã¨åŠ›ã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã®å’ŒãŒã¨ã‚‚ã« 0 ã§ã‚ã‚‹çŠ¶æ…‹ã‚’å¹³è¡¡ã¨å‘¼ã¶ã€‚åŠ›å­¦çš„å¹³è¡¡ï¼ˆè‹±èª: Mechanical equilibriumï¼‰ã‚’å‚ç…§ã€‚ ç†±åŠ›å­¦ã«ãŠã„ã¦ã¯é€šå¸¸ã€ç†±å¹³è¡¡ã€åŠ›å­¦çš„å¹³è¡¡ã€åŒ–å­¦å¹³è¡¡ã®ä¸‰ã¤ã‚’åˆã‚ã›ã¦ã€ç†±åŠ›å­¦çš„å¹³è¡¡ã¨ã‚ˆã¶ã€‚ çµ±è¨ˆåŠ›å­¦ã«ãŠã„ã¦ã¯ã€ç³»ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼åˆ†å¸ƒãŒã€ãƒœãƒ«ãƒ„ãƒãƒ³åˆ†å¸ƒã«å¾“ã†ã“ã¨ã§ã‚ã‚‹ã€‚ç†±åŠ›å­¦çš„å¹³è¡¡ã‚’å‚ç…§ã€‚ ç‰©ç†åŒ–å­¦ã«ãŠã„ã¦ã¯ã€è¤‡æ•°ã®ç‰©è³ªç›¸ã‹ã‚‰æ§‹æˆã•ã‚Œã‚‹ç³»ã«ãŠã„ã¦ã€ç›¸é–“ã®ç‰©è³ªã®å‡ºå…¥ã‚ŠãŒåˆã„ç­‰ã—ã„çŠ¶æ…‹ã‚’æŒ‡ã™ã€‚ç›¸å¹³è¡¡ã‚’å‚ç…§ã€‚ é›»æ°—å·¥å­¦ã«ãŠã„ã¦ã¯ã€ä¿¡å·æºã¨è² è·ã®é–“ã®ã‚¤ãƒ³ãƒ”ãƒ¼ãƒ€ãƒ³ã‚¹ãŒåˆè‡´ã—ã¦ã„ã‚‹ã“ã¨ã‚’æŒ‡ã™ã€‚ã‚¤ãƒ³ãƒ”ãƒ¼ãƒ€ãƒ³ã‚¹å¹³è¡¡ã‚’å‚ç…§ã€‚ é›»æ°—å›è·¯ã«ãŠã„ã¦ã¯ã€ä¿¡å·å›è·¯ã®åŒæ–¹ãŒæ¥åœ°ç‚¹ã«æ¥ç¶šã•ã‚Œã¦ã„ãªã„ã“ã¨ã‚’æŒ‡ã™ã€‚å¹³è¡¡æ¥ç¶šã‚’å‚ç…§ã€‚ æƒ…å ±å·¥å­¦ã«ãŠã„ã¦ã¯ã€ãƒ‡ãƒ¼ã‚¿æœ¨æ§‹é€ ã®ä»»æ„ã®ç¯€ã«ãŠã„ã¦ãã®é…ä¸‹ã®ç¯€ç‚¹ã®æ•°ãŒç­‰ã—ã„çŠ¶æ…‹ã‚’æŒ‡ã™ã€‚ ç”Ÿæ…‹å­¦ã«ãŠã„ã¦ã¯ã€ç”Ÿç‰©ç¾¤é›†é–“ã®åˆ†å¸ƒã¨å€‹ä½“æ•°ã®å¤‰åŒ–ãŒç„¡ã„çŠ¶æ…‹ã‚’æŒ‡ã™ã€‚ ç”Ÿç†å­¦ã«ãŠã„ã¦ã¯ã€æ°´å¹³ã§ã‚ã‚‹ã“ã¨ã‚’èªçŸ¥ã™ã‚‹ã“ã¨ã‚’æŒ‡ã™ã€‚å¹³è¡¡æ„Ÿè¦šã‚’å‚ç…§ã€‚ çµŒæ¸ˆå­¦ã«ãŠã„ã¦ã¯ã€éœ€è¦ã¨ä¾›çµ¦ãŒé‡£ã‚Šåˆã£ã¦ä¾¡æ ¼ãŒä¸å‹•ã«ãªã‚‹ã“ã¨ãªã©ã‚’æŒ‡ã™ã€‚å‡è¡¡ã‚’å‚ç…§ã€‚  é€£ç«‹(å¾®åˆ†)æ–¹ç¨‹å¼ã§è¨˜è¿°ã§ãã‚‹ãŸã‚ï¼\nãªãœç·šå½¢ãƒ¢ãƒ‡ãƒ«ãŒæœ‰ç”¨ãªã®ã‹ï¼Ÿ ç­”ãˆã¯ã‚·ãƒ³ãƒ—ãƒ«ã§ï¼ŒTaylorå±•é–‹\n$$ \\begin{align} \\frac{d x_1}{d t} \u0026amp;= f_1(x_1, \\dots, x_n; \\theta_1) \\\\\n\\frac{d x_2}{d t} \u0026amp;= f_2(x_1, \\dots, x_n; \\theta_1) \\\\\n\u0026amp;\\vdots \\\\\n\\frac{d x_n}{d t} \u0026amp;= f_n(x_1, \\dots, x_n; \\theta_1) \\end{align} $$\nã“ã®ãƒ¢ãƒ‡ãƒ«ãŒï¼Œå®šå¸¸çŠ¶æ…‹ã«ã„ã‚‹å ´åˆï¼Œ\n$$ \\begin{align} f_1 = f_2 = \\cdots = f_n = 0 \\end{align} $$\nãŒæˆã‚Šç«‹ã¤ã‹ã‚‰ï¼Œ$n$å€‹ã®å¤‰æ•°${\\bf x} = (x_1, x_2, \\dots, x_n)$ã«å¯¾ã—ã¦ï¼Œ$n$å€‹ã®æ–¹ç¨‹å¼ãŒå¾—ã‚‰ã‚Œã‚‹ï¼ã“ã®è§£ãŒã‚·ã‚¹ãƒ†ãƒ ã®å®šå¸¸åŒ–ã¨ãªã‚‹ï¼ã“ã‚Œã‚’${\\bf x}^* = (x_1^*, x_2^*, \\dots, x_n^*)$ã¨ãŠãã¨ï¼Œ$f_1, f_2, \\dots, f_n$ã«å¯¾ã—ã¦ï¼Œç‚¹${\\bf x}^*$ã®è¿‘å‚ã§Taylorå±•é–‹ãŒå¯èƒ½ã«ãªã‚‹ï¼\n$$ \\begin{align} \\frac{d x_1}{d t} = f_1(x_1, \\dots, x_n; \\theta_1) =\u0026amp; a_{11}(x_1 - x_1^*) + a_{12} {(x_2 - x_2^*)} + \\cdots a_{1n} {(x_n - x_n^*)} + \\\\\n\u0026amp; a_{111}{(x_1 - x_1^*)}^2 + a_{112}(x_1 - x_1^*)(x_2 - x_2^*) + \\cdots + a_{11n}{(x_1 - x_1^*)}^2 + \\\\\n\u0026amp; ~~ \\vdots \\\\\n\u0026amp; a_{11\\cdots1}{(x_1 - x_1^*)}^n + \\cdots \\\\\n\\frac{d x_2}{d t} = f_2(x_1, \\dots, x_n; \\theta_1) =\u0026amp; a_{21}(x_1 - x_1^*) + a_{22} {(x_2 - x_2^*)} + \\cdots a_{2n} {(x_n - x_n^*)} + \\\\\n\u0026amp; a_{211}{(x_1 - x_1^*)}^2 + a_{212}(x_1 - x_1^*)(x_2 - x_2^*) + \\cdots + a_{21n}{(x_1 - x_1^*)}^2 + \\\\\n\u0026amp; ~~ \\vdots \\\\\n\u0026amp; a_{21\\cdots1}{(x_1 - x_1^*)}^n + \\cdots \\\\ \\\\\n\u0026amp; ~~ \\vdots \\\\ \\\\\n\\frac{d x_n}{d t} = f_n(x_1, \\dots, x_n; \\theta_1) =\u0026amp; a_{n1}(x_1 - x_1^*) + a_{n2} {(x_2 - x_2^*)} + \\cdots a_{nn} {(x_n - x_n^*)} + \\\\\n\u0026amp; a_{n11}{(x_1 - x_1^*)}^2 + a_{n12}(x_1 - x_1^*)(x_2 - x_2^*) + \\cdots + a_{n1n}{(x_1 - x_1^*)}^2 + \\\\\n\u0026amp; ~~ \\vdots \\\\\n\u0026amp; a_{n1\\cdots1}{(x_1 - x_1^*)}^n + \\cdots \\\\\n\\end{align} $$\nã¤ã¾ã‚Šï¼Œä»»æ„ã®å¾®åˆ†å¯èƒ½é–¢æ•°$f_1, f_2, \\dots, f_n$ã«ã‚ˆã£ã¦è¡¨ç¾ã•ã‚ŒãŸãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚’ã‚‚ã¤å‹•çš„ãƒ¢ãƒ‡ãƒ«ã¯ï¼Œï¼ˆå®šå¸¸è§£ã®è¿‘å‚ã§ã¯ï¼‰ä»»æ„ã®næ¬¡å¤šé …å¼ã«ã‚ˆã£ã¦è¿‘ä¼¼ã§ãã‚‹ï¼ã“ã‚Œã«ã‚ˆã‚Šï¼Œç·šå½¢ã‚·ã‚¹ãƒ†ãƒ ã®å¦¥å½“æ€§ãŒä¿è¨¼ã•ã‚Œã‚‹ï¼ä¸€èˆ¬è§£ã¯ï¼Œ\n$$ \\begin{align} x_1 =\u0026amp; ~ x_1^* + C_{11}e^{\\lambda_1 t} + C_{12}e^{\\lambda_2 t} + \\cdots C_{1n}e^{\\lambda_n t} + \\\\\n\u0026amp; ~~~~~~~~~~ C_{111}e^{2\\lambda_1 t} + C_{112}e^{2\\lambda_2 t} + \\cdots + C_{11n}e^{2\\lambda_n t} + \\\\\n\u0026amp; ~~~~~~~~~~ ~~ \\vdots \\\\\n\u0026amp; ~~~~~~~~~~ C_{11\\cdots1}e^{n\\lambda_1 t} + C_{11\\cdots2}e^{n\\lambda_2 t} + \\cdots + C_{11\\cdots n}e^{n\\lambda_n t} \\\\\nx_2 =\u0026amp; ~ x_2^* + C_{21}e^{\\lambda_1 t} + C_{22}e^{\\lambda_2 t} + \\cdots C_{2n}e^{\\lambda_n t} + \\\\\n\u0026amp; ~~~~~~~~~~ C_{211}e^{2\\lambda_1 t} + C_{212}e^{2\\lambda_2 t} + \\cdots + C_{11n}e^{2\\lambda_n t} + \\\\\n\u0026amp; ~~~~~~~~~~ ~~ \\vdots \\\\\n\u0026amp; ~~~~~~~~~~ C_{21\\cdots1}e^{n\\lambda_1 t} + C_{21\\cdots2}e^{n\\lambda_2 t} + \\cdots + C_{21\\cdots n}e^{n\\lambda_n t} \\\\ \\\\\n\u0026amp; ~~~~~~~~~~ ~~ \\vdots \\ \\\nx_n =\u0026amp; ~ x_n^* + C_{n1}e^{\\lambda_1 t} + C_{n2}e^{\\lambda_2 t} + \\cdots C_{nn}e^{\\lambda_n t} + \\\\\n\u0026amp; ~~~~~~~~~~ C_{n11}e^{2\\lambda_1 t} + C_{n12}e^{2\\lambda_2 t} + \\cdots + C_{n1n}e^{2\\lambda_n t} + \\\\\n\u0026amp; ~~~~~~~~~~ ~~ \\vdots \\\\\n\u0026amp; ~~~~~~~~~~ C_{n1\\cdots1}e^{n\\lambda_1 t} + C_{n1\\cdots2}e^{n\\lambda_2 t} + \\cdots + C_{n1\\cdots n}e^{n\\lambda_n t} \\\\\n\\end{align} $$\nã¨ãªã‚‹ï¼\nãƒ¢ãƒ‡ãƒ«ã¨ã¯ä½•ã‹ï¼Ÿ ã¤ã¾ã‚Šï¼Œå¤šãã®åˆ†é‡ã«ãŠã„ã¦æ•°ç†ãƒ¢ãƒ‡ãƒ«ã¨ã‹è¨ˆé‡ãƒ¢ãƒ‡ãƒ«ã¨ã‹å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã¯ï¼Œä»¥ä¸‹ã®æ‰‹ç¶šãã‚’å¿…è¦ã¨ã™ã‚‹ï¼\n å®šå¸¸çŠ¶æ…‹ã«è‡³ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã™ã‚‹å‹•çš„ãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©ã™ã‚‹ ãƒ¢ãƒ‡ãƒ«ã®çŠ¶æ…‹å¤‰åŒ–ã‚’æœ€é©åŒ–å•é¡Œ(éç¨‹)ã¨ã—ã¦å®šå¼åŒ–ã™ã‚‹ï¼ å®šå¸¸çŠ¶æ…‹ã¸ã®åæŸãŒä¿è¨¼ã•ã‚ŒãŸæœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è€ƒãˆã‚‹  ç§»å‹•å¹³å‡  ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ Adamã«ç½®ã‘ã‚‹ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ  å¼·åŒ–å­¦ç¿’ã®å ±é…¬ ã‚²ãƒ¼ãƒ ç†è«–ã«ãŠã‘ã‚‹Ficticious Play æ ªä¾¡ã«ãŠã‘ã‚‹ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«åˆ†æç§»å‹•å¹³å‡ï¼ˆARMAï¼‰ (é‡‘èå·¥å­¦)ãƒãƒªãƒ¥ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã«ãŠã‘ã‚‹DCFæ³•  ç§»å‹•å¹³å‡ã¨ã¯ä½•ã‹ï¼Ÿ\n$$ \\begin{align} m_t \u0026amp;= \\gamma \\cdot m_t âˆ’ 1+Î· \\cdot \\frac{\\partial L(w_t)}{\\partial w} \\\\\nw_{t+1} \u0026amp;= w_t - m_t \\end{align} $$\n$$ m_t = g_t + \\gamma \\cdot g_{t-1} + \\gamma^2 \\cdot g_{t-2} \\cdots + \\gamma^t \\cdot g_{0} $$\n","date":1587686400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587686400,"objectID":"21fbd7d1785d6723151bf2b3dc3e018b","permalink":"https://yumaloop.github.io/post/2020-04-24-time-steady-state-on-system/","publishdate":"2020-04-24T00:00:00Z","relpermalink":"/post/2020-04-24-time-steady-state-on-system/","section":"post","summary":"å‹•çš„ã‚·ã‚¹ãƒ†ãƒ ã®å®šå¸¸çŠ¶æ…‹ï¼Œç·šå½¢æ€§ï¼Œç§»å‹•å¹³å‡ã«ã¤ã„ã¦ï¼","tags":["Tips","System"],"title":"Time-steady States on Systems","type":"post"},{"authors":null,"categories":["StatML"],"content":"   State Space Model (SSM)\nState Space Model(SSM) is widely used in the field requiring the sequential estimation or online learning. This model is effective if you consider a system having two different variables; one completely represents the actual state but cannot be observed and the other partially represents the actual state but can be observed. Here, I call the former $x$ (state variable) and the latter $y$ (observation variable).\nIn SSM, we intruduce the following equations $F, H$ (or $f, h$) and identify them by observed data sample $[y_1, \\dots, y_t]$.\n  Equation of each state $x_t$ :\n$$ \\begin{aligned} x_{t+1} \u0026amp;= F(x_t) ~~ (\\text{Deterministic process}) \\\\\nx_{t+1} \u0026amp;\\sim f(\\cdot\\vert x_t) ~~ (\\text{Stochastic process}) \\end{aligned} $$\n  Equation of each observation $y_t$ :\n$$ \\begin{aligned} y_t \u0026amp;= H(x_t) ~~ (\\text{Deterministic process}) \\\\\ny_t \u0026amp;\\sim h(\\cdot \\vert x_t) ~~ (\\text{Stochastic process}) \\end{aligned} $$\n  Perticle filter\n  For each $i$ in $[1 \\dots M]$\n  (Prediction)\nDerive prediction distribution $f(x_t \\vert \\cdot)$ depends on particles $\\hat{x}_{t-1}$.\nSample $x^{i}_{t \\vert t-1} ~~~ (i = 1, \\dots, M)$ following $f(x_t \\vert \\cdot)$.\n$$ \\begin{align} x^{i}_{t \\vert t-1} \\sim f(x_t \\vert \\hat{x}_{t-1}) \\end{align} $$\n    (Likelihood)\nDerive the likelihood of $x^i_{t \\vert t-1}$ from given sample data $y_t$ based on $h(\\cdot)$\n$$ w^i_t \\sim h(y_t \\vert x^i_{t \\vert t-1}) $$\n  (Resampling)\nResampe $\\hat{x}^i_{t \\vert t-1}$ based on the likelihood $w^i_t ~~~ (i=1,\\dots,M)$ .\nDerive the filter distribution $p(x_t \\vert y_{1:t})$ for any $x_t$: $$ \\begin{aligned} p(x_t \\vert y_{1:t}) \u0026amp;\\approx \\frac{1}{M} \\sum_{i=1}^{M} \\delta(x_t - \\hat{x}^i_{t \\vert t-1}) \\\\\n\u0026amp;\\approx \\sum_{i=1}^{M} \\frac{}{\\sum_{i=1}^{M} } \\delta(x_t - \\hat{x}^i_{t \\vert t-1}) \\end{aligned} $$\n  ","date":1584489600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584489600,"objectID":"f552b8fa81a0587f981381f6b2396798","permalink":"https://yumaloop.github.io/post/2020-03-18-particle_filter/","publishdate":"2020-03-18T00:00:00Z","relpermalink":"/post/2020-03-18-particle_filter/","section":"post","summary":"State Space Model (SSM)\nState Space Model(SSM) is widely used in the field requiring the sequential estimation or online learning. This model is effective if you consider a system having two different variables; one completely represents the actual state but cannot be observed and the other partially represents the actual state but can be observed.","tags":["Bayes","System"],"title":"State Space Model \u0026 Particle Filter","type":"post"},{"authors":null,"categories":["StatML"],"content":"å¹³å‡å ´è¿‘ä¼¼ã¨è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ ã‚ã‚‹å¤‰æ•°$X$ã®ã¨ã‚Šã†ã‚‹ã™ã¹ã¦ã®çŠ¶æ…‹(å®Ÿç¾å€¤)$x$ã«å¯¾ã—ã¦ï¼Œä½•ã‚‰ã‹ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°$\\phi(x)$ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ã™ã‚‹ï¼ã“ã®ã¨ãï¼Œå¤‰æ•°$X$ã®Gibbsåˆ†å¸ƒï¼ˆBoltzmannåˆ†å¸ƒï¼‰:\n$$ \\begin{aligned} p(x) \u0026amp;= \\frac{\\exp (- \\beta \\phi(x))}{\\int_X \\exp (- \\beta \\phi(x))} = \\frac{\\exp (- \\beta \\phi(x))}{Z^{\\phi}(\\beta)} \\end{aligned} $$\nã‚’è€ƒãˆã‚‹ï¼ã“ã®ã¨ãï¼ŒGibbsåˆ†å¸ƒ$p(x)$ã¨ä»»æ„ã®è¿‘ä¼¼åˆ†å¸ƒ$q(x)$ã¨ã®KL-divergence:\n$$ \\begin{aligned} D_{KL}(q \\vert\\vert p) := \\int_{X} q(x) \\log \\frac{q(x)}{p(x)} \\end{aligned} $$\nã¯ä»¥ä¸‹ã®ã‚ˆã†ã«åˆ†è§£ã§ãã‚‹ï¼\n$$ \\begin{aligned} D_{KL}(q \\vert\\vert p) \u0026amp;= \\beta \\int_X q(x)\\phi(x) - \\left\\{ - \\int_X q(x)\\log q(x) \\right\\} + \\log \\int_X \\exp(-\\beta \\phi(x)) \\\\\n\u0026amp;= \\beta~ \\mathbb{E}_{x \\sim q}[\\phi(x)] - H_q(X) + \\log Z^{\\phi}(\\beta) \\\\\n\u0026amp;= \\beta~ (\\text{Internal energy}) - (\\text{Entropy}) + (\\text{Const.}) \\\\\n\\end{aligned} $$\nã„ã¾ï¼Œè¿‘ä¼¼åˆ†å¸ƒ$q(x)$ã«å¯¾ã™ã‚‹æ±é–¢æ•°ã¨ã—ã¦ï¼Œè‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼:\n$$ F^{\\phi}(q) := \\mathbb{E}_{x \\sim q}[\\phi(x)] - \\frac{1}{\\beta}H_q(X) ~~~ (\\text{Free energy}) $$\nã‚’å®šç¾©ã™ã‚Œã°ï¼Œ\n$$ D_{KL}(q \\vert\\vert p) = \\beta~ F^{\\phi}(q) + \\log Z^{\\phi}(\\beta) $$\nã¨ãªã‚‹ã‹ã‚‰ï¼Œ$q(x)$ã«ã‚ˆã‚‹$p(x)$ã®è¿‘ä¼¼å•é¡Œã¯æ¬¡å¼ã§è¡¨ç¾ã§ãã‚‹ï¼\n$$ \\begin{aligned} \\underset{q}{\\rm min} ~ D_{KL}(p \\vert\\vert q) \u0026amp;= \\underset{q}{\\rm min} ~ F^{\\phi}(q) \\end{aligned} $$\nã¾ãŸï¼è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼$F^{\\phi}(q)$ã®æœ€å°å€¤ã¯ï¼Œ\n$$ {F^{\\phi}}^{*}(q) = - \\frac{1}{\\beta} \\log \\int_X \\exp (-\\beta \\phi(x)) = - \\frac{1}{\\beta} \\log Z^{\\phi}(\\beta) $$\nã¨ãªã‚‹ï¼ã™ãªã‚ã¡ï¼Œ\n$$ \\begin{aligned} {F^{\\phi}}(q) = - \\frac{1}{\\beta} \\log Z^{\\phi}(\\beta) ~~ \\Leftrightarrow ~~ D_{KL}(p \\vert\\vert q) = 0 ~~ \\Leftrightarrow ~~ p(\\cdot) \\equiv\tq(\\cdot) \\end{aligned} $$\nã¨ãªã‚‹ï¼\nç†±åŠ›å­¦(çµ±è¨ˆåŠ›å­¦)ã¨ã®é–¢ä¿‚ æ¸©åº¦$T$ï¼Œå†…éƒ¨ã‚¨ãƒãƒ«ã‚®ãƒ¼$U$ï¼Œã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼$S$ã«å¯¾ã—ã¦ï¼ŒHelmholtzã®è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼$F$ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«å®šç¾©ã•ã‚Œã‚‹ï¼\n$$ F = U - TS $$\n$F^{\\phi}(q)$ã®å®šç¾©å¼ã§ï¼Œ$F = F^{\\phi}(q)$ï¼Œ$U = \\mathbb{E}_{x \\sim q}[\\phi(x)]$ï¼Œ$S = H_q(X)$ã¨ãŠã‘ã°ï¼Œ\n$$ \\begin{aligned} \\beta F \u0026amp;= \\beta U - S \\\\\nF \u0026amp;= U - \\frac{1}{\\beta} S \\end{aligned} $$\nã¨ãªã‚‹ã‹ã‚‰ï¼Œæ±é–¢æ•°$F^{\\phi}(q)$ã¯ï¼Œç†±åŠ›å­¦ã«ãŠã‘ã‚‹Helmholtzã®è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼$F$ã¨é¡ä¼¼ã—ãŸå½¢å¼ã‚’æŒã£ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ï¼ãªãŠï¼ŒBayesç†è«–ã«ãŠã„ã¦å®šæ•°$\\beta$ã¯ã€Œé€†æ¸©åº¦ã€ã¨å‘¼ã°ã‚Œã‚‹ãŒï¼Œã“ã‚Œã¯æ¸©åº¦$T$ã«ç”±æ¥ã™ã‚‹ï¼\nBayesè„³ã‚„FEPã¨ã®é–¢ä¿‚ ç¥çµŒç§‘å­¦ã®åˆ†é‡ã§K.Fristonã«ã‚ˆã£ã¦æå”±ã•ã‚ŒãŸè‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼åŸç†(Free energy principle, FEP)ã¯ï¼Œä¸Šã«ã‚ã‚‹æ±é–¢æ•°$F^{\\phi}(q)$ã‚’å¤‰åˆ†æ¨è«–ã‚’çµ„ã¿åˆã‚ã›ãŸã‚‚ã®ã§ã‚ã‚‹ï¼ˆã¨è§£é‡ˆã§ãã‚‹ï¼‰ï¼ã“ã“ã§ã¯ï¼ŒELBOã¨ã®é–¢ä¿‚ã«ã®ã¿è§¦ã‚Œã¦ãŠãï¼\nELBOã®å®šç¾©å¼:\n$$ \\begin{align} (\\text{Evidence}) \u0026amp;= \\log p(y) \\\\\n\u0026amp;\\geq \\mathbb{E}_{\\theta \\sim q}\\left[ \\log p(y, \\theta) \\right] - \\mathbb{E}_{\\theta \\sim q} \\left[ \\log q(\\theta) \\right] \\\\\n\u0026amp;= \\mathcal{L}_{ELBO}(q) \\\\\n\u0026amp;= (\\text{Evidence Lower Bound}) \\end{align} $$\nã¨Fristonã®Cellè«–æ–‡(2009)ã«ã‚ã‚‹è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®å®šç¾©å¼\n$$ F(y) = - \\mathbb{E}_{\\theta \\sim q}[\\log p(y,\\theta)] + \\mathbb{E}_{\\theta \\sim q}[\\log q(\\theta)] $$\nã‚’æ¯”ã¹ã‚‹ã¨ï¼Œä»¥ä¸‹ã®é–¢ä¿‚ãŒå¾—ã‚‰ã‚Œã‚‹ï¼\n$$ \\begin{aligned} (\\text{Surprise}) \u0026amp;= - \\log p(y) \\\\\n\u0026amp;\\leq - \\mathbb{E}_{\\theta \\sim q}[\\log p(y,\\theta)] + \\mathbb{E}_{\\theta \\sim q}[\\log q(\\theta)] \\\\\n\u0026amp;= -\\mathcal{L}_{ELBO}(q) \\\\\n\u0026amp;= F(y) \\\\\n\u0026amp;= (\\text{Free energy}) \\end{aligned} $$\nã¤ã¾ã‚Šï¼ŒFristonã®è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼$F(y)$ã¯ã€Œè„³ã®å¤–éƒ¨ç’°å¢ƒ$Y$ã«å¯¾ã™ã‚‹è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿${\\{y_t\\}}_{t=1}^{n}$ã®å¯¾æ•°å°¤åº¦ä¸‹é™(ELBO)ã«$-1$ã‚’ã‹ã‘ãŸã‚‚ã®ã€ã§ã‚ã‚‹ï¼ãªãŠï¼ŒBayesæ¨è«–ã§ã¯,å¯¾æ•°å°¤åº¦$\\log p(y)$ã‚’ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹(Evidence)ã¨ã„ã„ï¼Œæƒ…å ±ç†è«–ã§ã¯è² ã®å¯¾æ•°å°¤åº¦$-\\log p(y)$ã‚’ã‚µãƒ—ãƒ©ã‚¤ã‚º(Surprise)ã¨ã„ã†ï¼\nFristonã®Cellè«–æ–‡(2009)ã«ã‚ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¡Œå‹•$\\alpha$ã‚„è„³ã®å†…éƒ¨çŠ¶æ…‹$\\mu$ã®æ›´æ–°å¼:\n$$ \\begin{align} \\alpha^{*} \u0026amp;= \\underset{\\alpha}{\\rm argmin} ~ F(y) \\\\\n\\mu^{*} \u0026amp;= \\underset{\\mu}{\\rm argmin} ~ F(y) \\end{align} $$\nã«ãŠã‘ã‚‹$F(y)$ã®æœ€å°åŒ–ã¯ï¼Œã€Œè„³ã®å¤–éƒ¨ç’°å¢ƒ$Y$ã«å¯¾ã™ã‚‹è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿${\\{y_t\\}}_{t=1}^{n}$ã®å¯¾æ•°å°¤åº¦(Evidence)ã€ã‚’æœ€å¤§åŒ–ã™ã‚‹éç¨‹ã‚’è¡¨ã—ã¦ã„ã‚‹ï¼ELBOã¨FEPã®é–¢ä¿‚ã‚’ã¾ã¨ã‚ã‚‹ã¨ä»¥ä¸‹ã®è¡¨ã®ã‚ˆã†ã«ãªã‚‹ï¼\n    åŸç† Jensenã®ä¸ç­‰å¼ Bayesæ¨è«–     ELBO Evidence: $ \\log p(y)$ ã®æœ€å¤§åŒ– $\\text{Evidence} \\geq \\mathcal{L}_{ELBO}$ ä¸‹é™$\\mathcal{L}_{ELBO}$ã‚’æœ€å¤§åŒ–   FEP Surprise: $- \\log p(y)$ ã®æœ€å°åŒ– $\\text{Surprise} \\leq F$ ä¸Šé™$F$ã‚’æœ€å°åŒ–    åŸ·ç­†æ™‚ã®å€‹äººçš„ãªç†è§£ã¨ã—ã¦ã¯ï¼ŒFEPã«ãŠã‘ã‚‹å„å¤‰æ•°$\\theta, \\mu, y, \\alpha$ã®æ›´æ–°è¦å‰‡ã¯ï¼Œã€Œè¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ãŸæœ€å°¤æ¨å®šã€ãã®ã‚‚ã®ã ã¨æ€ã£ã¦ã„ã‚‹ï¼è«–æ–‡ã§æå”±ã•ã‚Œã¦ã„ã‚‹è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼$F(y)$æœ€å°åŒ–ã¯ï¼ŒVariational Bayesã«ãŠã‘ã‚‹ELBOæœ€å¤§åŒ–ã¨åŒã˜ã§ã‚ã‚‹ã‹ã‚‰ï¼Œã‚€ã—ã‚4ã¤ã®å¤‰æ•°é–“ã®ãƒ«ãƒ¼ãƒ—æ§‹é€ ï¼ˆã‚°ãƒ©ãƒ•è¡¨ç¾ï¼‰ã®æ–¹ãŒé‡è¦ãªã®ã ã‚ã†ï¼\nFristonã®Natureè«–æ–‡(2010)ã§ã¯ï¼Œè‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼$F(y)$ã®å®šç¾©ãŒã‚ˆã‚Šè¤‡é›‘åŒ–ã—ã¦ãŠã‚Šï¼Œã‚ˆãç†è§£ã—ã¦ã„ãªã„ï¼\n","date":1583798400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583798400,"objectID":"44558e929078445c7ddf4f8e3db92205","permalink":"https://yumaloop.github.io/post/2020-03-10-free_energy_on_bayes_inference/","publishdate":"2020-03-10T00:00:00Z","relpermalink":"/post/2020-03-10-free_energy_on_bayes_inference/","section":"post","summary":"Fristonã®è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼åŸç†ã¨ELBOã®ç­‰ä¾¡æ€§ã«ã¤ã„ã¦","tags":["Bayes"],"title":"Free energy and Bayes inference","type":"post"},{"authors":null,"categories":["Tips"],"content":"   In Git, commit messages are very imoportant to avoid confused commit log in your branch. But in some small projects that you develope alone, thinking about every commit message might be dull.\nI usualy use the following script to send local data to the remote repository. Please try it.\n#!/bin/bash echo -e \u0026quot;\\033[0;32mDeploying updates to GitHub...\\033[0m\u0026quot; # Go To .git root directory cd ~/workspace/{project_name} # Add all changes to git. git add . # Commit changes. msg=\u0026quot;update repo `date`\u0026quot; if [ $# -eq 1 ] then msg=\u0026quot;$1\u0026quot; fi git commit -m \u0026quot;$msg\u0026quot; # Push source and build repos. git push origin master  ","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"813e3de4b4d1b414e68a4ba3b298a8c2","permalink":"https://yumaloop.github.io/post/2020-03-01-bash_script_for_git_push/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/post/2020-03-01-bash_script_for_git_push/","section":"post","summary":"In Git, commit messages are very imoportant to avoid confused commit log in your branch. But in some small projects that you develope alone, thinking about every commit message might be dull.","tags":["Git","Shell"],"title":"Bash script for git push","type":"post"},{"authors":null,"categories":["StatML"],"content":"   Evidence Lower Bound (ELBO) is widely used in variational inference. Recently, according to the massive success of DeepLearning and related models, variational inference (and its technic) gains exposure in the filed of representation learning. For instance, stochastic generative models such as VAE and GAN are famous for their variational aspects.\nELBO Evidence Lower Bound (ELBO) is a lower bound of Log likelihood of $X$ (Evidence) in the model. The below inequality holds based on Cauchy-Schwartz inequality because of the convexity of log function.\n$$ \\begin{aligned} (\\text{Evidence}) \u0026amp;= \\log p(x) \\\\\n\u0026amp;= \\log \\int_{Z} p(x,z) \\\\\n\u0026amp;= \\log \\int_{Z} p(x,z) \\frac{q(z)}{q(z)} \\\\\n\u0026amp;= \\log \\int_{Z} q(z) \\frac{p(x,z)}{q(z)} \\\\\n\u0026amp;= \\log \\mathbb{E}_{z \\sim q} \\left[ \\frac{p(x,z)}{q(z)} \\right] \\\\\n\u0026amp;\\geq \\mathbb{E}_{z \\sim q} \\left[ \\log \\frac{p(x,z)}{q(z)} \\right] \\\\\n\u0026amp;= \\mathbb{E}_{z \\sim q} \\left[ \\log p(x,z) \\right] + H_q(Z) \\\\\n\u0026amp;= ELBO(q) ~~~ (\\text{Evidence Lower Bound, ELBO}) \\end{aligned} $$\nSo that, we can obtain the optimization formula below.\n$$ \\begin{aligned} \\underset{\\theta}{\\rm max} ~ \\log p_{\\theta}(x) \u0026amp;= \\underset{q}{\\rm max} ~ ELBO(q) \\end{aligned} $$\nKL-divergence and ELBO $$ \\begin{aligned} D_{KL}( q(z) \\vert\\vert p(z \\vert x) ) \u0026amp;= \\int_{Z} q(z) \\frac{q(z)}{p(z \\vert x)} \\\\\n\u0026amp;= - H_q(Z) - \\mathbb{E}_{z \\sim q} \\left[ \\log p(z|x) \\right] \\end{aligned} $$\nELBO is considered as the difference between Log likelihood $\\log p(x)$ and KL-divergence $D_{KL}( q(z) \\vert\\vert p(z \\vert x) )$ as below.\n$$ \\begin{align} ELBO \u0026amp;= \\mathbb{E}_{z \\sim q} \\left[ \\log p(x,z) \\right] + H_q(Z) \\\\\n\u0026amp;= \\mathbb{E}_{z \\sim q} \\left[ \\log p(x) + \\log p(z|x) \\right] + H_q(Z) \\\\\n\u0026amp;= \\mathbb{E}_{z \\sim q} \\left[ \\log p(x) \\right] + \\mathbb{E}_{z \\sim q} \\left[ \\log p(z|x) \\right] + H_q(Z) \\\\\n\u0026amp;= \\log p(x) + H_q(Z) + \\mathbb{E}_{z \\sim q} \\left[ \\log p(z \\vert x) \\right] \\\\\n\u0026amp;= \\log p(x) - D_{KL}( q(z) \\vert\\vert p(z \\vert x) ) \\end{align} $$\nSo that, we can obtain the below relation.\n$$ \\begin{align} \\underset{\\theta}{\\rm max} ~ \\log p_{\\theta}(x) \u0026amp;= \\underset{q}{\\rm max} ~ ELBO(q) \\\\\n\u0026amp;= \\underset{q}{\\rm min} ~ D_{KL}( q(z) \\vert\\vert p(z|x) ) \\end{align} $$ \\\n","date":1582502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582502400,"objectID":"4c6e4097764feef99ecf700b24345c9b","permalink":"https://yumaloop.github.io/post/2020-02-24-deriving-elbo/","publishdate":"2020-02-24T00:00:00Z","relpermalink":"/post/2020-02-24-deriving-elbo/","section":"post","summary":"Evidence Lower Bound (ELBO) is widely used in variational inference. Recently, according to the massive success of DeepLearning and related models, variational inference (and its technic) gains exposure in the filed of representation learning.","tags":["Bayes"],"title":"Deriving ELBO","type":"post"},{"authors":null,"categories":["StatML"],"content":"In this post, I introduce you the Counterfactual Regret Minimization (CFR Algorithm). It is mainly used for the algorithm to figure out the optimal strategy of a extensive-form game with incomplete information such as Poker and Mahjong.\nExtensive-form Game  Set, variables  $N: $ set of players  $i \\in N$: player   $A :$ set of actions  $a \\in A: $ action   $H: $set of sequences  $h \\in H: $ sequences (= possible history of actions, $h = (a_1, \\dots, a_t$) $Z \\subseteq H: $ set of terminal histories. $Z = {z \\in H \\vert \\forall h \\in H, z \\notin h }$ $z \\in Z$: sea     Function, relations  $u_i: Z \\to \\mathbb{R}: $ utility function of player $i$ $\\sigma_i: A \\to [0,1]$ a strategy of player $i$, probability distribution on action set $A$. $\\sigma~: A^N \\to [0,1]$ a strategy profile, $\\sigma := (\\sigma_1, \\dots, \\sigma_N)$ $\\pi^{\\sigma}i: H \\to [0,1]: $ probability of history $h$ under a strategy $$\\sigma$ of player $i$ $\\pi^{\\sigma}: H^N \\to [0,1]: $ probability of history $h$ under a strategy profile $\\sigma$    Then, you can also interplate $u_i$ as the function mapping a storategy profile $\\sigma$ to its utility.\n$$ \\begin{align} u_i(\\sigma) \u0026amp;= \\sum_{h \\in Z} u_i(h) \\pi^{\\sigma}(h) \\\\\n\u0026amp;= \\sum_{h \\in Z} u_i(h) \\prod_{i \\in N} \\pi^{\\sigma}_i(h) \\end{align} $$ Nash equilibrium Definition: $(\\text{Nash equilibrium})$\nIn $N$-player extensive game, a strategy profile $\\acute{\\sigma} := (\\acute{\\sigma_1}, \\dots, \\acute{\\sigma_N})$ is the Nash equilibrium if and only if the followings holds.\n$$ \\begin{aligned} u_1(\\acute{\\sigma_1}, \\dots, \\acute{\\sigma_N}) \u0026amp;\\geq \\underset{\\sigma_1}{\\rm max} ~ u_1(\\sigma_1, \\acute{\\sigma_{-1}}) \\\\\nu_2(\\acute{\\sigma_1}, \\dots, \\acute{\\sigma_N}) \u0026amp;\\geq \\underset{\\sigma_2}{\\rm max} ~ u_2(\\sigma_2, \\acute{\\sigma_{-2}}) \\\\\n\u0026amp;~ \\vdots \\\\\nu_N(\\acute{\\sigma_1}, \\dots, \\acute{\\sigma_N}) \u0026amp;\\geq \\underset{\\sigma_N}{\\rm max} ~ u_N(\\sigma_N, \\acute{\\sigma_{-N}}) \\end{aligned} $$\nDefinition: $\\text{(}\\varepsilon\\text{-Nash equilibrium)}$\nIn $N$-player extensive game, a strategy profile $\\acute{\\sigma} := (\\acute{\\sigma_1}, \\dots, \\acute{\\sigma_N})$ is the $\\varepsilon$-Nash equilibrium if and only if the followings holds when $\\forall \\varepsilon \\geq 0$ is given.\n$$ \\begin{aligned} u_1(\\acute{\\sigma_1}, \\dots, \\acute{\\sigma_N}) + \\varepsilon \u0026amp;\\geq \\underset{\\sigma_1}{\\rm max} ~ u_1(\\sigma_1, \\acute{\\sigma_{-1}}) \\\\\nu_2(\\acute{\\sigma_1}, \\dots, \\acute{\\sigma_N}) + \\varepsilon \u0026amp;\\geq \\underset{\\sigma_2}{\\rm max} ~ u_2(\\sigma_2, \\acute{\\sigma_{-2}}) \\\\\n\u0026amp;~ \\vdots \\\\\nu_N(\\acute{\\sigma_1}, \\dots, \\acute{\\sigma_N}) + \\varepsilon \u0026amp;\\geq \\underset{\\sigma_N}{\\rm max} ~ u_N(\\sigma_N, \\acute{\\sigma_{-N}}) \\end{aligned} $$\nRegret matching  Average overall regret of player $i$ at time $T$ï¼š  $$ R_i^T := \\underset{\\sigma_i^*}{\\rm max} ~ \\frac{1}{T} \\sum_{t=1}^{T} \\left( u_i(\\sigma_i^*, \\sigma_{-i}^{t}) - u_i(\\sigma_i^t, \\sigma_{-i}^{t}) \\right) $$\n Average strategy for player $i$ from time $1$ to $T$ï¼š  $$ \\begin{align} \\overline{\\sigma}_i^t(I)(a) \u0026amp;:= \\frac{\\sum_{t=1}^{T} \\pi_i^{\\sigma^t}(I) \\cdot \\sigma^t(I)(a)}{\\sum_{t=1}^{T} \\pi_i^{\\sigma^t}(I)} \\\\\n\u0026amp;= \\frac{\\sum_{t=1}^{T} \\sum_{h \\in I} \\pi_i^{\\sigma^t}(h) \\cdot \\sigma^t(h)(a)}{\\sum_{t=1}^{T} \\sum_{h \\in I} \\pi_i^{\\sigma^t}(h)} \\end{align} $$\nIf the average overall regret holds $R_i^T \\leq \\varepsilon$, the average strategy $\\overline{\\sigma}_i^t(I)(a) $ is $2 \\varepsilon$-Nash equilibrium for player $i$ in time $t$. So that, in order to derive Nash equilibrium, we should minimize the average overall regret $R_i^T$ or its upper bound $\\varepsilon$ according to $R_i^T \\to 0 ~~ (\\varepsilon \\to 0)$.\nCFR Algorithm  Counterfactual utilityï¼š  $$ \\begin{align} u_i(\\sigma, I) = \\frac{\\sum_{h \\in H, h' \\in Z} \\pi_{-i}^{\\sigma}(h)\\pi^{\\sigma}(h,h')u_i(h) }{\\pi_{-i}^{\\sigma}(I)} \\end{align} $$\n immediate counteractual regret of action $a$ in Information set $I$:  $$ \\begin{aligned} R_{i,imm}^{T}(I, a) := \\frac{1}{T} \\sum_{t=1}^{T} \\pi_{-i}^{\\sigma^t}(I) \\left( u_i(\\sigma^t_{I \\to a}, I) - u_i(\\sigma^t, I) \\right) \\end{aligned} $$\n Immediate counterfactual regret of Information set $I$ï¼š  $$ \\begin{aligned} R_{i,imm}^{T}(I) \u0026amp;:= \\underset{a \\in A(I)}{\\rm max} ~ \\frac{1}{T} \\sum_{t=1}^{T} \\pi_{-i}^{\\sigma^t}(I) \\left( u_i(\\sigma^t_{I \\to a}, I) - u_i(\\sigma^t, I) \\right) \\end{aligned} $$\nThe following inequality holds for the average overall regret $R_i^T $ and the immediate counterfactual regret $R_{i,imm}^{T}(I)$:\n$$ \\begin{aligned} R_i^T \\leq \\sum_{I \\in \\mathcal{I}_i} \u0026amp;R_{i,imm}^{T,+}(I) \\\\\nwhere ~~~ \u0026amp;R_{i,imm}^{T, +}(I) := max(R_{i,imm}^{T}(I), 0) \\end{aligned} $$\nSo that, we obtain the sufficient condition of $R_{i,imm}^{T}(I)$ for the average strategy $\\overline{\\sigma}_i^t(I)(a)$ to become a Nash equilibrium strategy as below.\n$$ \\sum_{I \\in \\mathcal{I}_i} R_{i,imm}^{T,+}(I) \\to 0 ~~~ \\Rightarrow ~~~ R_i^T \\to 0 ~~~ \\Rightarrow ~~~ \\varepsilon \\to 0. $$\nNow all we need is to minimize the immediate counterfactual regret $R_{i,imm}^{T}(I)$.\nIn addition, as can be seen from the above formula, the computational complexity of the CFR algorithm depends on the number of information sets $I$. Also, to avoid the complete search of game tree (searching all information sets $I$), subsequent algorithms such as CFR + propose an abstraction of the game state.\nPython code to run CFR algorithm for Kuhn Poker import numpy as np # Number of actions a player can take at a decision node. _N_ACTIONS = 2 _N_CARDS = 3 def main(): \u0026quot;\u0026quot;\u0026quot; Run iterations of counterfactual regret minimization algorithm. \u0026quot;\u0026quot;\u0026quot; i_map = {} # map of information sets n_iterations = 10000 expected_game_value = 0 for _ in range(n_iterations): expected_game_value += cfr(i_map) for _, v in i_map.items(): v.next_strategy() expected_game_value /= n_iterations display_results(expected_game_value, i_map) def cfr(i_map, history=\u0026quot;\u0026quot;, card_1=-1, card_2=-1, pr_1=1, pr_2=1, pr_c=1): \u0026quot;\u0026quot;\u0026quot; Counterfactual regret minimization algorithm. Parameters ---------- i_map: dict Dictionary of all information sets. history : [{'r', 'c', 'b'}], str A string representation of the game tree path we have taken. Each character of the string represents a single action: 'r': random chance action 'c': check action 'b': bet action card_1 : (0, 2), int player A's card card_2 : (0, 2), int player B's card pr_1 : (0, 1.0), float The probability that player A reaches `history`. pr_2 : (0, 1.0), float The probability that player B reaches `history`. pr_c: (0, 1.0), float The probability contribution of chance events to reach `history`. \u0026quot;\u0026quot;\u0026quot; if is_chance_node(history): return chance_util(i_map) if is_terminal(history): return terminal_util(history, card_1, card_2) n = len(history) is_player_1 = n % 2 == 0 info_set = get_info_set(i_map, card_1 if is_player_1 else card_2, history) strategy = info_set.strategy if is_player_1: info_set.reach_pr += pr_1 else: info_set.reach_pr += pr_2 # Counterfactual utility per action. action_utils = np.zeros(_N_ACTIONS) for i, action in enumerate([\u0026quot;c\u0026quot;, \u0026quot;b\u0026quot;]): next_history = history + action if is_player_1: action_utils[i] = -1 * cfr(i_map, next_history, card_1, card_2, pr_1 * strategy[i], pr_2, pr_c) else: action_utils[i] = -1 * cfr(i_map, next_history, card_1, card_2, pr_1, pr_2 * strategy[i], pr_c) # Utility of information set. util = sum(action_utils * strategy) regrets = action_utils - util if is_player_1: info_set.regret_sum += pr_2 * pr_c * regrets else: info_set.regret_sum += pr_1 * pr_c * regrets return util def is_chance_node(history): \u0026quot;\u0026quot;\u0026quot; Determine if we are at a chance node based on tree history. \u0026quot;\u0026quot;\u0026quot; return history == \u0026quot;\u0026quot; def chance_util(i_map): expected_value = 0 n_possibilities = 6 for i in range(_N_CARDS): for j in range(_N_CARDS): if i != j: expected_value += cfr(i_map, \u0026quot;rr\u0026quot;, i, j, 1, 1, 1/n_possibilities) return expected_value/n_possibilities def is_terminal(history): possibilities = { \u0026quot;rrcc\u0026quot;: True, \u0026quot;rrcbc\u0026quot;: True, \u0026quot;rrcbb\u0026quot;: True, \u0026quot;rrbc\u0026quot;: True, \u0026quot;rrbb\u0026quot;: True} return history in possibilities def terminal_util(history, card_1, card_2): n = len(history) card_player = card_1 if n % 2 == 0 else card_2 card_opponent = card_2 if n % 2 == 0 else card_1 if history == \u0026quot;rrcbc\u0026quot; or history == \u0026quot;rrbc\u0026quot;: # Last player folded. The current player wins. return 1 elif history == \u0026quot;rrcc\u0026quot;: # Showdown with no bets return 1 if card_player \u0026gt; card_opponent else -1 # Showdown with 1 bet assert(history == \u0026quot;rrcbb\u0026quot; or history == \u0026quot;rrbb\u0026quot;) return 2 if card_player \u0026gt; card_opponent else -2 def card_str(card): if card == 0: return \u0026quot;J\u0026quot; elif card == 1: return \u0026quot;Q\u0026quot; elif card == 2: return \u0026quot;K\u0026quot; def get_info_set(i_map, card, history): \u0026quot;\u0026quot;\u0026quot; Retrieve information set from dictionary. \u0026quot;\u0026quot;\u0026quot; key = card_str(card) + \u0026quot; \u0026quot; + history info_set = None if key not in i_map: info_set = InformationSet(key) i_map[key] = info_set return info_set return i_map[key] class InformationSet(): def __init__(self, key): self.key = key self.regret_sum = np.zeros(_N_ACTIONS) self.strategy_sum = np.zeros(_N_ACTIONS) self.strategy = np.repeat(1/_N_ACTIONS, _N_ACTIONS) self.reach_pr = 0 self.reach_pr_sum = 0 def next_strategy(self): self.strategy_sum += self.reach_pr * self.strategy self.strategy = self.calc_strategy() self.reach_pr_sum += self.reach_pr self.reach_pr = 0 def calc_strategy(self): \u0026quot;\u0026quot;\u0026quot; Calculate current strategy from the sum of regret. \u0026quot;\u0026quot;\u0026quot; strategy = self.make_positive(self.regret_sum) total = sum(strategy) if total \u0026gt; 0: strategy = strategy / total else: n = _N_ACTIONS strategy = np.repeat(1/n, n) return strategy def get_average_strategy(self): \u0026quot;\u0026quot;\u0026quot; Calculate average strategy over all iterations. This is the Nash equilibrium strategy. \u0026quot;\u0026quot;\u0026quot; strategy = self.strategy_sum / self.reach_pr_sum # Purify to remove actions that are likely a mistake strategy = np.where(strategy \u0026lt; 0.001, 0, strategy) # Re-normalize total = sum(strategy) strategy /= total return strategy def make_positive(self, x): return np.where(x \u0026gt; 0, x, 0) def __str__(self): strategies = ['{:03.2f}'.format(x) for x in self.get_average_strategy()] return '{} {}'.format(self.key.ljust(6), strategies) def display_results(ev, i_map): print('player 1 expected value: {}'.format(ev)) print('player 2 expected value: {}'.format(-1 * ev)) print() print('player 1 strategies:') sorted_items = sorted(i_map.items(), key=lambda x: x[0]) for _, v in filter(lambda x: len(x[0]) % 2 == 0, sorted_items): print(v) print() print('player 2 strategies:') for _, v in filter(lambda x: len(x[0]) % 2 == 1, sorted_items): print(v) if __name__ == \u0026quot;__main__\u0026quot;: main()  ","date":1581292800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581292800,"objectID":"b0463a81c533ce0008fd98312178c36b","permalink":"https://yumaloop.github.io/post/2020-02-10-counterfactual-regret-minimization/","publishdate":"2020-02-10T00:00:00Z","relpermalink":"/post/2020-02-10-counterfactual-regret-minimization/","section":"post","summary":"In this post, I introduce you the Counterfactual Regret Minimization (CFR Algorithm). It is mainly used for the algorithm to figure out the optimal strategy of a extensive-form game with incomplete information such as Poker and Mahjong.","tags":["RL","Game Theory","Regret","Python"],"title":"Counterfactual Regret Minimization","type":"post"},{"authors":null,"categories":["Random"],"content":"   In this note, I describe how to install NVIDIA GPU and set up CUDA/cuDNN on Ubuntu 16.04LTS machine that has been clean booted. Also, I write down some linux commands used in debugging, since knowing your machine in detail would lead to resolving some errors related to the machine environment. This article could be updated from time to time.\n Example: My Ubuntu GPU machine (2020/01/10)\n OS :  Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-145-generic x86_64)   RAM(16GB) :  Memory: Kingston 8GB 288-Pin DDR4 SDRAM DDR4 2133 (PC4 17000) x2   ROM(250GB):  SSD: Samsung SSD 750 EVO 250GB (/dev/sda) HDD: Seagate Barracuda ST2000DM001 Desktop SATA (/dev/sdb)   CPU(x8) :  Intel Core i7-6700 CPU @ 3.40GHz   GPU(x1) :  NVIDIA Geforce GTX 1080  NVIDIA CUDA : 10.0.130 (/usr/local/cuda-10.0/) NVIDIA cuDNN : 7.4.2.24 (/usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.2)  Python3 : 3.6.9 (/usr/bin/python3.6) Python2 : 2.7.12 (/usr/bin/python) tensorflow 1.13.1 ($HOME/.local/lib/python3.6/site-packages) tensorflow-gpu 1.13.1 ($HOME/.local/lib/python3.6/site-packages) keras 2.2.4 ($HOME/.local/lib/python3.6/site-packages) pytorch 1.2.0 ($HOME/.local/lib/python3.6/site-packages)          Table of contents\n Operating System  Checking Linux OS Checking Linux distribution Checking Linux kernel   Storage (ROM)  Checking ROM devices Checking the number of files Checking disk space   Memory (RAM)  Checking RAM devices Checking memory space   CPU  Checking CPU devices   GPU  Checking GPU devices NVIDIA driver \u0026amp; CUDA/cuDNN  Installing NVIDIA driver Installing NVIDIA CUDA Installing NVIDIA cuDNN     I/O  Checking X11 display manager (DM)     Operating System Checking Linux OS uname command shows 1.OS Name, 2.Hostname, 3.Release, 4.Version, 5,Hardware Architecture, 6,CPU type, 7.Platform, 8.OS Name, respectively,\n$ uname -a Linux XXXX 4.4.0-145-generic #171-Ubuntu SMP Tue Mar 26 12:43:40 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux  Checking Linux distribution /etc/issue contains information about Linux distribution.\n$ cat /etc/issue Ubuntu 16.04.6 LTS \\n \\l  /etc/lsb-release contains the same information.\n$ cat /etc/lsb-release DISTRIB_ID=Ubuntu DISTRIB_RELEASE=16.04 DISTRIB_CODENAME=xenial DISTRIB_DESCRIPTION=\u0026quot;Ubuntu 16.04.6 LTS\u0026quot;  /etc/os-release contains the same information.\n$ cat /etc/os-release NAME=â€œUbuntuâ€ VERSION=â€œ16.04.6 LTS (Xenial Xerus)â€ ID=ubuntu ID_LIKE=debian PRETTY_NAME=â€œUbuntu 16.04.6 LTSâ€ VERSION_ID=â€œ16.04\u0026quot; HOME_URL=â€œhttp://www.ubuntu.com/â€ SUPPORT_URL=â€œhttp://help.ubuntu.com/â€ BUG_REPORT_URL=â€œhttp://bugs.launchpad.net/ubuntu/â€ VERSION_CODENAME=xenial UBUNTU_CODENAME=xenial  Checking Linux kernel /proc/version contains information about Linux kernel.\n$ cat /proc/version Linux version 4.4.0-159-generic (buildd@lgw01-amd64-042) (gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10) ) #187-Ubuntu SMP Thu Aug 1 16:28:06 UTC 2019  Storage (ROM) Storage device (HDD, SSD) and file systems.\nChecking ROM devices df commad shows information about ROM (HDD) devices\n$ df -h Filesystem Size Used Avail Use% Mounted on udev 7.8G 0 7.8G 0% /dev tmpfs 1.6G 46M 1.6G 3% /run /dev/sda1 214G 165G 39G 81% / tmpfs 7.9G 208K 7.9G 1% /dev/shm tmpfs 5.0M 4.0K 5.0M 1% /run/lock tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup /dev/loop3 384K 384K 0 100% /snap/patchelf/93 /dev/loop1 384K 384K 0 100% /snap/patchelf/87 none 7.9G 2.5M 7.9G 1% /tmp/guest-qyuodw tmpfs 1.6G 64K 1.6G 1% /run/user/998 /dev/loop4 90M 90M 0 100% /snap/core/8213 /dev/loop0 90M 90M 0 100% /snap/core/8268 tmpfs 1.6G 0 1.6G 0% /run/user/1001  Checking the number of files wc command shows the number of files under the current dir.\n$ du -hsc * 689M\tResearch 4.0K\tbuild 106M\tdataset 4.0K\tdocker 9.3M\tgym 50M\tkaggle 2.6M\tlatent.gif 2.0G\topencv 122G\tworkspace 4.0K\tãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ 4.0K\tãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ 4.0K\tãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ— 4.0K\tãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ 4.0K\tãƒ“ãƒ‡ã‚ª 4.0K\tãƒ”ã‚¯ãƒãƒ£ 4.0K\tãƒŸãƒ¥ãƒ¼ã‚¸ãƒƒã‚¯ 4.0K\tå…¬é–‹ 125G\tåˆè¨ˆ  ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡ã‚’ç¢ºèªã—ãŸã„ df -hã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ã†\nã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç›´ä¸‹ã«ã‚ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãŠã‚ˆã³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡ã¨ãã®åˆè¨ˆã‚’è¡¨ç¤ºã™ã‚‹\n$ du -hsc * 689M\tResearch 4.0K\tbuild 106M\tdataset 4.0K\tdocker 9.3M\tgym 50M\tkaggle 2.6M\tlatent.gif 2.0G\topencv 122G\tworkspace 4.0K\tãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ 4.0K\tãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ 4.0K\tãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ— 4.0K\tãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ 4.0K\tãƒ“ãƒ‡ã‚ª 4.0K\tãƒ”ã‚¯ãƒãƒ£ 4.0K\tãƒŸãƒ¥ãƒ¼ã‚¸ãƒƒã‚¯ 4.0K\tå…¬é–‹  ãƒ¡ãƒ¢ãƒªï¼ˆRAMï¼‰ ãƒ¡ãƒ¢ãƒªãƒ‡ãƒã‚¤ã‚¹ã‚’ç¢ºèªã—ãŸã„ /proc/meminfoã‚’ã¿ã‚‹\nãƒ¡ãƒ¢ãƒªã®è©³ç´°æƒ…å ±ãŒè¡¨ç¤ºã•ã‚Œã‚‹\n$ cat /proc/meminfo MemTotal: 16377200 kB MemFree: 3077848 kB MemAvailable: 15767804 kB Buffers: 363052 kB Cached: 12274992 kB SwapCached: 66936 kB Active: 8048088 kB Inactive: 4689560 kB Active(anon): 25860 kB Inactive(anon): 86584 kB ... HugePages_Total: 0 HugePages_Free: 0 HugePages_Rsvd: 0 HugePages_Surp: 0 Hugepagesize: 2048 kB DirectMap4k: 1907316 kB DirectMap2M: 14815232 kB DirectMap1G: 0 kB  ãƒ¡ãƒ¢ãƒªã®ç©ºãå®¹é‡ã‚’ç¢ºèªã—ãŸã„ freeã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ã†\n$ free total used free shared buff/cache available Mem: 16377148 2470228 314496 17140 13592424 13460232 Swap: 16720892 431568 16289324  vmstatã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ã†\n$ vmstat procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 431584 267696 944212 12638044 0 2 389 15 0 0 6 2 91 0 0  topã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ã†\n$ top top - 15:55:05 up 64 days, 23:12, 5 users, load average: 1.00, 1.04, 1.07 Tasks: 232 total, 2 running, 230 sleeping, 0 stopped, 0 zombie %Cpu(s): 9.1 us, 3.5 sy, 0.0 ni, 86.9 id, 0.5 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 16377148 total, 271964 free, 2527528 used, 13577656 buff/cache KiB Swap: 16720892 total, 16289228 free, 431664 used. 13403420 avail Mem ...  CPU CPUãƒ‡ãƒã‚¤ã‚¹ã‚’ç¢ºèªã—ãŸã„ /proc/cpuinfoã‚’ã¿ã‚‹\nCPUã®ã‚³ã‚¢ã”ã¨ã«è©³ç´°æƒ…å ±ãŒè¡¨ç¤ºã•ã‚Œã‚‹\n $ cat /proc/cpuinfo processor : 0 vendor_id : GenuineIntel cpu family : 6 model : 94 model name : Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz stepping : 3 microcode : 0xc6 cpu MHz : 800.062 cache size : 8192 KB physical id : 0 siblings : 8 ... processor : 1 vendor_id : GenuineIntel cpu family : 6 model : 94 model name : Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz ...  GPU GPUãƒ‡ãƒã‚¤ã‚¹ã®ç¢ºèª lswsã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ã†\n$ sudo lshw -C display *-display è©³ç´°: VGA compatible controller è£½å“: GP104 [GeForce GTX 1080] ãƒ™ãƒ³ãƒ€ãƒ¼: NVIDIA Corporation ç‰©ç†ID: 0 ãƒã‚¹æƒ…å ±: pci@0000:01:00.0 ãƒãƒ¼ã‚¸ãƒ§ãƒ³: a1 å¹…: 64 bits ã‚¯ãƒ­ãƒƒã‚¯: 33MHz æ€§èƒ½: pm msi pciexpress vga_controller bus_master cap_list rom è¨­å®š: driver=nvidia latency=0 ãƒªã‚½ãƒ¼ã‚¹: irq:317 ãƒ¡ãƒ¢ãƒªãƒ¼:de000000-deffffff ãƒ¡ãƒ¢ãƒªãƒ¼:c0000000-cfffffff ãƒ¡ãƒ¢ãƒªãƒ¼:d0000000-d1ffffff IOãƒãƒ¼ãƒˆ:e000(ã‚µã‚¤ã‚º=128) ãƒ¡ãƒ¢ãƒªãƒ¼:df000000-df07ffff  lspciã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ã†\nLinuxã«æ­è¼‰ã•ã‚Œã¦ã„ã‚‹PCIãƒã‚¹ã®æƒ…å ±ã‚’è¡¨ç¤ºã™ã‚‹ï¼\n$ lspci | grep -i nvidia 01:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1080] (rev a1) 01:00.1 Audio device: NVIDIA Corporation GP104 High Definition Audio Controller (rev a1)  NVIDIAãƒ‰ãƒ©ã‚¤ãƒã¨CUDA/cuDNNã®å°å…¥ NVIDIAãƒ‰ãƒ©ã‚¤ãƒã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« 1.ä¸‹è¨˜ãƒªãƒ³ã‚¯ã‹ã‚‰ï¼Œè‡ªåˆ†ã®GPUã«ã‚ã†ãƒ‰ãƒ©ã‚¤ãƒã‚’æ¤œç´¢ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ï¼\nhttps://www.nvidia.co.jp/Download/index.aspx?lang=jp\nãŸã¨ãˆã°ï¼ŒGPUã€ŒNVIDIA GeForce 1080ã€ã«å¯¾å¿œã—ãŸãƒ‰ãƒ©ã‚¤ãƒã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚‹ï¼\nï¼\næ–°ã—ãGPUãƒ‰ãƒ©ã‚¤ãƒï¼ˆNVIDIAãƒ‰ãƒ©ã‚¤ãƒï¼‰ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å‰ã«ï¼Œæ—¢ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹GPUãƒ‰ãƒ©ã‚¤ãƒã‚’ç¢ºèªã™ã‚‹ï¼  aptã«NVIDIAãƒ‰ãƒ©ã‚¤ãƒã‚’æä¾›ã—ã¦ã„ã‚‹xorg-edgersãƒ¬ãƒã‚¸ãƒˆãƒªã‚’è¿½åŠ ã™ã‚‹ï¼\naptã§NVIDIAãƒ‰ãƒ©ã‚¤ãƒã€Œnvidia-396ã€ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ï¼Œãƒã‚·ãƒ³ã‚’å†èµ·å‹•ï¼\nCUDAã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« ï¼ˆæ³¨æ„ï¼‰CUDAãƒ»cuDNNãƒ»tensorFlow-gpuã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’åˆã‚ã›ã‚‹å¿…è¦ãŒã‚ã‚‹ï¼\n  CUDAã®å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚ˆãèª­ã‚€ï¼\nCUDA Toolkit Documentation https://docs.nvidia.com/cuda/index.html\n  ä¸‹è¨˜ãƒªãƒ³ã‚¯ã‹ã‚‰ï¼ŒNVIDIAãƒ‰ãƒ©ã‚¤ãƒã«å¯¾å¿œã™ã‚‹CUDAã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¢ºèªã™ã‚‹\nCUDA Toolkit Documentation \u0026gt; Release Notes \u0026gt; 1. CUDA Toolkit Major Components \u0026gt; CUDA Driver https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html\n   ä¸‹è¨˜ãƒªãƒ³ã‚¯ã‹ã‚‰ï¼Œtensorflow-gpuã«å¯¾å¿œã™ã‚‹cuDNN/CUDAã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¢ºèªã™ã‚‹\nTensorFlow (Linux) - ãƒ†ã‚¹ãƒˆæ¸ˆã¿ã®ãƒ“ãƒ«ãƒ‰è¨­å®š\nhttps://www.tensorflow.org/install/source#linux\n  CUDAãƒ»cuDNNãƒ»tensorFlow-gpuã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèªã‚’çµ‚ãˆãŸï¼\nä»Šå›ã¯ï¼Œä»¥ä¸‹ã§ç’°å¢ƒæ§‹ç¯‰ã‚’ã™ã‚‹ï¼\n  Python 3.6.9 tensorflow-gpu 1.13.1 CUDA 10.0 cuDNN 7.4    ä¸‹è¨˜ãƒªãƒ³ã‚¯ã‹ã‚‰ï¼Œè‡ªåˆ†ã®ç’°å¢ƒã«ã‚ã£ãŸã€ŒCUDA Toolkitãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã€ã‚’ç¢ºèªã—ï¼Œãƒã‚·ãƒ³ã¸ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ï¼\nCUDA Toolkit Archive https://developer.nvidia.com/cuda-toolkit-archive\n  ä»Šå›ã¯ï¼ŒCUDA10.0ã§ï¼Œãƒã‚·ãƒ³ã®ç’°å¢ƒã¨ã—ã¦ï¼Œä»¥ä¸‹ã‚’é¸æŠï¼\n  Operating System: Linux Architecture: x86_64 Distribution: Ubuntu Version: 16.04 Installer Type: deb [network]   ï¼ˆæ³¨æ„ï¼‰https://developer.nvidia.com/cuda-downloadsã¯ï¼Œæœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ãªã®ã§ï¼Œã“ã“ã‹ã‚‰å®‰æ˜“ã«CUDAã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã¯ã„ã‘ãªã„ï¼ç‰¹ã«ï¼Œtensorflow-gpuã¯ï¼Œæœ€æ–°ã®CUDA Toolkitã«å¯¾å¿œã—ã¦ã„ãªã„ã®ã§æ³¨æ„ã™ã‚‹ï¼CUDAã¨Tensorflow-gpuã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒã‚ã£ã¦ã„ãªã„ã¨ï¼ŒãŸã¨ãˆã°ImportError: libcublas.so.10.0ãŒç™ºç”Ÿã™ã‚‹ï¼\nå¯¾å¿œã™ã‚‹CUDA Toolkitï¼ˆCUDA 10.0ï¼‰ã®.debãƒ•ã‚¡ã‚¤ãƒ«(network)ã¯ã€Œcuda-repo-ubuntu1604_10.0.130-1_amd64.debã€ã¨ãªã‚‹ï¼ ã“ã®.debãƒ•ã‚¡ã‚¤ãƒ«ã‚’wgetã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ã£ã¦ï¼Œãƒã‚·ãƒ³ã¸ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ï¼\nãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸCUDA Toolkitãƒ‘ãƒƒã‚±ãƒ¼ã‚¸(.deb)ã‚’ï¼Œãƒã‚·ãƒ³ã¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹  dpkgã‚³ãƒãƒ³ãƒ‰ã§CUDA Toolkitãƒ‘ãƒƒã‚±ãƒ¼ã‚¸(.deb)ã‚’cudaãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¨ã—ã¦ä¿å­˜ã—ã¾ã™ï¼ã•ã‚‰ã«ï¼Œaptã‚³ãƒãƒ³ãƒ‰ã§cudaãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ï¼ æ³¨æ„ï¼šå…¬å¼ã«æ›¸ã‹ã‚Œã¦ã„ã‚‹sudo apt-get install cudaã‚’å®Ÿè¡Œã™ã‚‹ã¨è‡ªå‹•çš„ã«æœ€æ–°ç‰ˆã®CUDAãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã‚‹ï¼\nã“ã‚Œã§CUDA Toolkitï¼ˆCUDA 10.0ï¼‰ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¯å®Œäº†ï¼\næ¬¡ã«ï¼Œç’°å¢ƒå¤‰æ•°ï¼ˆPATHï¼‰ã‚’è¨­å®šã™ã‚‹ï¼\ncuDNNã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« PATHãƒã‚§ãƒƒã‚¯ ãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤ X11ãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤ãƒãƒãƒ¼ã‚¸ãƒ£(DM)ã‚’ç¢ºèª /etc/X11/default-display-managerã‚’ã¿ã‚‹\n","date":1579132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579132800,"objectID":"1269f2e0e4402c1d9168bd4342744f90","permalink":"https://yumaloop.github.io/post/2020-01-16-setup-gpu-machine-for-ml/","publishdate":"2020-01-16T00:00:00Z","relpermalink":"/post/2020-01-16-setup-gpu-machine-for-ml/","section":"post","summary":"In this note, I describe how to install NVIDIA GPU and set up CUDA/cuDNN on Ubuntu 16.04LTS machine that has been clean booted. Also, I write down some linux commands used in debugging, since knowing your machine in detail would lead to resolving some errors related to the machine environment.","tags":["Env"],"title":"Setting up a GPU machine for Machine Learning","type":"post"},{"authors":null,"categories":["Random"],"content":"Prof. George M. Whitesides is a top-level researcher on chemistry and also known as one of the highest h-index researchers in the world. He explains his unique writing techniques called Outline Method in the article, \u0026ldquo;Whitesides' Group: Writing a Paper\u0026rdquo;. I\u0026rsquo;ve translated that into Japanese and publish it here.\nOriginal paper This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n  Japanese translation This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n  ","date":1574640000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574640000,"objectID":"981967c8ac223875067a00abfb3c335b","permalink":"https://yumaloop.github.io/post/2019-11-25-whitesides_outline_method/","publishdate":"2019-11-25T00:00:00Z","relpermalink":"/post/2019-11-25-whitesides_outline_method/","section":"post","summary":"Prof. George M. Whitesides is a top-level researcher on chemistry and also known as one of the highest h-index researchers in the world. He explains his unique writing techniques called Outline Method in the article, \u0026ldquo;Whitesides' Group: Writing a Paper\u0026rdquo;.","tags":["Tips","Book"],"title":"Whiteside's Outline Method","type":"post"},{"authors":null,"categories":["StatML"],"content":"   â€‹\tEM algorithm is an algorithm for deriving the maximum likelihood estimator (MLE), which is generally applied to statistical methods for incomplete data. Originally, the concept of â€œincomplete data and complete dataâ€ was established to handle missing data, but by extending the definition, it can be applied to cut data, censored data, mixed distribution models, Robust distribution models, and latent data. It can also be applied to variable models, and Bayesian modeling.\nâ€‹\tAlso, a number of statistical approach for clustering and unsupervised learning (eg, k-means, Gaussian mixture models) can be generalized as EM algorithms when focusing on the computational process. In addition, researches on analyzing the EM algorithm from the viewpoint of information geometry has been active, and applying EM algorithm to the stochastic model including an exponential family can be summarized in the form of e-projection / m-projection.\n1. Statistical inference  Objectives: To find out the probability distribution $q(x)$ that a certain variable $x \\in X$ follows.\n Namely, when considering a stochastic model $p(x \\vert \\theta)$ determined by the parameter $\\theta \\in \\Theta$ and detecting the optimal parameter $\\theta^{*} \\in \\Theta$ from dataset $ \\mathcal{D} := {\\{x_i\\}}_{i=1}^{n}$, the follwing Approximation holds.\n$$ \\begin{align} x \\sim q(x) \\approx p(x|\\theta) \\end{align} $$\nThis is called a statistical inference (or statistical estimation).\n2. Maximum likelihood estimation The most basic algorithm for statistical inference is maximum likelihood estimation (MLE). A log likelihood function of the stochastic model $p(x \\vert \\theta)$ is defined as\n$$ \\begin{align} \\ell(\\theta | x) := \\log p(x | \\theta) \\end{align} $$\nand an empirical objective function of $\\theta$:\n$$ \\begin{align} J(\\theta) := \\frac{1}{n} \\sum_{i=1}^{n} \\ell(\\theta | x_i) \\end{align} $$\nthat depends on dataset $ \\mathcal{D} := {{x_i}}_{i=1}^{n}$ can be obtained, MLE of parameter $\\theta$ is derived as follows.\n$$ \\begin{align} \\hat{\\theta}_{MLE} = \\underset{\\theta \\in \\Theta}{\\rm argmax} ~ J(\\theta) \\end{align} $$\n3. EM algorithm Let\u0026rsquo;s define the following data categories.\n Complete data $Y \\in \\mathcal{Y}$ :  not observable but completely follows the true distribution $p(y)$   Imcomplete data $X \\in \\mathcal{X}$ :  observable but not completely follows the true distribution $p(x)$    In general, the relationship complete data $y$ and incomplete data $x$ is a one-to-many relationship. but here, as a convenient assumption, I introduce a latent variable $z \\in Z$ to express this constraint, that is assume $y = [x, z]$ holds. Considering the stochastic model for the complete data $x$,\n$$ \\begin{align} p(y | \\theta) = p(x,z | \\theta) \\end{align} $$\n Complete data $\\{X,Z\\} \\in \\mathcal{X \\times Z}$ :  not observable but completely follows the true distribution $p(x,z)$   Imcomplete data $X \\in \\mathcal{X}$ :  observable but not completely follows the true distribution $p(x)$    data sample $x_i$ cannot be observed and its likelihood $p(x_i \\vert \\theta)$ cannot be calculated. However, for pair data sample $\\{x_i, z_i\\}$ can be observed and its likelihood $p(x_i, z_i \\vert \\theta)$ can be calculated.\n$$ \\begin{align} p(x_i | \\theta) \u0026amp;= \\int_{Z} p(x_i, z_i | \\theta) ~ dz \\\n\\end{align} $$\nBy using this formula, the estimated value of $\\hat{\\theta}_{MLE}$ can be obtained by approximating $p(x,z \\vert \\theta)$, the likelihood function of complete data $\\{x, z\\}$. The procedure to derive the estimated value of $\\hat{\\theta}_{MLE}$ is called EM algorithm because it is an iterative method that repeats E-step and M-step alternately.\n EM algorithm\n  Initialize $\\theta$ with $\\theta^{0}$.\n  For each step $t$:\nE Step: Update the expectation value $Q$.\n$$ \\begin{aligned} Q(\\theta | \\theta^{(t)}) \u0026amp;= \\mathbb{E}_{z \\sim p(z \\vert x, \\theta^{(t)})} \\left[ \\log p(x, z \\vert \\theta) \\right] \\\\ \u0026amp;\\simeq \\sum_{i=1}^{n} p(z_i \\vert x_i, \\theta^{(t)}) \\log p(x_i, z_i \\vert \\theta) \\\\ \u0026amp;= \\sum_{i=1}^{n} p(z_i \\vert x_i, \\theta^{(t)}) \\log p(z_i \\vert x_i, \\theta) + Const. \\end{aligned}$$\nM Step: Derive the optimal parameter ${\\theta}^{(t+1)}$ that maximize $Q$ value.\n$$\\begin{aligned} {\\theta}^{(t+1)} \u0026amp;= \\underset{\\theta \\in \\Theta}{\\rm argmax} ~ Q(\\theta \\vert {\\theta}^{(t)}) \\end{aligned}$$\n  Consider the convergence value $\\theta^{(\\infty)}$ as the algorithm output $\\hat{\\theta}_{EM}$.\n   As a result, the estimated value of $\\hat{\\theta}_{MLE}$ is derived as $\\hat{\\theta}_{EM}$ and the following holds.\n$$ \\begin{align} \\hat{\\theta}_{MLE} \\approx \\hat{\\theta}_{EM}, ~~~ p(x|\\hat{\\theta}_{MLE} ) \\approx p(x|\\hat{\\theta}_{EM} ) \\end{align} $$\nAlso, the summarized formula of calculations in E step and M step is as follows.\n  For each step $t$:\nEM Step:\n$$\\begin{align} \\theta^{(t+1)} \u0026amp;= \\underset{\\theta \\in \\Theta}{\\rm argmax} ~ \\mathbb{E}_{z \\sim p(z \\vert x, \\theta^{(t)})} \\left[ \\log p(x, z \\vert \\theta) \\right] \\\\ \u0026amp;= \\underset{\\theta \\in \\Theta}{\\rm argmax} ~ \\sum_{i=1}^{n} p(z_i \\vert x_i, \\theta^{(t)}) \\log p(x_i, z_i \\vert \\theta) \\end{align} $$\n   References  PRML Chapter 9: Mixture models and EM 9ç«  EMã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  - ã€Œ21 ä¸–ç´€ã®çµ±è¨ˆç§‘å­¦ã€ç¬¬ III å·» æ—¥æœ¬çµ±è¨ˆå­¦ä¼š, 2008 è§£èª¬ EMã‚¢ãƒ«ã‚³ã‚™ãƒªã‚¹ã‚™ãƒ ã®å¹¾ä½•å­¦ - èµ¤ç©‚æ˜­å¤ªéƒ, é›»å­æŠ€è¡“ç·åˆç ”ç©¶æ‰€ EMã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ç¥çµŒå›è·¯ç¶², 2000, çµ±è¨ˆæ•°ç†ç ”ç©¶æ‰€  ","date":1569369600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569369600,"objectID":"69426175cd5772c606e672386dab8632","permalink":"https://yumaloop.github.io/post/2019-09-25-em-algorithm/","publishdate":"2019-09-25T00:00:00Z","relpermalink":"/post/2019-09-25-em-algorithm/","section":"post","summary":"â€‹\tEM algorithm is an algorithm for deriving the maximum likelihood estimator (MLE), which is generally applied to statistical methods for incomplete data. Originally, the concept of â€œincomplete data and complete dataâ€ was established to handle missing data, but by extending the definition, it can be applied to cut data, censored data, mixed distribution models, Robust distribution models, and latent data.","tags":["Bayes"],"title":"EM Algorithm","type":"post"},{"authors":null,"categories":["Finance"],"content":"   1. è³‡æœ¬ä¸»ç¾©ã®åŸç† è³‡æœ¬ä¸»ç¾© - Capitalism\nè³‡æœ¬ä¸»ç¾©ã¨ã¯ä½•ã‹ï¼Ÿã‚’è€ƒãˆã‚‹éš›ã«ã¯ï¼Œè³‡æœ¬ä¸»ç¾©ã§ã¯ãªã„ã‚‚ã®ã¯ä½•ã‹ï¼Ÿã‚’è€ƒãˆï¼Œãã®å·®åˆ†ã‚’ã¾ã¨ã‚ç›´ã›ã°è‰¯ã„ï¼è³‡æœ¬ä¸»ç¾©ã®å¯¾ç¾©èªã¯ï¼Œå…±ç”£ä¸»ç¾©ï¼ˆç¤¾ä¼šä¸»ç¾©ï¼‰ã«ãªã‚‹ï¼ã“ã‚Œã‚‰ã‚’æ’ä»–çš„ã«æ¯”è¼ƒã™ã‚Œã°ï¼Œãã®å®šç¾©ï¼ˆæ”¿æ²»çš„æ¦‚å¿µã§ã‚ã‚‹\u0026quot;è‡ªç”±\u0026quot;ã‚„\u0026quot;æ°‘ä¸»ä¸»ç¾©\u0026quot;ã«ã¤ã„ã¦ã¯é™¤å¤–ã™ã‚‹ï¼‰ã¯ï¼Œæ¬¡ã®ã‚ˆã†ã«ã¾ã¨ã‚ã‚‰ã‚Œã‚‹ã¨æ€ã†ï¼\n è³‡æœ¬ä¸»ç¾©ï¼šå€‹äººãŒè³‡æœ¬ã‚’æ‰€æœ‰ã§ãã‚‹ å…±ç”£ä¸»ç¾©ï¼šå€‹äººãŒè³‡æœ¬ã‚’æ‰€æœ‰ã§ããªã„  éå¸¸ã«ã‚·ãƒ³ãƒ—ãƒ«ãªå®šç¾©ã ãŒï¼Œåƒ•ã¯æ°—ã«å…¥ã£ã¦ã„ã‚‹ã—ï¼Œã“ã®å®šç¾©ã‚’ä½¿ã£ã¦æ–‡æ„è§£é‡ˆã«è‹¦é›£ã—ãŸã“ã¨ã¯ãªã„ï¼è¦ã™ã‚‹ã«ï¼Œä½•ã‚‰ã‹ã®æ–‡è„ˆã§ã€Œè³‡æœ¬ä¸»ç¾©ã€ã¨ã„ã†ãƒ¯ãƒ¼ãƒ‰ãŒä½¿ã‚ã‚ŒãŸå ´åˆï¼Œãã“ã§ã¯ç§çš„è²¡ç”£ã®æ‰€æœ‰ã‚’èªã‚ã‚‹ã‹ãƒ»èªã‚ãªã„ã‹ãŒè¨€åŠã•ã‚Œã¦ã„ã‚‹ã®ã ï¼\nNote:\nåŸç†ã¨ã—ã¦ã®ã€Œè³‡æœ¬ä¸»ç¾©/å…±ç”£ä¸»ç¾©ã€2å…ƒè«–ã¯éå¸¸ã«ã‚·ãƒ³ãƒ—ãƒ«ã ãŒï¼Œå®Ÿéš›ã®äººé¡å²ã§ã¯ï¼Œã“ã‚Œã‚‰ã‚’ç¾å®Ÿç¤¾ä¼šã¸åˆ¶åº¦ãƒ»ã‚·ã‚¹ãƒ†ãƒ ã¨ã—ã¦å°å…¥ã—ï¼Œã“ã®åŸç†ã‚’é§†å‹•ã•ã›ã‚‹ãŸã‚ã«ï¼Œæ§˜ã€…ãªæ³•çš„ãƒ»æ”¿æ²»çš„ãªã—ãã¿ãŒææ¡ˆã•ã‚Œã¦ã„ã‚‹ï¼ã—ã‹ã—ï¼Œå†ç¾æ€§ãŒãªã„ç¾è±¡(=å²å®Ÿ)ã«ã¤ã„ã¦ã¯é»™ã‚‹ã®ãŒç§‘å­¦ã®ãƒ«ãƒ¼ãƒ«ãªã®ã§ï¼Œã“ã“ã§ã¯ç«‹ã¡å…¥ã‚‰ãªã„ã“ã¨ã¨ã™ã‚‹ï¼\nè³‡æœ¬ - Capital\nè³‡æœ¬ã¨ã¯ï¼Œå¯Œã‚’ç”Ÿç”£ã™ã‚‹ãŸã‚ã®æ§‹æˆè¦ç´ ã‚’æŒ‡ã™æŠ½è±¡æ¦‚å¿µã§ã‚ã‚‹ï¼\n è³‡æœ¬ï¼šç”Ÿç”£è¦ç´   è³‡æœ¬ã¯ï¼Œåºƒç¾©ã§ã¯ç”Ÿç”£è¦ç´ ã‚’æŒ‡ã™ç”¨èªã§ã‚ã‚Šï¼Œä»¥é™ã§æ‰±ã†ãƒ’ãƒˆãƒ»ãƒ¢ãƒãƒ»ã‚«ãƒã¯å…¨ã¦è³‡æœ¬ã¨ã¿ãªã™ã“ã¨ã‚‚ã§ãã‚‹ï¼ã—ã‹ã—ï¼Œå®Ÿéš›ã«ã¯ï¼Œãã®ç¤¾ä¼šçš„å½¹å‰²ã‚„æ­´å²çš„æ„å‘³ã«æ³¨æ„ã—ã¦ï¼Œã“ã‚Œã‚‰ã‚’åˆ†ã‘ã¦æ‰±ã†ã“ã¨ãŒå¤šã„ï¼ã“ã‚Œã¯çµŒæ¸ˆå­¦ãŒï¼Œè‡ªç„¶ç§‘å­¦ã§ã¯ãªãç¤¾ä¼šç§‘å­¦ãŸã‚‹æ‰€ä»¥ã§ã‚ã‚‹ï¼ã™ãªã‚ã¡ï¼ŒçµŒæ¸ˆå­¦ãŒæ‰±ã†ç¯„ç–‡ã§ã‚ã‚‹ã€Œäººé–“ç¤¾ä¼šã€ã®åˆ¶åº¦ãƒ»ã‚·ã‚¹ãƒ†ãƒ ã«ã¯ï¼Œæ³•ã‚„æ”¿æ²»ãŒè¨­è¨ˆç†å¿µã¨ã—ã¦æ·±ãé–¢ã‚ã£ã¦ãŠã‚Šï¼Œå®Ÿéš›çš„ãƒ»å®Ÿç”¨çš„ãªåˆ†æã‚’è¡Œã†éš›ã«ã¯ï¼Œã“ã‚Œã‚‰ã¯åˆ†é›¢ä¸å¯èƒ½ã§ã‚ã‚‹ã¨ã„ã†ã“ã¨ã§ã‚ã‚‹ï¼\nNote:\nè³‡æœ¬ã«ã‚ˆã£ã¦ï¼Œæˆ‘ã€…ãŒç”Ÿç”£ã—ã¦ã„ã‚‹ã‚‚ã®(=å¯Œ)ã¨ã¯ä½•ã‹ï¼Ÿã¨ã„ã†ç–‘å•ãŒæ®‹ã‚‹ï¼çµè«–ã‹ã‚‰è¨€ã†ã¨ï¼Œåƒ•ã¯ã¾ã ç†è§£ã§ãã¦ã„ãªã„ï¼ˆè©³ã—ã„æ–¹ã«æ•™ãˆã¦æ¬²ã—ã„ï¼‰ï¼ä»¥ä¸‹ã«ç¾æ™‚ç‚¹ã§ã®ä¸€å¿œã®è§£é‡ˆã‚’ã¾ã¨ã‚ã¦ãŠã\n ã‚¢ãƒ€ãƒ ãƒ»ã‚¹ãƒŸã‚¹ã®æ™‚ä»£ã«ï¼Œå½“æ™‚ã®ç†è«–å®¶ã¯ï¼ˆæŠ½è±¡çš„ãªæ¦‚å¿µã¨ã—ã¦ã®ï¼‰ã€Œå¯Œã€ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«æ€è€ƒã‚’é‡ã­ãŸï¼ãã®çµæœï¼Œå›½å®¶ã«ãŠã‘ã‚‹ã€Œå¯Œã€ã‚’èª¬æ˜ã™ã‚‹éš›ã«ï¼Œè¦³æ¸¬å¯èƒ½ãªã‚‚ã®ã®ã†ã¡ï¼Œã‚‚ã£ã¨ã‚‚å¦¥å½“ãªæŒ‡æ¨™ã¯ï¼Œè³‡æœ¬(=ç”Ÿç”£è¦ç´ )ã ã£ãŸï¼ãã®ãŸã‚ã€Œå›½å®¶ã®å¯Œâˆå›½å®¶ã®ç”Ÿç”£åŠ›ã€ã¨ä»®å®šã—ï¼Œãã®å›½ã®è³‡æœ¬ã«æ³¨ç›®ã—ãŸï¼\n ãªãŠï¼Œã€Œè¦³æ¸¬ã§ããªã„äº‹å®Ÿã€ã«ã¤ã„ã¦ã¯å•é¡Œã®ç¯„ç–‡å¤–ã¨ã™ã‚‹ã‹ï¼Œã‚ã‚‹ã„ã¯å¦¥å½“ãªä»®å®šã‚’ãŠã„ã¦æ¢æ±‚ã‚’é€²ã‚ã‚‹ã®ãŒçµŒé¨“ç§‘å­¦ã®ä½œæ³•ãªã®ã§ï¼Œã“ã“ã§ã¯ç«‹ã¡å…¥ã‚‰ãªã„ã“ã¨ã¨ã™ã‚‹ï¼\nå–å¼•ã¨å¸‚å ´ - Trading \u0026amp; markets\næˆ‘ã€…ãŒï¼ŒçµŒæ¸ˆå­¦çš„è¦–ç‚¹ã§ç¤¾ä¼šã‚’ã¿ã‚‹ã¨ãï¼Œãã‚Œã¯ã™ãªã‚ã¡ï¼Œæˆ‘ã€…ãŒã€Œå–å¼•ã€ã«ç€ç›®ã™ã‚‹ã“ã¨ã‚’æ„å‘³ã™ã‚‹ï¼ã€Œå–å¼•ã€ã¨ã¯ã€Œè³‡æœ¬(=ç”Ÿç”£è¦ç´ )ã®æ‰€æœ‰æ¨©ã®äº¤æ›ã€ã§ã‚ã‚Šï¼Œå–å¼•ã®å‚¾å‘ã‚’åˆ†æã™ã‚‹ã“ã¨ã§ï¼ŒçµŒæ¸ˆã®ãƒŸã‚¯ãƒ­ãªçŠ¶æ…‹ã‚’æ¨å®šã™ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼ã•ã‚‰ã«ï¼Œå–å¼•ã®é›†åˆã«å¯¾ã—ã¦ã€Œå¸‚å ´ã€ã¨ã„ã†æ¦‚å¿µã‚’ä¸ãˆï¼Œå–å¼•ã®çµ±è¨ˆçš„å‚¾å‘ã‚’åˆ†æã™ã‚‹ã“ã¨ã§ï¼ŒçµŒæ¸ˆã®ãƒã‚¯ãƒ­ãªçŠ¶æ…‹ã‚’æ¨å®šã™ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼\n2. è³‡æœ¬ä¸»ç¾©ç¤¾ä¼šã«å¯¾ã™ã‚‹å€‹äººã®é©åˆæˆ¦ç•¥ ã“ã“ã‹ã‚‰ã¯ï¼Œè³‡æœ¬ä¸»ç¾©ã®åŸç†ã‚’è¸ã¾ãˆãŸä¸Šã§ï¼Œã‚ã‚Œã‚ã‚Œå€‹äººãŒå–ã‚‹ã¹ãæˆ¦ç•¥ã«ã¤ã„ã¦è€ƒãˆã¦ã¿ã‚ˆã†ã¨æ€ã†ï¼ã“ã“ã§ã„ã†ã€Œæˆ¦ç•¥ã€ã¨ã¯ï¼Œã€Œè¡Œå‹•é¸æŠã€ã¨åŒç¾©ã§ã‚ã‚‹ï¼ãªãŠï¼Œå®Ÿéš›ã®ç¤¾ä¼šã§è€ƒæ…®ã™ã¹ãå€‹åˆ¥å…·ä½“çš„ãªæ¡ä»¶ã¯è€ƒãˆãšï¼Œã‚ãã¾ã§è³‡æœ¬ä¸»ç¾©ã®\u0026quot;åŸç†\u0026quot;ã‚’è¡¨ç¾ã™ã‚‹ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ï¼Œã‚²ãƒ¼ãƒ ã‚’å°å…¥ã—ã¦ã„ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ã»ã—ã„ï¼\nè³‡æœ¬ä¸»ç¾©ã‚²ãƒ¼ãƒ  - The capitalist\u0026rsquo;s game\nå€‹äººã«ã¨ã£ã¦é‡è¦ãªå•é¡Œã¯ï¼ŒãƒŸã‚¯ãƒ­ãªç¤¾ä¼šçŠ¶æ³ã«å¯¾ã—ã¦ï¼Œç§‘å­¦çš„ãªè¦–ç‚¹ã‚’ã‚‚ã¡ï¼Œåˆç†çš„ã«æ„æ€æ±ºå®šãƒ»è¡Œå‹•é¸æŠã‚’è¡Œã†ã“ã¨ã§ã‚ã‚‹ï¼ã“ã“ã§ã¯ï¼Œè³‡æœ¬ä¸»ç¾©ã®åŸç†ã‚’è¸ã¾ãˆãŸä»¥ä¸‹ã®ã‚ˆã†ãªã‚²ãƒ¼ãƒ ã‚’è€ƒãˆã¦ï¼Œå€‹äººã®é©åˆæˆ¦ç•¥ã‚’è€ƒãˆã¦ã¿ã‚‹ï¼\n  ç¤¾ä¼šï¼šã‚²ãƒ¼ãƒ ä¸–ç•Œ\n  å€‹äººï¼šãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼\n è³‡æœ¬ï¼šãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æ‰€æœ‰ã‚¢ã‚¤ãƒ†ãƒ     è³‡æœ¬ä¸»ç¾©çµŒæ¸ˆï¼šã‚²ãƒ¼ãƒ ãƒ«ãƒ¼ãƒ«\n å–å¼•ï¼šè³‡æœ¬ã®æ‰€æœ‰æ¨©ã®äº¤æ› å¸‚å ´ï¼šå–å¼•ã®é›†åˆ ä¾¡å€¤ï¼šå¸‚å ´ã‹ã‚‰çµ±è¨ˆçš„ã«æ¨å®šã•ã‚Œã‚‹å„è³‡æœ¬ã®è©•ä¾¡å€¤    ã“ã®ã‚²ãƒ¼ãƒ ã«ãŠã‘ã‚‹å„ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ç›®æ¨™ã¯ï¼Œã€Œå¸‚å ´ä¾¡å€¤ã€ãŒå¸¸ã«æœ€å¤§ã¨ãªã‚‹ã‚ˆã†ã«æ‰€æœ‰ã™ã‚‹è³‡æœ¬ã‚’é¸æŠã—ç¶šã‘ã‚‹ã“ã¨ã¨ãªã‚‹ï¼ã“ã“ã§ï¼Œã€Œå¸‚å ´ä¾¡å€¤ã€ã¨ã¯ï¼Œã‚²ãƒ¼ãƒ å‚åŠ è€…ã«ã‚ˆã£ã¦è¡Œã‚ã‚Œã‚‹å–å¼•ï¼ˆè³‡æœ¬ã®æ‰€æœ‰æ¨©ã®äº¤æ›ï¼‰ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã‹ã‚‰ï¼Œãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§çµ±è¨ˆçš„ã«æ¨å®šã•ã‚ŒãŸï¼Œå„è³‡æœ¬ã®è©•ä¾¡å€¤ã§ã‚ã‚‹ï¼\nNote:\nè³‡æœ¬ä¾¡å€¤ã®å®šç¾©ã«ã€Œè©•ä¾¡å€¤ã€ã¨ã„ã†è¡¨ç¾ã‚’ä½¿ã£ãŸãŒï¼Œã“ã‚Œã¯ä¸€ä½“ä½•ãªã®ã‹ï¼Ÿå„è³‡æœ¬ã®è©•ä¾¡å€¤ã¯ï¼Œå–å¼•ãŒè¡Œã‚ã‚Œã‚‹å¸‚å ´ã«ã‚ˆã£ã¦ç•°ãªã‚‹ãŒï¼Œãã®ç®—å®šãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã¯ã€Œéœ€è¦ã€ã¨ã€Œä¾›çµ¦ã€ã¨ã„ã†æŠ½è±¡æ¦‚å¿µã§èª¬æ˜ã•ã‚Œã‚‹ï¼ä¾‹ã¨ã—ã¦ï¼Œæ ªå¼å¸‚å ´ã§ã¯ï¼Œä¼æ¥­ã®æ‰€æœ‰æ¨©ï¼ˆæ ªå¼ï¼‰ã«å¯¾ã—ã¦è©•ä¾¡å€¤ï¼ˆæ ªä¾¡ï¼‰ã‚’ä¸ãˆã¦ã„ã‚‹ãŒï¼Œã“ã‚Œã¯éœ€è¦ï¼ˆè²·ã„æ³¨æ–‡ï¼‰ã¨ä¾›çµ¦ï¼ˆå£²ã‚Šæ³¨æ–‡ï¼‰ã«ã‚ˆã£ã¦ç®—å®šã•ã‚Œã‚‹ï¼\nä»£è¡¨çš„ãªè³‡æœ¬: ãƒ’ãƒˆãƒ»ãƒ¢ãƒãƒ»ã‚«ãƒ\nè³‡æœ¬ã®åˆ†é¡ã«ã¤ã„ã¦ã¯ï¼Œæ³•çš„åˆ†é¡ã‚„ä¼šè¨ˆå­¦çš„åˆ†é¡ãªã©ç¨®ã€…ã§ã‚ã‚‹ï¼ã“ã“ã§ã¯ï¼Œè³‡æœ¬ã‚’ãƒ’ãƒˆãƒ»ãƒ¢ãƒãƒ»ã‚«ãƒã«åˆ†ã‘ã¦ï¼Œãã®å…·ä½“ä¾‹ã‚’è€ƒãˆã¦ã¿ã‚‹ï¼\nï¼Šä»£è¡¨çš„ãªè³‡æœ¬ï¼ˆ=ç§çš„è²¡ç”£ï¼‰\n  ãƒ’ãƒˆï¼ˆå€‹äººï¼‰ï¼šæ³•çš„åˆ¶ç´„ã‚’æ ¹æ‹ ã¨ã—ãŸä¾¡å€¤ã‚’æŒã¤ï¼\nâ€‹\tcf) çµŒæ¸ˆå–å¼•ã«ãŠã‘ã‚‹è¡Œå‹•ä¸»ä½“ã€€cf) æ³•äºº\n  ãƒ¢ãƒï¼ˆæ¶ˆè²»è²¡ï¼Œç”Ÿç”£è²¡ï¼‰ï¼šç‰©ç†çš„åˆ¶ç´„ã‚’æ ¹æ‹ ã¨ã—ãŸä¾¡å€¤ã‚’ã‚‚ã¤ï¼\nâ€‹\tcf) ã‚µãƒ¼ãƒ“ã‚¹ã‚‚å«ã‚€ã€€exï¼‰é‡‘ãƒ»çŸ³æ²¹ãƒ»åœŸåœ°ã€€  ã‚«ãƒï¼ˆç¾é‡‘ï¼Œæœ‰ä¾¡è¨¼åˆ¸ï¼‰ï¼šãƒ¢ãƒãƒ»ãƒ’ãƒˆã¨ã®äº¤æ›å¯èƒ½æ€§ã«ã‚ˆã‚Šä¾¡å€¤ã‚’æŒã¤ï¼\n  ï¼Šä»£è¡¨çš„ãªå¸‚å ´ï¼ˆ=ä¾¡å€¤ã‚’æ¸¬ã‚‹è©•ä¾¡é–¢æ•°ï¼‰\n ãƒ’ãƒˆï¼šåŠ´åƒå¸‚å ´/é›‡ç”¨å¸‚å ´ ãƒ¢ãƒï¼šè²¡å¸‚å ´ã€€ ã‚«ãƒï¼šè³‡æœ¬å¸‚å ´/é‡‘èå¸‚å ´  ï¼Šå¤‰æ›è¦å‰‡\n  ãƒ’ãƒˆâ†’ã‚«ãƒï¼ˆåŠ´åƒï¼‰\n  ã‚«ãƒâ†’ãƒ¢ãƒï¼ˆæ¶ˆè²»ï¼‰\n  ãƒ¢ãƒâ†’ãƒ’ãƒˆï¼ˆæ•™è‚²ï¼‰ï¼šã“ã“ã§ã„ã†ãƒ¢ãƒ/ã‚µãƒ¼ãƒ“ã‚¹ã¨ã¯ï¼ŒçŸ¥è­˜ã‚„ã‚¹ã‚­ãƒ«ã‚’æŒ‡ã™\n  ãƒ¢ãƒâ†’ã‚«ãƒï¼ˆå£²å´ï¼‰ï¼šã»ã¨ã‚“ã©ã®ãƒ¢ãƒã¯ï¼Œè©•ä¾¡ã•ã‚Œãªã„ï¼ˆä¸­å¤å“å¸‚å ´ï¼‰\n  â†’é©åˆ‡ãªè¡Œå‹•é¸æŠ\nï¼Šå¸‚å ´ã«æ ¹ã–ã—ãŸä¿æœ‰è³‡æœ¬ã®ãƒã‚¸ã‚·ãƒ§ãƒ‹ãƒ³ã‚°\n  ãƒ’ãƒˆï¼ˆå°±è·ï¼Œè»¢è·ï¼Œå‰¯æ¥­ï¼‰\n  ãƒ¢ãƒï¼ˆã›ã©ã‚Šï¼Œè»Šã‚„å®¶ï¼Œéºç”£ç›¸ç¶šï¼‰\n  ã‚«ãƒï¼ˆãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªè¨­è¨ˆï¼‰\n  â†’é©åˆ‡ãªç¾çŠ¶åˆ†æ\nï¼Šç”Ÿç”£åŠ›(=ä¾¡å€¤)ã¯ï¼ŒçµŒéæ™‚é–“ã«å¯¾ã—ã¦æˆé•·/æ¸›è¡°ã™ã‚‹\n  ãƒ’ãƒˆã®ä¾¡å€¤ï¼šUP/DOWNï¼ˆçŸ¥è­˜ã¨æŠ€è¡“ï¼Œç®¡ç†ï¼Œäººé–“é–¢ä¿‚ã®è³ª/é‡ï¼‰\n  ãƒ¢ãƒã®ä¾¡å€¤ï¼šUP/DOWNï¼ˆéª¨è‘£å“ã‚„å¤é…’ï¼ŒçµŒå¹´åŠ£åŒ–ï¼‰\n  ã‚«ãƒã®ä¾¡å€¤ï¼šUP/DOWNï¼ˆç‰©ä¾¡ã¨é‡‘åˆ©ï¼ŒçµŒæ¸ˆæˆé•·ã¨æ ªä¾¡ï¼‰\n  â†’é©åˆ‡ãªæœªæ¥äºˆæ¸¬\nï¼Šãªãœï¼Œé‡‘èå¸‚å ´ã«ç€ç›®ã™ã‚‹ã¹ãã‹ï¼Ÿ\n ã‚«ãƒã¯ï¼Œæ³•çš„ãƒ»ç‰©ç†çš„åˆ¶ç´„ã®å½±éŸ¿ãŒã‚‚ã£ã¨ã‚‚å°ã•ã„ï¼  ãƒ’ãƒˆã¯æ³•çš„åˆ¶ç´„ã‚’å—ã‘ã‚‹ï¼šEx.)åŠ´åƒæ™‚é–“ã®ä¸Šé™ï¼Œå‹¤å‹™åœ°ã®é¸æŠ ãƒ¢ãƒã¯ç‰©ç†çš„åˆ¶ç´„ã‚’å—ã‘ã‚‹ï¼šEx.)çŸ³æ²¹åŸ‹è”µé‡ã®ä¸Šé™ï¼Œäººå£ä¸Šé™    â†’ã‚«ãƒã®å¸‚å ´ä¾¡å€¤ã¯ï¼Œå®šå¸¸çŠ¶æ…‹ã«åæŸã—ãªã„ãŸã‚ï¼Œã‚‚ã£ã¨ã‚‚å¤‰åŒ–ãŒæ¿€ã—ã„ï¼\n","date":1561075200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561075200,"objectID":"202c2d8d41279ef86362d59a029b53ec","permalink":"https://yumaloop.github.io/post/2020-04-05-adaptation-strategies-for-capitalism/","publishdate":"2019-06-21T00:00:00Z","relpermalink":"/post/2020-04-05-adaptation-strategies-for-capitalism/","section":"post","summary":"Thinking of adaptation strategies for the capitalism","tags":["Tips","System","Capitalism"],"title":"A Capitalist's Game","type":"post"},{"authors":null,"categories":["Random"],"content":"Have you ever want to login to keio.jp automatically? Don\u0026rsquo;t you think it is cool? At least I think so and I write down the way to achieve that with Python.\nYour browser does not support the video tag.\n  In order to login to keio.jp (Keio Single Sign-On System), it is necessary to satisfy the page transition as below.\n  https://auth.keio.jp (SSO login page)\n  https://gslbs.adst.keio.ac.jp/student/index.html (Syllabus page)\n  https://www.edu.keio.jp/ess2/login? (Class support page)\n  So, this time, a static web-scraping library like BeautifulSoup is not enough, because it doesn\u0026rsquo;t support the dynamic site with Javascript or page redirection. Then I use Selenium and ChromeDriver in python.\nExample 1 : Auto login to keio.jp If you are the student in Keio University, you can login to keio.jp automatically. All you need is to assign your email address and password in the below script and run it in command line.\nfrom selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC from selenium.common.exceptions import TimeoutException ID = \u0026quot;*****\u0026quot; # your email in keio.jp (ex. example@keio.jp) # PW = \u0026quot;*****\u0026quot; # your password in keio.jp # # Optional settings of chrome driver options = webdriver.ChromeOptions() options.add_argument('--headless') # Boot chrome driver driver = webdriver.Chrome(\u0026quot;/usr/local/bin/chromedriver\u0026quot;, options=options) driver.set_page_load_timeout(15) # Time out 15 sec # GET (HTML Page) driver.get(\u0026quot;https://auth.keio.jp\u0026quot;) # Find elements and POST (send keys to the input tag) id_element = driver.find_element_by_name(\u0026quot;j_username\u0026quot;) id_element.send_keys(ID) pw_element = driver.find_element_by_name(\u0026quot;j_password\u0026quot;) pw_element.send_keys(PW) # Click login button login_button = driver.find_element_by_name(\u0026quot;_eventId_proceed\u0026quot;) login_button.click() # GET (HTML Page) driver.get(\u0026quot;https://gslbs.adst.keio.ac.jp/student/index.html\u0026quot;) # GET (HTML Page) driver.get(\u0026quot;https://www.edu.keio.jp/ess2/login?\u0026quot;) # Close chrome driver driver.quit()  Example 2 : Auto login to twitter.com If you have twitter account, you can also login to twitter.com automatically. All you need is to assign your username (@********) and password (********) in the below script and run it in command line.\nfrom selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC from selenium.common.exceptions import TimeoutException USERNAME=\u0026quot;*****\u0026quot; # your username in twitter # PASSWORD=\u0026quot;*****\u0026quot; # your password in twitter # # Optional settings of chrome driver options = webdriver.ChromeOptions() options.add_argument('--headless') # Boot chrome driver driver = webdriver.Chrome(\u0026quot;/usr/local/bin/chromedriver\u0026quot;, options=options) driver.set_page_load_timeout(15) # Time out 15 sec # GET (HTML Page) driver.get(\u0026quot;https://twitter.com/login\u0026quot;) # Find elements and POST (send keys to the input tag) username_element = driver.find_element_by_class_name('js-username-field') username_element.send_keys(USERNAME) password_element = driver.find_element_by_class_name('js-password-field') password_element.send_keys(PASSWORD) # Click login button login_button = driver.find_element_by_css_selector('button.submit.EdgeButton.EdgeButton--primary.EdgeButtom--medium') login_button.click() # Close chrome driver driver.quit()  Example 3 : Auto search in google.com If you refer to Selenium Getting started guide, you can aquire the search result with the keyword \u0026ldquo;cheese\u0026rdquo; at google.com with Selenium on Python. (This requires Selenium WebDriver 3.13 or newer.)\nfrom selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support.expected_conditions import presence_of_element_located # Open web driver (Google Chrome) driver = webdriver.Firefox() wait = WebDriverWait(driver, 10) # GET HTML page source of google.com driver.get(\u0026quot;https://google.com/ncr\u0026quot;) # GET # POST the keyword \u0026quot;cheese\u0026quot; in \u0026quot;q\u0026quot; element in google.com driver.find_element_by_name(\u0026quot;q\u0026quot;).send_keys(\u0026quot;cheese\u0026quot; + Keys.RETURN) # POST first_result = wait.until(presence_of_element_located((By.CSS_SELECTOR, \u0026quot;h3\u0026gt;div\u0026quot;))) # Search result as text print(first_result.get_attribute(\u0026quot;textContent\u0026quot;)) # Close web driver (Google Chrome) driver.quit()  References   The Selenium Browser Automation Project \u0026gt; Getting started \u0026gt; Quick tour\nhttps://selenium.dev/documentation/en/getting_started/quick/#webdriver\n  ","date":1561075200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561075200,"objectID":"ca9f10c0f3d78d0df17137a403cb2bff","permalink":"https://yumaloop.github.io/post/2019-06-21-keiojp-auto-login/","publishdate":"2019-06-21T00:00:00Z","relpermalink":"/post/2019-06-21-keiojp-auto-login/","section":"post","summary":"Have you ever want to login to keio.jp automatically? Don\u0026rsquo;t you think it is cool? At least I think so and I write down the way to achieve that with Python.","tags":["Tips","Python"],"title":"Automatically Login to keio.jp using Selenium on Python","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://yumaloop.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://yumaloop.github.io/about/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"Hello!","tags":null,"title":"Landing Page","type":"widget_page"},{"authors":null,"categories":null,"content":"post post post\n","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"f38631c06dc52d51e25323771c266097","permalink":"https://yumaloop.github.io/post/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/post/","section":"","summary":"post post post","tags":null,"title":"Posts","type":"page"},{"authors":null,"categories":["StatML"],"content":"KL-divergence frequently appears in many fields such as statistics and information theory. It is defined as the expected value of logarithmic transformation of likelihood ratio. Note that:\n expected value: weighted integration with probability density. logarithmic transformation: conversion multiplication to linear combination that is suitable for convex optimization and function analysis. likelihood ratio: a measure of likelihood comparison  1. What is KL-divergence? 1.1 Definition ã€€For any probability distributions $P$ and $Q$, KL-divergence (Kullback-Leibler divergence)1 is defined as follows, using their probability density function $p(x)$ and $q(x)$.\n\\begin{align} D_{KL}( Q \\mid\\mid P ) \u0026amp;:= \\int q(x) \\log \\frac{q(x)}{p(x)} ~dx \\end{align}\n1.2 Basic properties ã€€KL-divergence has the following properties.\n ï¼ˆnon-negativeï¼‰It has a non-negative range.  \\begin{align} 0 \\leq D_{KL}( Q \\mid\\mid P ) \u0026amp;\\leq \\infty \\end{align}\n ï¼ˆcompletenessï¼‰When it equals to $0$, $P$ and $Q$ are equivalent.  \\begin{align} D_{KL}( Q \\mid\\mid P ) \u0026amp;= 0 ~~ \\Leftrightarrow ~~ P = Q \\end{align}\n ï¼ˆassymmetryï¼‰It is not symmetric about $P$ and $Q$.  \\begin{align} D_{KL}( Q \\mid\\mid P ) \u0026amp;\\neq D_{KL}( P \\mid\\mid Q ) \\end{align}\n ï¼ˆabsolute continuityï¼‰Unless it diverges, $Q$ is absolutely continuous with respect to $P$.  \\begin{align} D_{KL}( Q \\mid\\mid P ) \u0026amp;\\lt \\infty ~~ \\Rightarrow ~~ P \\gg Q \\end{align}\nâ€‹\tFor example, calculating KL-divergence2 between two Gaussian distributions gives the following results: It can be seen that the more the shapes between two distributions do not match, the more KL-divergence increases.\n1.3 Is KL-divergence a metrics? â€‹\tKL-divergence is so important measurement when considering probability and information that it is called by various names depending on the field and context.\n  \u0026ldquo;KL-divergence\u0026rdquo; \u0026ldquo;KL-metrics\u0026rdquo; \u0026ldquo;KL-information\u0026rdquo; \u0026ldquo;Information divergence\u0026rdquo; \u0026ldquo;Information gain\u0026rdquo; \u0026ldquo;Relative entropy\u0026rdquo;   Since KL-divergence is always non-negative, it might be interpreted as the metrics in the space where the probability distributions $P$ and $Q$ exist. However, KL-divergence is not strictly a metric because it only satisfies \u0026ldquo;non-negativity\u0026rdquo; and \u0026ldquo;completeness\u0026rdquo; among the following axioms of metrics.\n Axioms of metrics $d(~)$:\n  non-negativity $d(x, ~ y) \\geq 0$\n  completeness $d(x, ~ y) = 0 ~~ \\Leftrightarrow ~~ x = y$\n  symmetry $d(x, ~ y) = d(y, ~ x)$\n  The triangle inequality $d(x, ~ y) + d(y, ~ z) \\geq d(x, ~ z)$\n   Note that $d()$ is called the distance function or simply distance\nFor example, Euclidean distance, squared distance, Mahalanobis distance, and Hamming distance satisfy these conditions, and can be clearly considered as metrics. On the other hand, KL-divergence is a divergence, not metrics. In mathematics, \u0026ldquo;divergence\u0026rdquo; is an extended concept of \u0026ldquo;metrics\u0026rdquo; that satisfies only non-negativity and completeness among axioms of metrics. By introducing \u0026ldquo;divergence\u0026rdquo;, you can reduce the constraints of axioms of metrics and have a high level of abstraction.\nThe word \u0026ldquo;divergence\u0026rdquo; is generally interpreted as the process or state of diverging; for example, in physics it appears as a vector operator div. There is no Japanese words that corresponds to the meaning of divergence, but it seems that \u0026ldquo;ç›¸é•åº¦\u0026rdquo;, \u0026ldquo;åˆ†é›¢åº¦\u0026rdquo;, \u0026ldquo;é€¸è„±åº¦\u0026rdquo;, \u0026ldquo;ä¹–é›¢åº¦\u0026rdquo; etc. might be used.\nAs an example, let\u0026rsquo;s measure the KL-divergence between two Gaussian distributions $ N (0, 1) $ (blue) and $ N (1, 2) $ (red). In the figure, the left shows KL-divergence from red one as seen from blue one, and the right shows KL-divergence from blue one as seen from red one. Their value are surely different.\nNote that given two Gaussian distribution $p_1,p_2$ as\n$$ \\begin{align} p_1(x) \u0026amp;= \\mathcal{N}(\\mu_1, \\sigma_1^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma_1^2}} \\exp \\left\\{ - \\frac{ {(x - \\mu_1)}^2}{2 \\sigma_1^2} \\right\\} \\\\\np_2(x) \u0026amp;= \\mathcal{N}(\\mu_2, \\sigma_2^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma_2^2}} \\exp \\left\\{ - \\frac{ {(x - \\mu_2)}^2}{2 \\sigma_2^2} \\right\\} \\end{align} $$\nthe following holds.\n$$ \\begin{align} {D}_{KL}(p_1 \\mid\\mid p_2) \u0026amp;= \\int_{-\\infty}^{\\infty} p_1(x) \\log \\frac{p_1(x)}{p_2(x)} dx \\\\\n\u0026amp;= \\log \\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + {( \\mu_1 - \\mu_2 )}^2}{2 \\sigma_2^2} - \\frac{1}{2} \\end{align} $$\nIncidentally, in addition to the KL-divergence, the following is known as a measure of the proximity (or closeness) between two probability distributions.\n The metrics to measure closeness between $q(x)$ and $p(x)$\n $ {\\chi}^2(q ; p) := \\sum_{i=1}^{k} \\frac{ { { p_i - q_i } }^{2} }{p_i}$ ($\\chi^2$-statistics) $ L_1(q ; p) := \\int \\vert q(x) - p(x) \\vert ~ dx$ ($L_1$-norm) $ L_2(q ; p) := \\int { { q(x) - p(x) } }^{2} ~ dx$ ($L_2$-norm) $ I_K(q ; p) := \\int { \\{ \\sqrt{ q(x) } - \\sqrt{ p(x) } \\} }^{2} ~ dx $ (Herringer distance) $ \\mathbb{D}(q ; p) := \\int f \\left( {\\large \\frac{q(x)}{p(x)} } \\right) q(x) ~ dx$ ($f$-divergence) $ I_{\\lambda}(q ; p) := \\int \\left\\{ { \\left( {\\large \\frac{q(x)}{p(x)} } \\right) }^{\\lambda} - 1 \\right\\} q(x) ~ dx$ (Generalized information) $ {D}_{KL}(q ; p) := \\int \\log \\left( {\\large \\frac{q(x)}{p(x)} } \\right) q(x) ~ dx$ (KL-divergence) $ JSD(q \\mid\\mid p) := \\frac{1}{2} {D}_{KL}(q \\mid\\mid \\frac{q+p}{2}) + \\frac{1}{2} {D}_{KL}(p \\mid\\mid \\frac{q+p}{2})$ (JS-divergence)   2. Relatinoship to other measurements 2.1 KL-divergence vs Mutual information ã€€In information theory, entropy $H(X)$, join entropy $H(X,Y)$, conditional entropy $H(X \\vert Y)$, mutual information $MI(X,Y)$ are defined as follows by using probability density $Pr()$3.\n\\begin{align} H(X) \u0026amp;:= - \\int Pr(x) \\log Pr(x) ~dx \\\\\nH(X,Y) \u0026amp;:= - \\int Pr(x,y) \\log Pr(x,y) ~dy~dx \\\\\nH(X|Y) \u0026amp;:= - \\int Pr(x,y) \\log Pr(x|y) ~dx~dy \\\\\nMI(X,Y) \u0026amp;:= \\int \\int Pr(x,y) \\log \\frac{Pr(x,y)}{Pr(x)Pr(y)} ~dxdy \\end{align}\nFor any two random variable $X$ and $Y$, mutual information $MI(X, Y)$ specifies the mutual (symmetric) dependence between them.\n\\begin{align} MI(X,Y) \u0026amp;= H(X) - H(X|Y) \\\\\n\u0026amp;= H(Y) - H(Y|X) \\\\\n\u0026amp;= H(X) + H(Y) - H(X,Y) \\end{align}\nHere, the following relationship holds between KL-divergence and mutual information.\n\\begin{align} MI(X, Y) \u0026amp;= D_{KL} \\bigl( Pr(x, y) \\mid\\mid Pr(x)Pr(y) \\bigr) \\\\\n\u0026amp;= \\mathbb{E}_{Y} \\bigl[ D_{KL} \\bigl( Pr(x|y) \\mid\\mid Pr(x) \\bigr) \\bigr] \\\\\n\u0026amp;= \\mathbb{E}_{X} \\bigl[ D_{KL} \\bigl( Pr(y|x) \\mid\\mid Pr(y) \\bigr) \\bigr] \\end{align}\nSo that, mutual information $MI (X, Y)$ is interpreted as the degree of difference (average degree of deviation) between the joint distribution $Pr (x, y)$ when the $X$ and $Y$ are not independent and the joint distribution $Pr (x) Pr (y)$ when $X$ and $Y$ are independent.\nï¼ˆcf.ï¼‰Formula transformation of mutual information:\n\\begin{align} MI(X,Y) \u0026amp;= \\int \\int Pr(x,y) \\log \\frac{Pr(x,y)}{Pr(x)Pr(y)} ~dxdy \\\\\n\u0026amp;= \\int \\int Pr(x|y)Pr(y) \\log \\frac{Pr(x|y)Pr(y)}{Pr(x)Pr(y)} ~dxdy \\\\\n\u0026amp;= \\int Pr(y) \\int Pr(x|y) \\log \\frac{Pr(x|y)}{Pr(x)} ~dx~dy \\\\\n\u0026amp;= \\int Pr(y) \\cdot D_{KL} \\bigl( Pr(x|y) \\mid\\mid Pr(x) \\bigr) ~dy \\\\\n\u0026amp;= \\mathbb{E}_{Y} \\bigl[ D_{KL} \\bigl( Pr(x|y) \\mid\\mid Pr(x) \\bigr) \\bigr] \\end{align}\n2.2 KL-divergence vs Log likelihood ratio In the field of Bayes inference and statistical modeling, you often face the problem of estimating the true distribution $q(x)$ by $p_{\\hat{\\theta}}(x)$ (that is the combination of stochastic model $p_{\\theta}(x)$ and estimated parameter $\\hat{\\theta}$ ) . Therefore, KL-divergence is used when you want to measure the difference between two distributions, or when you want to incorporate the estimation error into the loss function or risk function in order to solve the optimization problem for the parameter $\\theta$.\nAlso, KL-divergence is related to the log likelihood ratio so much that it has a deep connection to the model selection method 4 such as likelihood ratio test, Bayes factor, and AIC (Akaike\u0026rsquo;s information criterion).\n KL-divergence of estimated distribution $p_{\\theta}(x)$ for the true distribution $q(x)$ : $D_{KL}(q \\mid\\mid p_{\\theta})$ is considerd as the expected value of the log likelihood ratio $q(x)/p_{\\theta}(x)$ for tue true distribution $q(x)$.  \\begin{align} \\left( \\text{Log likelihood ratio} \\right) \u0026amp;= \\log \\frac{q(x)}{p_{\\theta}(x)} \\\\\nD_{KL}( q \\mid\\mid p_{\\theta} ) \u0026amp;:= \\int q(x) \\log \\frac{q(x)}{p_{\\theta}(x)} ~dx \\\\\n\u0026amp;= \\mathbb{E}_{X} \\left[ \\log \\frac{q(x)}{p_{\\theta}(x)} \\right] \\left(\\text{Expected log likelihood ratio} \\right) \\end{align}\nWhen using KL-divergence as the evaluation/loss value in model selection/comparison, it is equivalent that minimizing KL-divergence: $D_{KL}( q \\mid\\mid p )$ and maximizing the log likelihood: $\\log p(x)$ as follows.\n\\begin{align} D_{KL}( q \\mid\\mid p_{\\theta} ) \u0026amp;= \\mathbb{E}_{X} \\bigl[ \\log q(x) \\bigr] - \\mathbb{E}_{X} \\bigl[ \\log p_{\\theta}(x) \\bigr] \\\\\n\u0026amp;\\propto - \\mathbb{E}_{X} \\bigl[ \\log p_{\\theta}(x) \\bigr] \\left(-1 \\cdot \\text{ Expected log likelihood} \\right) \\end{align}\n  For any parametric stochastic model $f(x \\vert \\theta)$ (such as a linear regression model) which represents the estimated distribution as\n\\begin{align} p_{\\theta}(x) = f(x|\\theta) \\end{align}\n, if a certain loss function $L(\\theta)$ is given, the optimal parameter $\\theta^*$ exists as it satisfy the following.\n\\begin{align} q(x) \u0026amp;= f(x|\\theta^*) \\end{align}\nThen, for any estimated parameter $\\hat{\\theta}$ ,the estimated loss of the model $f(x \\vert \\hat{\\theta})$ is represented by KL-divergence. (Note that $\\ell( \\cdot \\vert x)$ means the log likelihood function.)\n  \\begin{align} \\left( \\text{Log likelihood ratio} \\right) \u0026amp;= \\log \\frac{f(x|\\theta^{*})}{f(x|\\hat{\\theta})} \\end{align}\n\\begin{align} \\hat{\\theta} \u0026amp;:= \\underset{\\theta \\in \\Theta}{\\rm argmin} ~ L(\\theta) \\tag{7} \\\\\nD_{KL}( q \\mid\\mid p_{\\hat{\\theta}} ) \u0026amp;= D_{KL}( p_{\\theta^{*}} \\mid\\mid p_{\\hat{\\theta}} ) \\\\\n\u0026amp;= D_{KL}( f_{\\theta^{*}} \\mid\\mid f_{\\hat{\\theta}} ) \\\\\n\u0026amp;= \\int f(x|\\theta^{*}) \\log \\frac{f(x|\\theta^{*})}{ f(x|\\hat{\\theta})} dx \\\\\n\u0026amp;= \\mathbb{E}_{X} \\left[ \\log \\frac{ f(x|\\theta^{*}) }{ f(x|\\hat{\\theta}) } \\right] \\\\\n\u0026amp;= \\mathbb{E}_{X} \\bigl[ \\ell( \\theta_{0}|x ) \\bigr] - \\mathbb{E}_{X} \\bigl[ \\ell( \\hat{\\theta} | x ) \\bigr] \\end{align}\n2.3 KL-divergence vs Fisher information Given a certain stochastic model $f(\\cdot \\vert \\theta)$, Fisher information $I(\\theta)$ for the parameter $\\theta$ is defined as follows. (Note that $ \\ell( \\cdot \\vert x) $ means the log likelihood function.)\n\\begin{align} I(\\theta) \u0026amp;:= \\mathbb{E}_{X} \\left[ { \\left\\{ \\frac{d}{dx} \\ell(\\theta \\vert x) \\right\\} }^{3} \\right] \\\\\n\u0026amp;= \\mathbb{E}_{X} \\left[ { \\left\\{ \\frac{d}{dx} \\log f(x|\\theta) \\right\\} }^{2} \\right] \\end{align}\nAlso, between KL-divergence and Fisher information, the following holds.\n\\begin{align} \\lim_{h \\to 0} \\frac{1}{h^{2}} D_{KL} \\bigl( f(x|\\theta) \\mid\\mid f(x|\\theta+h) \\bigr) \u0026amp;= \\frac{1}{2} I(\\theta)\n\\end{align}\n(cf.) The following equation holds by using Taylor expansion of $\\ell( \\cdot \\vert x)$.\n\\begin{align} \\ell(\\theta + h) - \\ell(\\theta) \u0026amp;= {\\ell}^{'}(\\theta)h + \\frac{1}{2} {\\ell}^{''}(\\theta) h^{2} + O(h^{3}) \\end{align}\nThis formula indicates that in parameter space $\\Theta$, for all point $ \\theta \\in \\Theta $ ant its neighborring point $ \\theta + h $, their KL-divergenceï¼š$ D_{KL} ( f(x \\vert \\theta) \\mid\\mid f(x \\vert \\theta+h) )$ is **directly proportional to** Fisher information $I(\\theta)$. After all, Fisher information $ I(\\theta)$ measures **the local information** that the stochastic model $f(\\cdot \\vert \\theta)$ has at the point $\\theta$.\n3. References    \n  Also, f-divergence is defined as its generalized class. \u0026#x21a9;\u0026#xfe0e;\n I used scipy.stats.entropy(). \u0026#x21a9;\u0026#xfe0e;\n Although thermodynamic entropy is originated in Boltzmann, the historical background of Shannon information is mentioned below link. There seems to be a reference flow: Hartley â†’ Nyquist â†’ Shannon. http://www.ieice.org/jpn/books/kaishikiji/200112/200112-9.html \u0026#x21a9;\u0026#xfe0e;\n Article on gneralized information criterion(GIC): https://www.ism.ac.jp/editsec/toukei/pdf/47-2-375.pdf \u0026#x21a9;\u0026#xfe0e;\n   ","date":1524096000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524096000,"objectID":"27a281007a11aade3bc5bd85329a41fd","permalink":"https://yumaloop.github.io/post/2018-04-19-kl-divergence/","publishdate":"2018-04-19T00:00:00Z","relpermalink":"/post/2018-04-19-kl-divergence/","section":"post","summary":"This is a basic notebook for KL-divergence which frequently appears in many fields such as statistics and information theory.","tags":["Bayes"],"title":"Kullback-Leibler Divergence","type":"post"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://yumaloop.github.io/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":["Nelson Bighetti","å³æ©é”"],"categories":["Demo","æ•™ç¨‹"],"content":"Overview  The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It\u0026rsquo;s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more    The template is mobile first with a responsive design to ensure that your site looks stunning on every device.   Get Started  ğŸ‘‰ Create a new site ğŸ“š Personalize your site ğŸ’¬ Chat with the Wowchemy community or Hugo community ğŸ¦ Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy ğŸ’¡ Request a feature or report a bug for Wowchemy â¬†ï¸ Updating Wowchemy? View the Update Guide and Release Notes  Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\nâ¤ï¸ Click here to become a sponsor and help support Wowchemy\u0026rsquo;s future â¤ï¸ As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features ğŸ¦„âœ¨\nEcosystem  Wowchemy Admin: An admin tool to automatically import publications from BibTeX  Inspiration Check out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures  Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, ä¸­æ–‡, and PortuguÃªs Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://yumaloop.github.io/post/getting-started/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome ğŸ‘‹ We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","å¼€æº"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":["Nelson Bighetti","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"ff6a19061a984819d30c916886db56ef","permalink":"https://yumaloop.github.io/publication/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/example/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"An example conference paper","type":"publication"}]