[{"authors":null,"categories":null,"content":"Iâ€™m currently delivering data science at the consulting firm. My current interests are the algorithmic marketing and cognitive problems with human thinking.\nğŸ‘» ğŸ­ ğŸ¦„ ğŸŒˆ â›… âœ¨\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Iâ€™m currently delivering data science at the consulting firm. My current interests are the algorithmic marketing and cognitive problems with human thinking.\nğŸ‘» ğŸ­ ğŸ¦„ ğŸŒˆ â›… âœ¨","tags":null,"title":"Yuma Uchiumi","type":"authors"},{"authors":null,"categories":null,"content":"Hi, Iâ€™m Yuma Uchiumi (@yumaloop), a graduate student majoring in computer science. I was born on December 8, 1997 in Tokyo, Japan. My research goal is to understand and implement the computation for human thinking; I study statistical algorithms, stochastic models, and information processing systems to solve the cognitive problems with human thinking.\nDownload my resumÃ©.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"21026022778da3d752ad64b58b6791bd","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hi, Iâ€™m Yuma Uchiumi (@yumaloop), a graduate student majoring in computer science. I was born on December 8, 1997 in Tokyo, Japan. My research goal is to understand and implement the computation for human thinking; I study statistical algorithms, stochastic models, and information processing systems to solve the cognitive problems with human thinking.","tags":null,"title":"Yuma Uchiumi","type":"authors"},{"authors":null,"categories":null,"content":" Which does this look like a duck or a rabbit? Overview In order to elucidate human perceptual functions, it is necessary to consider both bottom-up information processing, in which stimulus information received from the sensory organs is encoded into symbolic information, and top-down information processing, which is objective-oriented and based on memory, beliefs, and context. In this paper, we take the ResNet50 image classification problem as an example task, and conduct a basic study on the information processing when humans make judgments about visual information with ambiguities, and discuss the computation by which working memory during task execution penetrates the discrimination results of the model in a top-down manner.\nYuma Uchiumi, Yosuke Fukuchi, Mitsuhiko Kimoto, Michita Imai, â€œA Top-down Penetration to the Visual Attention via Elimination of the Ambiguityâ€, The 35th Annual Conference of the Japanese Society for Artificial Intelligence (JSAI 2021), Jun. 8-11, 2021, Virtual Conference. Measurement of the Ambiguity When a single stochastic model $f:X \\to Y$ get a data sample $\\boldsymbol{x}^{*}$ of the input random variable $X$, the ambiguity of the model\u0026rsquo;s inference result can be defined as the conditional entropy of the output random variable $Y$.\n$$ \\begin{align} A(Y; f, \\boldsymbol{x}^{*}) := H^{(f)}(Y | X=\\boldsymbol{x}^{*} ) = -\\sum_{\\boldsymbol{y}} f(\\boldsymbol{y} | \\boldsymbol{x}^{*}) \\log f(\\boldsymbol{y} | \\boldsymbol{x}^{*}) \\end{align} $$\nNext, if the model has the prior belief for the output variable $Y$, the surprise of the model\u0026rsquo;s inference result can be defined as the Kullbackâ€“Leibler divergence between the prior distribution $p(\\boldsymbol{y})$ and the predicted distribution $f(\\boldsymbol{y} | \\boldsymbol{x}^{*})$.\n$$ \\begin{align} S(Y; f, \\boldsymbol{x}^{*}, p) := D_{KL}( p || f ) = -\\sum_{\\boldsymbol{y}} p(\\boldsymbol{y}) \\log \\frac{ p(\\boldsymbol{y}) }{ f(\\boldsymbol{y} | \\boldsymbol{x}^{*}) } \\end{align} $$\nSo that, the optimization process of $f$ is obtained as\n$$ \\underset{f}{\\rm minimize} ~~ A(Y; f, \\boldsymbol{x}^{*}) + \\beta \\cdot S(Y; f, \\boldsymbol{x}^{*}, p) $$\nwhere $\\beta$ is a weighting factor in $[0,1]$. The overall computation is shown below.\nTraining the Model \u0026amp; Deriving the Ambiguity First of all, we trained the ResNet-50 as the image classification model $f:$\n$$ X \\in \\mathbb{R}^{H \\times W \\times C} \\to Y \\in \\{\\text{duck}, \\text{rabbit}, \\text{others}\\} $$\non the Google Open Image Dataset. Then, when the model makes inferences sequentially for a given image the ambiguity and the surprise in the model is calculated. The overall computation is shown below.\nIn the experiment, initially 30 images were sampled from the prior categorical distribution of the class label $Y$ and then its posterior distribution $p(y)$ (belief) was formed. Afterwards, the model made inferences for the Duck-Rabbit illusion image $\\boldsymbol{x}^{*}$.\nPlasticity of the Visual Attention Futhermore, we applied the elimination process of the ambiguity to the model. Target parameters of the convolutional kernels conv1 in ResNet-50 was updated according to the defined metrics of the ambiguity and surprise.\nThe attention maps that the model had for the input image during inference are shown as the following figures. As the belief distribution makes a difference between them, the features that strongly affect the inference result are different in the duck-rabbit illusion image.\nAs the result, the proposed computation explains the phenomenon in which the inference process of the model itself is penetrated by higher cognitive elements such as the belief in a top-down manner.\n","date":1614384000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614384000,"objectID":"546cfabd128993555f9c3f766367fc70","permalink":"https://yumauchiumi.com/project/duck-rabbit-illusion/","publishdate":"2021-02-27T00:00:00Z","relpermalink":"/project/duck-rabbit-illusion/","section":"project","summary":"A Top-down Penetration to the Visual Attention via Elimination of the Ambiguity","tags":["Cognitive Science"],"title":"Duck-Rabbit Illusion","type":"project"},{"authors":null,"categories":["Dev"],"content":"StackOverflow published its Developer Survey 2020. Click here for details.\nIn 2020, Python, Go, TypeScript, Rust are the programming language most loved by developers in the world. It means that these languages are in vogue and new learners are on the rise. While JavaScript swallows everything, Perl, PHP, and Ruby seem to be still in high demand.\nFrameworks packaging frontends, backends and database into one, such as Rails, are already not preferred. Today, dividing frontends with JavaScript (Node.js, Vue.js, React.js) and backends/database with scalable cloud services (AWS, GCP) is major approach for adaptive web applications.\nPython and Java have always been very popular and in demand as general languages that can be used for many purposes. In the future, it will be interesting to see if Go and Rust can take their place.\nFinally, I would like to introduce the cluster map of major development technologies found by the StackOverflow Developer Survey 2020. With the penetration of new technologies such as mobile, container and cloud computing, it will be more difficult to become a full-stack engineer in the near future.\nAssembly, C, C++ Raspberry Pi, Arduino Unity, UnrealEngine Hadoop, Scala, Apache Spark Python, Pandas, Torch/PyTorch Linux, Docker, Kubernetes, Bash/Shell AWS, Redis, Ansible, DynamoDB, PostgreSQL JavaScript, Node.js, React.js, Angular, TypeScript, MongoDB, PHP, MySQL, jQuery, WordPress Java, Swift, Android, iOS, Kotlin, SQLite, Firebase C#, .NET, Windows, Azure ","date":1608422400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608422400,"objectID":"0e7eb9f6d1f17ebdc4c693cc3bea4796","permalink":"https://yumauchiumi.com/post/2020-12-20-stackoverflow-developer-survey-2020/","publishdate":"2020-12-20T00:00:00Z","relpermalink":"/post/2020-12-20-stackoverflow-developer-survey-2020/","section":"post","summary":"StackOverflow published its Developer Survey 2020. Click here for details.\nIn 2020, Python, Go, TypeScript, Rust are the programming language most loved by developers in the world. It means that these languages are in vogue and new learners are on the rise.","tags":["Tips","Programming","Web","Python","Go","Java","React"],"title":"StackOverflow - 2020 Developer Survey","type":"post"},{"authors":null,"categories":["Dev"],"content":"TL;DR Put the following magic command (cell magic) in a cell and execute it.\n%%javascript IPython.OutputArea.auto_scroll_threshold = 9999; Note: This is not line magic like % matplotlib inline and some errors will occur unless the executed cell is independent. See IPython Official Documentation for details.\n% - All available line magic commands - check here %% - All available cell magic commands - check here Cf. Magic functions - IPythonå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ comment:53708976 issue:2172 ipython github.com ipythonãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®å‡ºåŠ›ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ã‚µã‚¤ã‚ºã‚’å¤‰æ›´ã™ã‚‹ Jupyter notebookã§åˆ—ã‚’ã™ã¹ã¦è¡¨ç¤ºã—ãŸã„ - Qitta ","date":1606608000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606608000,"objectID":"651530262a23570a9ef45979d6c62fb2","permalink":"https://yumauchiumi.com/post/2020-11-29-maximize-jupyter-notebook-cell-height/","publishdate":"2020-11-29T00:00:00Z","relpermalink":"/post/2020-11-29-maximize-jupyter-notebook-cell-height/","section":"post","summary":"TL;DR Put the following magic command (cell magic) in a cell and execute it. %%javascript IPython.OutputArea.auto_scroll_threshold = 9999; Note: This is not line magic like % matplotlib inline and some","tags":["Python","Jupyter"],"title":"Maximize the cell size of Jupyter notebook","type":"post"},{"authors":null,"categories":["Finance"],"content":"å¹¾ä½•ãƒ–ãƒ©ã‚¦ãƒ³é‹å‹•ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‘ã‚¹ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šï¼Œæ ªä¾¡ãŒå¹¾ä½•ãƒ–ãƒ©ã‚¦ãƒ³é‹å‹•ã«ã—ãŸãŒã†å ´åˆã®ã‚³ãƒ¼ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä¾¡æ ¼ã‚’ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚Šå°å‡ºã—ï¼Œã‚µãƒ³ãƒ—ãƒ«ãƒ‘ã‚¹ã®ç”Ÿæˆå›æ•°ã‚’å¢—ã‚„ã™ã“ã¨ã§ç†è«–å€¤ã«åæŸã™ã‚‹ã“ã¨ã‚’ãƒ—ãƒ­ãƒƒãƒˆã«ã‚ˆã‚Šç¢ºèªã™ã‚‹ï¼\n1. ã‚³ãƒ¼ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ä¾¡æ ¼ç†è«– ãƒ–ãƒ©ãƒƒã‚¯ã‚·ãƒ§ãƒ¼ãƒ«ã‚ºå¼ã«ã‚ˆã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä¾¡æ ¼ã®å°å‡º æº€æœŸ$T$ã§åŸè³‡ç”£ä¾¡æ ¼(æ ªå¼ä¾¡æ ¼)ãŒé€£ç¶šæ™‚é–“ç¢ºç‡éç¨‹$S = {(S_t)}_{t \\in [0,T]}$ã«å¾“ã†ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®æ™‚åˆ»$t \\in [0, T]$ã«ãŠã‘ã‚‹ä¾¡æ ¼$C(t, S_t)$ã«ã¤ã„ã¦è€ƒãˆã‚‹ï¼$S$ãŒç¢ºç‡å¾®åˆ†æ–¹ç¨‹å¼:\n$$ \\begin{align} d S_t = \\sigma S_t dt + \\mu S_t d W_t \\end{align} $$\nã®è§£ã§ä¸ãˆã‚‰ã‚Œã¦ã„ã‚‹ã¨ã™ã‚‹($S$ã¯å¹¾ä½•ãƒ–ãƒ©ã‚¦ãƒ³é‹å‹•ã«å¾“ã†)ï¼æ™‚åˆ»$t \\in [0, T]$ã«ãŠã„ã¦ï¼Œã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®åŸè³‡ç”£ä¾¡æ ¼$S_t$ã¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®è¡Œä½¿ä¾¡æ ¼$K$ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãï¼Œãƒ–ãƒ©ãƒƒã‚¯ãƒ»ã‚·ãƒ§ãƒ¼ãƒ«ã‚ºå¼ã«ã‚ˆã£ã¦ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ç†è«–ä¾¡æ ¼$C(t, S_t)$ã¯\n$$ \\begin{align} C(t, S_t) \u0026amp;= S_t \\Phi(d_1) - K e^{-r(T-t)} \\Phi(d_2) \\\\ where ~~ d_1 \u0026amp;= \\frac{\\log \\left( \\frac{S_t}{K} \\right) + \\left( r + \\frac{\\sigma^2}{2} \\right) T}{\\sigma \\sqrt{T}} \\\\ d_2 \u0026amp;= \\frac{\\log \\left( \\frac{S_t}{K} \\right) + \\left( r - \\frac{\\sigma^2}{2} \\right) T}{\\sigma \\sqrt{T}} \\end{align} $$\nã¨ãªã‚‹ï¼ãŸã ã—ï¼Œ$r$ã¯ç„¡ãƒªã‚¹ã‚¯è³‡ç”£ã®åˆ©ç‡ï¼Œ$\\Phi$ã¯æ¨™æº–æ­£è¦åˆ†å¸ƒ$\\mathcal{N}(0,1)ã®ç´¯ç©åˆ†å¸ƒé–¢æ•°ã¨ã™ã‚‹ï¼ã“ã“ã§ï¼Œç°¡å˜ã®ãŸã‚ã«æº€æœŸ$T$ã‚’$1$ã¨ã™ã‚‹ã¨ï¼Œç¾åœ¨($t=0$)ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä¾¡æ ¼ã®ç†è«–å€¤ã¯ï¼Œ\n$$ \\begin{align} C(0, S_0) \u0026amp;= S_0 \\Phi(d_1) - K e^{-r} \\Phi(d_2) \\\\ where ~~ d_1 \u0026amp;= \\frac{\\log \\left( \\frac{S_0}{K} \\right) + \\left( r + \\frac{\\sigma^2}{2} \\right)}{\\sigma } \\\\ d_2 \u0026amp;= \\frac{\\log \\left( \\frac{S_0}{K} \\right) + \\left( r - \\frac{\\sigma^2}{2} \\right)}{\\sigma} \\end{align} $$\nã¨ãªã‚‹ï¼\nãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã«ã‚ˆã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä¾¡æ ¼ã®å°å‡º ãƒ–ãƒ©ãƒƒã‚¯ã‚·ãƒ§ãƒ¼ãƒ«ã‚ºå¼ã«å«ã¾ã‚Œã‚‹$S_T$ã®æœŸå¾…å€¤è¨ˆç®—ã‚’ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã«ã‚ˆã£ã¦è¿‘ä¼¼ã™ã‚‹ã“ã¨ã‚’è€ƒãˆã‚‹ï¼ã™ãªã‚ã¡ï¼Œå¹¾ä½•ãƒ–ãƒ©ã‚¦ãƒ³é‹å‹•ã«å¾“ã†ã‚µãƒ³ãƒ—ãƒ«ãƒ‘ã‚¹$S = {(S_t)}_{t \\in [0,T]}$ã‚’å¤§é‡ã«ç”Ÿæˆã™ã‚‹ã“ã¨ã§ï¼Œ$S_T$ã®æœŸå¾…å€¤ã‚’æ±‚ã‚ã‚‹ï¼\nãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚ŒãŸï¼Œæº€æœŸ$T$ã«ãŠã‘ã‚‹åŸè³‡ç”£ä¾¡æ ¼$S_T$ã®$n$å€‹ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’$(s^{(1)}{T}, \\cdots, s^{(n)}{T})$ã¨ã™ã‚‹ã¨ï¼Œæ™‚åˆ»$t \\in [0, T]$ã«ãŠã‘ã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ä¾¡æ ¼ã®æ¨å®šå€¤$\\hat{C}(t, S_t)$ã¯\n$$ \\begin{align} \\hat{C}(t, S_t) \u0026amp;= \\frac{1}{n} \\sum_{i=1}^{n} e^{-r(T-t)} \\cdot max(s^{(i)}_{T} - K, 0) \\end{align} $$\nã¨æ±‚ã‚ã‚‰ã‚Œã‚‹ï¼ã“ã“ã§ï¼Œç°¡å˜ã®ãŸã‚ã«æº€æœŸ$T$ã‚’$1$ã¨ã™ã‚‹ã¨ï¼Œç¾åœ¨($t=0$)ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä¾¡æ ¼ã®æ¨å®šå€¤ã¯ï¼Œ\n$$ \\begin{align} \\hat{C}(0, S_0) \u0026amp;= \\frac{1}{n} \\sum_{i=1}^{n} e^{-r} \\cdot max(s^{(i)}_{1} - K, 0) \\end{align} $$\nã¨ãªã‚‹ï¼\n2. Rã§ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ YUIMAãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆor èª­ã¿è¾¼ã¿ï¼‰ã™ã‚‹ï¼\ninstall.packages(\u0026quot;yuima\u0026quot;) # ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« (åˆå›ã®ã¿) library(yuima) # ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿ ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã™ã‚‹ã‚³ãƒ¼ãƒ‰ï¼\n# Calculation of call-option prices by Black-Sholes eq. BlackScholesCallPrice = function(S, K, r, sigma, T=1) { d1 \u0026lt;- ( log(S/K) + (r + sigma^2/2) * T)/( sigma * sqrt(T)) d2 \u0026lt;- ( log(S/K) + (r - sigma^2/2) * T)/( sigma * sqrt(T)) C0 \u0026lt;- S * pnorm(d1) - K * exp(-r * T) * pnorm(d2) return(C0) } # Calculation of call-option prices by Monte Carlo method MonteCarloCallPrice = function(S, K, r, sigma, n, T=1) { n_sample \u0026lt;- 1000 c0_list \u0026lt;- list() c0 \u0026lt;- 0 for (i in 1:n) { resultGBM \u0026lt;- GBM_sample(S, r, sigma, n_sample) sT \u0026lt;- resultGBM@data@original.data[n_sample] c0 \u0026lt;- (1/i) * exp(-1*r*T) * max(sT - K, 0) + ((i-1)/i) * c0 c0_list \u0026lt;- append(c0_list, list(c0)) } return(c0_list) } # A function which generates sample paths that follows a Geometric Brownian motion GBM_sample = function(x0, alpha, beta, n_sample, T=1) { # Step1: Define SDE # dS_t = alpha * S_t * dt + beta * S_t * dW_t mod \u0026lt;-setModel(drift=\u0026quot;alpha*x\u0026quot;, diffusion=\u0026quot;beta*x\u0026quot;) # Step2: Define samples samp \u0026lt;-setSampling(Initial=0, Terminal=T, n=n_sample) # Step3: define the statistical model smod \u0026lt;-setYuima(model=mod, sampling=samp) # Step4: Generate sample paths xinit \u0026lt;- x0 param \u0026lt;- list(alpha=alpha, beta=beta) resultGBM \u0026lt;- simulate(smod, xinit=xinit, true.parameter=param) return(resultGBM) } # Params for call-option pricing n \u0026lt;- 10000 # Num. of MonteCalro simulation K \u0026lt;- 900 # Option exercise price (at t=T) S \u0026lt;- 1000 # Curent asset price (at t=0) r \u0026lt;- 0.005 # Drift for Geometric Brownian motion sigma \u0026lt;- 0.3 # Diffusion for Geometric Brownian motion T \u0026lt;- 1 # Optional term # Main procedure: Run simulation bs_price \u0026lt;- BlackScholesCallPrice(S, K, r, sigma, T=1) mc_price \u0026lt;- MonteCarloCallPrice(S, K, r, sigma, n, T=1) print(bs_price) plot(1:n, mc_price, main=\u0026quot;Monte Carlo Simulation:\\nBlack-Scholes Option Pricing Model\u0026quot;, xlab=\u0026quot;Number of sample paths: # of ST\u0026quot;, ylab=\u0026quot;Option Price: C0\u0026quot;, cex=0.5) abline(h=bs_price, col='red', lwd=1, lty=2) Case 1.\nãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã«ã‚ˆã‚‹ã‚³ãƒ¼ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®æ¨å®šä¾¡æ ¼ãŒï¼Œ BSå¼ã«ã‚ˆã‚‹ç†è«–ä¾¡æ ¼172.7457ã«æ¼¸è¿‘ã—ã¦ã„ã‚‹ï¼\nãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š n: 10000 # ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®å›æ•° K: 900 # ã‚³ãƒ¼ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®æ¨©åˆ©è¡Œä½¿ä¾¡æ ¼ (t=T) S: 1000 # æ ªå¼ã®ç¾åœ¨ä¾¡æ ¼ (t=0) r: 0.005 # å¹¾ä½•ãƒ–ãƒ©ã‚¦ãƒ³é‹å‹•ã®drift sigma: 0.3 # å¹¾ä½•ãƒ–ãƒ©ã‚¦ãƒ³é‹å‹•ã®diffusion T: 1 # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®æº€æœŸ Case 2.\nãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã«ã‚ˆã‚‹ã‚³ãƒ¼ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®æ¨å®šä¾¡æ ¼ãŒï¼Œ BSå¼ã«ã‚ˆã‚‹ç†è«–ä¾¡æ ¼83.1821ã«æ¼¸è¿‘ã—ã¦ã„ã‚‹ï¼\nãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š n: 10000 # ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®å›æ•° K: 1100 # ã‚³ãƒ¼ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®æ¨©åˆ©è¡Œä½¿ä¾¡æ ¼ (t=T) S: 1000 # æ ªå¼ã®ç¾åœ¨ä¾¡æ ¼ (t=0) r: 0.005 # å¹¾ä½•ãƒ–ãƒ©ã‚¦ãƒ³é‹å‹•ã®drift sigma: 0.3 # å¹¾ä½•ãƒ–ãƒ©ã‚¦ãƒ³é‹å‹•ã®diffusion T: 1 # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®æº€æœŸ ","date":1606262400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606262400,"objectID":"177189947e741f52fd29bb2a328c1a29","permalink":"https://yumauchiumi.com/post/2020-11-25-monte-carlo-sim-bsoption-r/","publishdate":"2020-11-25T00:00:00Z","relpermalink":"/post/2020-11-25-monte-carlo-sim-bsoption-r/","section":"post","summary":"å¹¾ä½•ãƒ–ãƒ©ã‚¦ãƒ³é‹å‹•ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‘ã‚¹ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šï¼Œæ ªä¾¡ãŒå¹¾ä½•ãƒ–ãƒ©ã‚¦ãƒ³é‹å‹•ã«ã—ãŸãŒã†å ´åˆã®ã‚³ãƒ¼ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä¾¡æ ¼ã‚’ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚Šå°å‡ºã—ï¼Œã‚µãƒ³ãƒ—ãƒ«ãƒ‘ã‚¹ã®ç”Ÿæˆå›æ•°ã‚’å¢—ã‚„ã™ã“ã¨ã§ç†è«–å€¤ã«åæŸã™ã‚‹ã“ã¨ã‚’ãƒ—ãƒ­ãƒƒãƒˆã«ã‚ˆã‚Šç¢ºèªã™ã‚‹ï¼","tags":["Tips","R"],"title":"Monte Carlo Simulation of Black-Scholes Option Pricing Model","type":"post"},{"authors":null,"categories":["Finance"],"content":"Pythonã§backtestã™ã‚‹éš›ã®Tipsã‚’ã¾ã¨ã‚ãŸã‚‚ã®ã§ã™ï¼é¢å€’ãªå‰å‡¦ç†ã‚’ã•ãã£ã¨çµ‚ã‚ã‚‰ã›ã¦ãƒ¢ãƒ‡ãƒ«ä½œã‚Šã«å°‚å¿µã—ã¾ã—ã‚‡ã†ï¼ã¨ã„ã†ä¸»æ—¨ã§ã™ï¼è¨˜äº‹ã§ã¯ç´¹ä»‹ã—ã¦ã„ã¾ã›ã‚“ãŒï¼Œpandas-datareaderã§ãƒã‚¯ãƒ­ãƒ‡ãƒ¼ã‚¿ã‚‚ã ã„ãŸã„å–ã‚Œã‚‹ã®ã§ï¼Œè¤‡æ•°å› å­ãƒ¢ãƒ‡ãƒ«ãªã©ï¼Œã•ã¾ã–ã¾ãªãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªé¸æŠãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™ã“ã¨ãŒã§ãã¾ã™ï¼\nOverview:\nPythonã§Backtestã™ã‚‹ç’°å¢ƒã‚’æ•´ãˆã‚‹ï¼ æ±è¨¼TOPIXæ§‹æˆéŠ˜æŸ„ã‹ã‚‰å¯¾è±¡è³‡ç”£ã‚’é¸ã³æœ€å°åˆ†æ•£ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã‚’çµ„ã‚€ï¼ 1. æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã®å–å¾— ã¾ãšï¼Œpandas-datareaderã‚’ç’°å¢ƒã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ï¼\npandas-datareaderã¯ï¼Œæ ªä¾¡ãªã©ã®å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‚’Web APIçµŒç”±ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã‚‹ï¼ˆpandas.Dataframe friendlyã§ï¼‰ä¾¿åˆ©ãªPythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã§ã™ï¼IEX, World Bank, OECD, Yahoo! Financeï¼ŒFREDï¼ŒStooqãªã©ã®APIã‚’å†…éƒ¨ã§å©ãã€pythonã‚³ãƒ¼ãƒ‰ä¸Šã«å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã“ã¨ãŒã§ãã¾ã™ï¼è©³ã—ã„ä½¿ã„æ–¹ã¯å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã—ã¦ãã ã•ã„ï¼\n# Install pandas-datareader (latest version) pip install git+https://github.com/pydata/pandas-datareader.git # Or install pandas-datareader (stable version) pip install pandas-datareader ä»Šå›ã¯ï¼Œæ±äº¬è¨¼åˆ¸å–å¼•æ‰€ï¼ˆæ±è¨¼ï¼‰ã«ä¸Šå ´ã—ã¦ã„ã‚‹æ ªå¼éŠ˜æŸ„ã‚’å¯¾è±¡å•†å“ã¨ã—ã¾ã™ï¼ Webä¸Šã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã¯åœ§å€’çš„ã«ç±³å›½å¸‚å ´ã®ã‚‚ã®ãŒå¤šã„ã§ã™ãŒï¼Œãƒãƒ¼ãƒ©ãƒ³ãƒ‰ã®æœ€å¼·ã‚µã‚¤ãƒˆstooq.comã¯æ±äº¬è¨¼åˆ¸å–å¼•æ‰€ã®éå»ãƒ‡ãƒ¼ã‚¿ã‚’å…¬é–‹ã—ã¦ã„ã¾ã™ï¼pandas-datareaderã‚’ä½¿ã£ã¦stooqã‹ã‚‰å€‹åˆ¥éŠ˜æŸ„ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ã‚‡ã†ï¼\nåŸºæœ¬çš„ã«ã¯ï¼Œpandas_datareader.stooq.StooqDailyReader()ã‚’å®Ÿè¡Œã™ã‚Œã°OKã§ã™ï¼å¼•æ•°ã«ã¯ï¼Œå„å¸‚å ´ã«ç™»éŒ²ã—ã¦ã‚ã‚‹è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰(or ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚·ãƒ³ãƒœãƒ«)ã¨ã€ãƒ‡ãƒ¼ã‚¿å…¬é–‹å…ƒã®ã‚µã‚¤ãƒˆ(Yahoo!, Stooq, \u0026hellip;)ã‚’æŒ‡å®šã—ã¾ã™ï¼\næ±äº¬è¨¼åˆ¸å–å¼•æ‰€ã§å–ã‚Šæ‰±ã„ã•ã‚Œã¦ã„ã‚‹æ ªå¼éŠ˜æŸ„ã«ã¯ï¼Œ4æ¡ã®è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ãŒå‰²ã‚Šå½“ã¦ã‚‰ã‚Œã¦ã„ã‚‹ã®ã§ã€ä»Šå›ã¯ã“ã‚Œã‚’ä½¿ã„ã¾ã™ï¼ï¼ˆä¾‹ï¼šãƒˆãƒ¨ã‚¿è‡ªå‹•è»Šã®æ ªå¼ã¯ã€æ±è¨¼ã§ã¯è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ãŒ7203ã§ã‚ã‚‹éŠ˜æŸ„ã¨ã—ã¦ï¼ŒNYSEã§ã¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚·ãƒ³ãƒœãƒ«ãŒTMã§ã‚ã‚‹éŠ˜æŸ„ã¨ã—ã¦å–å¼•ã•ã‚Œã¦ã„ã¾ã™ï¼ï¼‰\nè©¦ã—ã«ï¼Œãƒˆãƒ¨ã‚¿è‡ªå‹•è»Š(æ±è¨¼:7203)ã®æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦ãƒ—ãƒ­ãƒƒãƒˆã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼Œ\nimport datetime import pandas_datareader start = datetime.datetime(2015, 1, 1) end = datetime.datetime(2020, 11, 30) stockcode = \u0026quot;7203.jp\u0026quot; # Toyota Motor Corporation (TM) df = pandas_datareader.stooq.StooqDailyReader(stockcode, start, end).read() df = df.sort_values(by='Date',ascending=True) display(df) # Show dataframe ----- Open\tHigh\tLow\tClose\tVolume Date\t2015-01-05\t6756.50\t6765.42\t6623.43\t6704.69\t10653925 2015-01-06\t6539.48\t6601.09\t6519.83\t6519.83\t13870266 2015-01-07\t6480.52\t6685.05\t6479.64\t6615.40\t12837377 2015-01-08\t6698.46\t6748.46\t6693.98\t6746.69\t11257646 2015-01-09\t6814.56\t6846.70\t6752.92\t6795.80\t11672928 ...\t...\t...\t...\t...\t... 2020-11-04\t7024.00\t7054.00\t6976.00\t6976.00\t6278100 2020-11-05\t6955.00\t7032.00\t6923.00\t6984.00\t5643400 2020-11-06\t7070.00\t7152.00\t7015.00\t7019.00\t11092900 2020-11-09\t7159.00\t7242.00\t7119.00\t7173.00\t7838600 2020-11-10\t7320.00\t7360.00\t7212.00\t7267.00\t8825700 æ—¥åˆ¥ã®æ ªä¾¡æ¨ç§»ãƒ‡ãƒ¼ã‚¿ãŒpandas.Dataframeã¨ã—ã¦å–å¾—ã§ãã¾ã—ãŸï¼ ã„ã¾ä½œæˆã—ãŸdfã®ä¸­èº«ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¦ã¿ã¾ã™ï¼ï¼ˆåŸºæœ¬çš„ã«çµ‚å€¤ã‚’ã¤ã‹ã„ã¾ã™ï¼‰\n# Plot timeseries (2015/1/1 - 2020/11/30) plt.figure(figsize=(12,8)) plt.plot(df.index, df[\u0026quot;Close\u0026quot;].values) plt.show() ä¸‹å›³ã®ã‚ˆã†ã«ï¼Œçµ‚å€¤(Close)ã®æ¨ç§»ãŒç°¡å˜ã«ãƒ—ãƒ­ãƒƒãƒˆã§ãã¾ã—ãŸï¼ 2. å¯¾è±¡è³‡ç”£ã®ãƒ‘ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œã‚‹ ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæœ€é©åŒ–å•é¡Œã‚’è§£ããŸã‚ã®æº–å‚™ã¨ã—ã¦ï¼Œè¤‡æ•°ã®è³‡ç”£(æ ªå¼éŠ˜æŸ„)ã«å¯¾ã™ã‚‹ãƒ‘ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œã‚Šï¼Œpandas.Dataframeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦æ•´ç†ã—ã¾ã™ï¼\nä»Šå›ã¯TOPIX 500ã«æ²è¼‰ã•ã‚Œã¦ã„ã‚‹éŠ˜æŸ„ã‹ã‚‰5ã¤é¸ã³ï¼ŒæŠ•è³‡å¯¾è±¡è³‡ç”£ã¨ã—ã¾ã™ï¼ã¾ãŸï¼Œå‰å‡¦ç†ã¨ã—ã¦ã€Œçµ‚å€¤ã€ã‚’ã€Œçµ‚å€¤ãƒ™ãƒ¼ã‚¹ã®åç›Šç‡ã€ã¸å¤‰æ›ã—ã¦ã„ã¾ã™ï¼ã“ã®éƒ¨åˆ†ã®ã‚³ãƒ¼ãƒ‰ã¯çŠ¶æ³ã«åˆã‚ã›ã¦å¤‰ãˆã¦ãã ã•ã„ï¼\nimport datetime import numpy as np import pandas as pd import pandas_datareader.data as web import pandas_datareader.stooq as stooq def get_stockvalues_tokyo(stockcode, start, end, use_ratio=False): \u0026quot;\u0026quot;\u0026quot; stockcode: market code of each target stock (ex. \u0026quot;NNNN\u0026quot;) defined by the Tokyo stock market. start, end: datetime object \u0026quot;\u0026quot;\u0026quot; # Get index data from https://stooq.com/ df = stooq.StooqDailyReader(f\u0026quot;{stockcode}.jp\u0026quot;, start, end).read() df = df.sort_values(by='Date',ascending=True) if use_ratio: df = df.apply(lambda x: (x - x[0]) / x[0] ) return df def get_paneldata_tokyo(stockcodes, start, end, use_ratio=False): # Use \u0026quot;Close\u0026quot; value only dfs = [] for sc in stockcodes: df = get_stockvalues_tokyo(sc, start, end, use_ratio)[['Close']] df = df.rename(columns={'Close': sc}) dfs.append(df) df_concat = pd.concat(dfs, axis=1) return df_concat get_paneldata_tokyo()ã‚’ä½¿ã£ã¦ãƒ‘ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã—ã¾ã™ï¼\nstart = datetime.datetime(2015, 1, 1) end = datetime.datetime(2020, 11, 30) stockcodes=[\u0026quot;1301\u0026quot;, \u0026quot;1762\u0026quot;, \u0026quot;1820\u0026quot;, \u0026quot;1967\u0026quot;, \u0026quot;2127\u0026quot;] df = get_paneldata_tokyo(stockcodes, start, end, use_ratio=True) display(df) # return ratio daily ----- 1301\t1762 1820 1967\t2127 Date\t2015-01-05\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000 2015-01-06\t-0.010929\t-0.018385\t-0.033937\t-0.002265\t-0.038448 2015-01-07\t-0.014564\t-0.020433\t-0.059863\t-0.013823\t-0.059680 2015-01-08\t-0.007302\t-0.016338\t-0.057883\t-0.013823\t-0.039787 2015-01-09\t0.000000\t-0.004490\t-0.031938\t-0.025407\t-0.043770 ...\t...\t...\t...\t...\t... 2020-10-29\t0.096138\t-0.032923\t-0.030777\t0.858573\t5.682321 2020-10-30\t0.093336\t-0.039657\t-0.041199\t0.832831\t5.704266 2020-11-02\t0.107748\t-0.026188\t-0.032198\t0.845702\t5.418978 2020-11-04\t0.099341\t-0.024392\t-0.020829\t0.858573\t5.704266 2020-11-05\t0.069315\t-0.014964\t-0.042147\t0.904909\t6.055390 ã“ã‚Œã§ï¼Œè©•ä¾¡å¯¾è±¡ã¨ãªã‚‹å„è³‡ç”£ã®ãƒ‘ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã—ãŸï¼\n3. ãƒãƒ¼ã‚³ãƒ“ãƒƒãƒ„ã®å¹³å‡åˆ†æ•£ãƒ¢ãƒ‡ãƒ«ã¨ãã®è§£æ³• æŠ•è³‡å¯¾è±¡ã¨ãªã‚‹è¤‡æ•°ã®è³‡ç”£ã«å¯¾ã—ã¦ï¼Œé©å½“ãªæŠ•è³‡æ¯”ç‡ã‚’ãã‚Œãã‚Œæ±ºå®šã™ã‚‹ã“ã¨ã‚’ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæœ€é©åŒ–ã¨ã„ã„ã¾ã™ï¼ä»Šå›ã¯ï¼Œæœ€ã‚‚åŸºæœ¬çš„ãªãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæœ€é©åŒ–ã®å•é¡Œè¨­å®šã¨ã—ã¦ï¼ŒMarkowitzãŒæå”±ã—ãŸå¹³å‡åˆ†æ•£ãƒ¢ãƒ‡ãƒ«(Mean-Variance Model)ã‚’æ¡ç”¨ã—ã¾ã™ï¼\n3.1. Markowitzã®å¹³å‡åˆ†æ•£ãƒ¢ãƒ‡ãƒ« Markowitzã®å¹³å‡åˆ†æ•£ãƒ¢ãƒ‡ãƒ«ã§ã¯ï¼Œã€Œãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®æœŸå¾…åç›Šç‡(Expected return)ãŒä¸€å®šå€¤ä»¥ä¸Šã¨ãªã‚‹ã€ã¨ã„ã†åˆ¶ç´„æ¡ä»¶ã®ä¸‹ã§ï¼Œã€Œãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®åˆ†æ•£ã‚’æœ€å°åŒ–ã™ã‚‹ã€æœ€é©åŒ–å•é¡Œã‚’è€ƒãˆã¾ã™ï¼\nä¸€èˆ¬ã«ï¼Œ$n$ã‚³ã®è³‡ç”£ã§æ§‹æˆã•ã‚Œã‚‹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®å ´åˆï¼Œãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®åˆ†æ•£ã¯$n$ã‚³ã®è³‡ç”£é–“ã®å…±åˆ†æ•£è¡Œåˆ—ã®äºŒæ¬¡å½¢å¼ã¨ãªã‚‹ã®ã§ï¼Œã“ã®æœ€é©åŒ–å•é¡Œã¯äºŒæ¬¡è¨ˆç”»å•é¡Œ(Quadratic Programming, QP)ã®ã‚¯ãƒ©ã‚¹ã¨ãªã‚Šï¼Œæ¬¡ã®ã‚ˆã†ã«å®šå¼åŒ–ã•ã‚Œã¾ã™ï¼\n$$ \\begin{align} \\underset{\\bf x}{\\rm minimize} ~~~ \u0026amp;{\\bf x}^T \\Sigma {\\bf x} \\\\ {\\rm subject~to} ~~~ \u0026amp;{\\bf r}^T {\\bf x} = \\sum_{i=1}^{n} r_i x_i \\geq r_e \\\\ \u0026amp;{|| {\\bf x} ||}_{1} = \\sum_{i=1}^{n} x_i = 1 \\\\ \u0026amp;x_i \\geq 0 ~~ (i = 1, \\cdots, n) \\end{align} $$\n$\\Sigma \\in \\mathbb{R}^{n \\times n}$ ãƒ¼ $n$ã‚³ã®è³‡ç”£ã®å…±åˆ†æ•£è¡Œåˆ— ${\\bf x} \\in \\mathbb{R}^{n}$ ãƒ¼ $n$ã‚³ã®è³‡ç”£ã®æŠ•è³‡æ¯”ç‡ãƒ™ã‚¯ãƒˆãƒ« $\\bar{\\bf r} \\in \\mathbb{R}^{n}$ ãƒ¼ $n$ã‚³ã®è³‡ç”£ã®æœŸå¾…åç›Šç‡ãƒ™ã‚¯ãƒˆãƒ« $x_i \\in \\mathbb{R}$ ãƒ¼ è³‡ç”£$i$ã®æŠ•è³‡æ¯”ç‡ $\\bar{r}_i \\in \\mathbb{R}$ ãƒ¼ è³‡ç”£$i$ã®æœŸå¾…åç›Šç‡ $r_e \\in \\mathbb{R}$ ãƒ¼ æŠ•è³‡å®¶ã®è¦æ±‚æœŸå¾…åç›Šç‡ $\\bar{r}_p \\in \\mathbb{R}$ ãƒ¼ ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®åç›Šç‡ã®æœŸå¾…å€¤ $\\sigma_p \\in \\mathbb{R}$ ãƒ¼ ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®åç›Šç‡ã®æ¨™æº–åå·® 1ã¤ç›®ã®åˆ¶ç´„å¼ã¯ï¼Œãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®æœŸå¾…åç›Šç‡ãŒä¸€å®šå€¤($=r_e$)ä»¥ä¸Šã¨ãªã‚‹ã“ã¨ã‚’è¦è«‹ã—ã¦ã„ã¾ã™ï¼2ã¤ç›®ï¼Œ3ã¤ç›®ã®åˆ¶ç´„å¼ã¯ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®å®šç¾©ã‹ã‚‰ãã‚‹è‡ªæ˜ãªã‚‚ã®ã§ã™ï¼è³‡ç”£ã®ç©ºå£²ã‚Šã‚’è¨±ã™å ´åˆï¼Œ3ã¤ç›®ã®åˆ¶ç´„å¼ã‚’é™¤ãã“ã¨ã‚‚ã‚ã‚Šã¾ã™ï¼\n3.2. CVXOPTã®ä½¿ã„æ–¹ Pythonã®å‡¸æœ€é©åŒ–å‘ã‘ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸CVXOPTã‚’ä½¿ã£ã¦ï¼Œã“ã®äºŒæ¬¡è¨ˆç”»å•é¡Œ(QP)ã‚’è§£ãã¾ã™ï¼ CVXOPTã§äºŒæ¬¡è¨ˆç”»å•é¡Œã‚’æ‰±ã†å ´åˆã¯ï¼Œè§£ããŸã„æœ€é©åŒ–å•é¡Œã‚’ä»¥ä¸‹ã®ä¸€èˆ¬åŒ–ã•ã‚ŒãŸãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«æ•´ç†ã—ã¦ï¼Œ\n$$ \\begin{align} \\underset{\\bf x}{\\rm minimize} ~~~ \u0026amp;\\frac{1}{2} {\\bf x}^{T} P {\\bf x} + {\\bf q}^{T} {\\bf x} \\\\ {\\rm subject~to} ~~~ \u0026amp; G {\\bf x} \\leq {\\bf h} \\\\ \u0026amp;A {\\bf x} = {\\bf b} \\end{align} $$\nãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿P,q,G,h,Aã‚’è¨ˆç®—ã—ï¼Œcvxopt.solvers.qp()é–¢æ•°ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ã§æœ€é©è§£ã¨æœ€é©å€¤ã‚’æ±‚ã‚ã¾ã™ï¼Markowitzã®å¹³å‡ãƒ»åˆ†æ•£ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯ï¼Œ\n$$ P = 2 \\cdot \\Sigma, ~~~ q = {\\bf 0}_n, ~~~ G = -1 \\cdot \\begin{pmatrix} \\bar{r}_1 \u0026amp; \\cdots \u0026amp; \\bar{r}_n \\\\ 1 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; \\cdots \u0026amp; 1 \\end{pmatrix}, ~~~ h = -1 \\cdot \\left( \\begin{array}{c} r_e \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right), ~~~ A = {\\bf 1}_n^{\\mathrm{T}}, ~~~ b = 1 $$\nã¨ãªã‚Šã¾ã™ï¼\nå‚è€ƒ:\nhttps://cvxopt.org/userguide/coneprog.html#quadratic-programming https://qiita.com/ryoshi81/items/8b0c6add3e367f94c828 3.3. Pythonã§è¨ˆç®— å¯¾è±¡è³‡ç”£ã®ãƒ‘ãƒãƒ«ãƒ‡ãƒ¼ã‚¿dfã‹ã‚‰ï¼Œå¿…è¦ãªçµ±è¨ˆé‡ã‚’è¨ˆç®—ã—ã¾ã™ï¼\nãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªå†…ã®è³‡ç”£é–“ã®å…±åˆ†æ•£è¡Œåˆ— $\\Sigma$ï¼š\ndf.cov() # Covariance matrix ----- 1301\t1762 1820\t1967\t2127 1301\t0.024211\t0.015340\t0.018243\t0.037772\t0.081221 1762\t0.015340\t0.014867\t0.015562\t0.023735\t0.038868 1820\t0.018243\t0.015562\t0.025023\t0.029918\t0.040811 1967\t0.037772\t0.023735\t0.029918\t0.109754\t0.312827 2127\t0.081221\t0.038868\t0.040811\t0.312827\t1.703412 ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªå†…ã®å„è³‡ç”£ã®æœŸå¾…åç›Šç‡ ${\\bf r}$ï¼š\ndf.mean().values # Expected returns ----- array([0.12547322, 0.10879767, 0.07469455, 0.44782516, 1.75209493]) CVXOPTã‚’ä½¿ã£ã¦æœ€é©åŒ–å•é¡Œã‚’è§£ãï¼\nimport cvxopt def cvxopt_qp_solver(r, r_e, cov): # CVXOPT QP Solver for Markowitz' Mean-Variance Model # See https://cvxopt.org/userguide/coneprog.html#quadratic-programming # See https://cdn.hackaday.io/files/277521187341568/art-mpt.pdf n = len(r) r = cvxopt.matrix(r) P = cvxopt.matrix(2.0 * np.array(cov)) q = cvxopt.matrix(np.zeros((n, 1))) G = cvxopt.matrix(np.concatenate((-np.transpose(r), -np.identity(n)), 0)) h = cvxopt.matrix(np.concatenate((-np.ones((1,1)) * r_e, np.zeros((n,1))), 0)) A = cvxopt.matrix(1.0, (1, n)) b = cvxopt.matrix(1.0) sol = cvxopt.solvers.qp(P, q, G, h, A, b) return sol r = df.mean().values # Expected returns r_e = 0.005 * # Lower bound for portfolio's return cov = df.cov() # Covariance matrix # Solve QP and derive optimal portfolio sol = cvxopt_qp_solver(r, r_e, cov) x_opt = np.array(sol['x']) print(x_opt) print(\u0026quot;Variance (x_opt) :\u0026quot;, sol[\u0026quot;primal objective\u0026quot;]) ----- pcost dcost gap pres dres 0: 4.3680e-03 -8.6883e-02 5e+00 2e+00 2e+00 1: 9.1180e-02 -2.2275e-01 5e-01 1e-01 1e-01 2: 2.1337e-02 -6.0274e-02 8e-02 2e-16 1e-16 3: 1.0483e-02 -1.7810e-03 1e-02 1e-16 3e-17 4: 4.9857e-03 1.5180e-03 3e-03 2e-16 8e-18 5: 4.0217e-03 3.6059e-03 4e-04 3e-17 1e-17 6: 3.7560e-03 3.7107e-03 5e-05 3e-17 1e-18 7: 3.7187e-03 3.7168e-03 2e-06 1e-17 4e-18 8: 3.7169e-03 3.7168e-03 2e-08 1e-16 6e-18 Optimal solution found. [ 5.56e-05] [ 1.00e+00] [ 1.76e-05] [ 3.84e-07] [ 2.63e-07] Variance (x_opt): 0.003716866155475511 # æœ€é©ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®åˆ†æ•£ æœ€é©è§£ï¼ˆå„è³‡ç”£ã¸ã®æœ€é©ãªæŠ•è³‡æ¯”ç‡ï¼‰ã¨ï¼Œæœ€é©å€¤ï¼ˆæœ€é©ãªæŠ•è³‡æ¯”ç‡ã‚’é©ç”¨ã—ãŸå ´åˆã®ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®åˆ†æ•£ï¼‰ãŒæ±‚ã‚ã‚‰ã‚Œã¾ã—ãŸï¼ãªãŠï¼Œä»Šå›ä½¿ã£ãŸå¹³å‡åˆ†æ•£ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹æœ€é©è§£ã¯ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®ãƒªã‚¹ã‚¯(åˆ†æ•£)ã«å¯¾ã™ã‚‹æœ€é©æ€§ã‚’é‡è¦–ã—ã¦ã„ã‚‹ã®ã§ï¼Œã€Œæœ€å°åˆ†æ•£ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã€ã¨å‘¼ã°ã‚Œã¾ã™ï¼\nãªãŠï¼Œåç›Šç‡ã«å¯¾ã™ã‚‹è©•ä¾¡æŒ‡æ¨™ã«ã¯ï¼Œç„¡ãƒªã‚¹ã‚¯è³‡ç”£ã®åç›Šç‡(ã‚¤ãƒ³ãƒ•ãƒ¬ç‡)ã‚’åŠ å‘³ã—ãŸã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ªã‚’ç”¨ã„ã‚‹ã‚±ãƒ¼ã‚¹ãŒå¤šã„ã§ã™ï¼backtestã®æ–¹æ³•ã«ã¤ã„ã¦ã¯ã„ãã¤ã‹æµå„€ãŒã‚ã‚‹ã®ã§ï¼Œå°‚é–€æ›¸ã‚„è«–æ–‡ã‚’å‚ç…§ã—ã¦ãã ã•ã„ï¼\n4. å®Ÿè£…ä¾‹ ä¸Šã®ã‚³ãƒ¼ãƒ‰ã‚’ã¾ã¨ã‚ã¦ï¼Œè‡ªä½œã®ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆç”¨Pythonã‚¯ãƒ©ã‚¹MarkowitzMinVarianceModel()ã‚’ä½œã‚Šã¾ã—ãŸï¼ ä»¥ä¸‹ã¯å‚è€ƒä¾‹ã§ã™ï¼\n4.1. ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆç”¨ã®Pythonã‚¯ãƒ©ã‚¹ import cvxopt import numpy as np import pandas as pd import matplotlib.pyplot as plt class MarkowitzMinVarianceModel(): \u0026quot;\u0026quot;\u0026quot; Args: ===== - df: pandas.dataframe panel data for target assets for the portfolio. its index must be `numpy.datetime64` type. its columns must be time-series data of target assets. - window_size: int the size of time-window which is used when deriving (or updating) the portfolio. - rebalance_freq: int rebalance frequency of the portfolio. - r_e: float min of the return ratio (= capital gain / investment). - r_f: float rate of returns of the risk-free asset. \u0026quot;\u0026quot;\u0026quot; def __init__(self, df, window_size, rebalance_freq, r_e=None, r_f=None): self.df = self._reset_index(df) self.df_chg = self.df.pct_change() self.df_chg[:1] = 0.0 # set 0.0 to the first record self.df_bt = None self.df_bt_r = None self.df_bt_x = None self.window_size = window_size self.rebalance_freq = rebalance_freq self.jgb_int = 0.0001 # 0.01% per year (Japanese Government Bond) self.r_f = r_f if r_f is not None else self.jgb_int * (1/12) # adjust monthly self.r_e = r_e if r_e is not None else r_f def _reset_index(self, df): df = df.copy() df['date'] = pd.to_datetime(df.index) df = df.set_index('date') return df def get_dfbt_r(self): return self.df_bt_r def get_dfbt_x(self): return self.df_bt_x def backtest(self): date_init = self.df.index.values[self.window_size] df_bt = pd.DataFrame([[0.0, np.nan]], index=[date_init], columns=['ror', 'std']) df_bt_r = pd.DataFrame(columns=list(self.df.columns.values)) df_bt_x = pd.DataFrame(columns=list(self.df.columns.values)) for idx, date in enumerate(self.df.index.values): if idx \u0026gt;= self.window_size + self.rebalance_freq: if (idx - self.window_size) % self.rebalance_freq == 0: # df_chg_train st = idx - self.rebalance_freq - self.window_size ed = idx - self.rebalance_freq df_chg_train = self.df_chg[st:ed] # expected returns per target term if isinstance(self.r_e, pd.core.frame.DataFrame): r_e = self.r_e.iloc[st:ed].values.mean() else: r_e = self.r_e # x_p: min variance portfolio x_p = self.calc_portfolio(df_chg_train, r_e) # df_chg_test st = idx - self.rebalance_freq ed = idx df_chg_test = self.df_chg[st:ed] df_chgcum_test = (1.0 + df_chg_test).cumprod() - 1.0 # ror_p: rate of return (portfolio) ror_test = df_chgcum_test.iloc[-1].values ror_p = float(np.dot(ror_test, x_p)) df_bt_r.loc[date] = ror_test df_bt_x.loc[date] = x_p # std (portfolio) if self.rebalance_freq == 1: std_p = np.nan else: std_test = df_chg_test.std(ddof=True).values std_p = float(np.dot(std_test, np.abs(x_p))) # append df_one = pd.DataFrame([[ror_p, std_p]], index=[date], columns=df_bt.columns) df_bt = df_bt.append(df_one) # reset index self.df_bt = self._reset_index(df_bt) self.df_bt_r = self._reset_index(df_bt_r) self.df_bt_x = self._reset_index(df_bt_x) return self.df_bt def calc_portfolio(self, df_retchg, r_e): r = df_retchg.mean().values cov = np.array(df_retchg.cov()) x_opt = self.cvxopt_qp_solver(r, r_e, cov) return x_opt def cvxopt_qp_solver(self, r, r_e, cov): \u0026quot;\u0026quot;\u0026quot; CVXOPT QP Solver for Markowitz' Mean-Variance Model - See also https://cvxopt.org/userguide/coneprog.html#quadratic-programming - See also https://cdn.hackaday.io/files/277521187341568/art-mpt.pdf r: mean returns of target assets. (vector) r_e: min of the return ratio (= capital gain / investment). cov: covariance matrix of target assets. (matrix) \u0026quot;\u0026quot;\u0026quot; n = len(r) r = cvxopt.matrix(r) # Create Objective matrices P = cvxopt.matrix(2.0 * np.array(cov)) q = cvxopt.matrix(np.zeros((n, 1))) # Create constraint matrices G = cvxopt.matrix(np.concatenate((-np.transpose(r), -np.eye(n)), 0)) h = cvxopt.matrix(np.concatenate((-np.ones((1,1))*r_e, np.zeros((n,1))), 0)) A = cvxopt.matrix(1.0, (1, n)) b = cvxopt.matrix(1.0) # Adjust params (stop log messages) cvxopt.solvers.options['show_progress'] = False # default: True cvxopt.solvers.options['maxiters'] = 1000 # default: 100 sol = cvxopt.solvers.qp(P, q, G, h, A, b) x_opt = np.squeeze(np.array(sol['x'])) return x_opt def get_yearly_performance(self): if self.df_bt is None: pass else: df_yearly = self.df_bt[[\u0026quot;ror\u0026quot;]].resample('y').sum() df_yearly[\u0026quot;std\u0026quot;] = self.df_bt[\u0026quot;ror\u0026quot;].resample('y').std().values df_yearly[\u0026quot;sharpe_ratio\u0026quot;] = df_yearly.apply(lambda d: (d[\u0026quot;ror\u0026quot;] - self.r_f) / d[\u0026quot;std\u0026quot;], axis=1) return df_yearly def evaluate_backtest(self, logging=False): if self.df_bt is None: pass else: self.r_mean = self.df_bt[\u0026quot;ror\u0026quot;].mean() self.r_std = self.df_bt[\u0026quot;ror\u0026quot;].std(ddof=True) self.sharpe_ratio = (self.r_mean - self.r_f) / self.r_std self.net_capgain = (self.df_bt[\u0026quot;ror\u0026quot;] + 1.0).cumprod().iloc[-1] - 1.0 self.r_mean_peryear = 12 * self.r_mean self.r_std_peryear = np.sqrt(12) * self.r_std self.sharpe_ratio_peryear = (self.r_mean_peryear - self.jgb_int) / self.r_std_peryear if logging: print(\u0026quot;Portfolio Performance\u0026quot;) print(\u0026quot;=======================\u0026quot;) print(\u0026quot;Returns per month\u0026quot;) print(\u0026quot; sharpe ratio : {:.8f}\u0026quot;.format(self.sharpe_ratio)) print(\u0026quot; mean of returns : {:.8f}\u0026quot;.format(self.r_mean)) print(\u0026quot; std of returns : {:.8f}\u0026quot;.format(self.r_std)) print(\u0026quot; risk-free rate : {:.8f}\u0026quot;.format(self.r_f)) print(\u0026quot; capgain ratio : {:.8f}\u0026quot;.format(self.net_capgain)) print(\u0026quot;Returns per year\u0026quot;) print(\u0026quot; sharpe ratio : {:.8f}\u0026quot;.format(self.sharpe_ratio_peryear)) print(\u0026quot; mean of returns : {:.8f}\u0026quot;.format(self.r_mean_peryear)) print(\u0026quot; std of returns : {:.8f}\u0026quot;.format(self.r_std_peryear)) def plot_returns(self): if self.df_bt is None: pass else: xlabels = [d.strftime('%Y-%m') for idx, d in enumerate(self.df_bt.index) if idx % 12 == 0] fig, ax = plt.subplots(figsize=(12,6)) ax.plot(self.df_bt.index.values, self.df_bt[\u0026quot;ror\u0026quot;].values, label=\u0026quot;rate of returns\u0026quot;) ax.plot(self.df_bt.index.values, self.df_bt[\u0026quot;ror\u0026quot;].cumsum().values, label=\u0026quot;total capital gain ratio\u0026quot;) ax.legend(loc=\u0026quot;upper left\u0026quot;) ax.set_xticks(xlabels) ax.set_xticklabels(xlabels, rotation=40) return fig def plot_returns_histgram(self): if self.df_bt is None: pass else: x = self.df_bt[\u0026quot;ror\u0026quot;].values r_mean = \u0026quot;{:.4f}\u0026quot;.format(x.mean()) r_std = \u0026quot;{:.4f}\u0026quot;.format(x.std()) fig, ax = plt.subplots(figsize=(12,6)) ax.hist(x, bins=30, alpha=0.75) ax.set_title(f\u0026quot;mean={r_mean}, std={r_std}\u0026quot;) return fig 4.2. ä½¿ã„æ–¹ å¯¾è±¡è³‡ç”£ã¨ã—ã¦TOPIX Core30ã«å«ã¾ã‚Œã‚‹å†…å›½æ ª30éŠ˜æŸ„ã‚’é¸ã³ï¼Œã“ã‚Œã‚‰ã«(æœ€é©ãª)æŠ•è³‡æ¯”ç‡ã‚’ä¸ãˆã¦ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼\nã¾ãšï¼Œpandas_datareader.data.DataReaderã§TOPIX Core30æ§‹æˆéŠ˜æŸ„ã®ãƒ’ã‚¹ãƒˆãƒªã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ï¼Œã™ã“ã—æ•´å½¢ã—ã¾ã™ï¼1\n# Get historical data st = '2004/10/31' # start date ed = '2020/10/31' # end date stocks_topix30 = [2914, 3382, 4063, 4452, 4502, 4503, 5401, 6301, 6501, 6502, 6752, 6758, 6954, 7201, 7203, 7267, 7751, 8031, 8058, 8306, 8316, 8411, 8604, 8766, 8802, 9021, 9432, 9433, 9437, 9984] # list of tickers in TOPIX Core30 symbols = [str(s)+'.T' for s in stocks_topix30] dfs = [] for symbol in symbols: df = pandas_datareader.data.DataReader(symbol, 'yahoo', st, ed) # daily df = df.resample('M').mean() # daily -\u0026gt; monthly df = df.sort_values(by='Date', ascending=True) df = df.fillna(method='ffill') # 1ã¤å‰ã®è¡Œã®å€¤ã§åŸ‹ã‚ã‚‹ df = df[['Close']].rename(columns={'Close': symbol}) dfs.append(df) df_tpx30 = pd.concat(dfs, axis=1) # fill nan for col in df_tpx30.columns: st_idx = df_tpx30[col].first_valid_index() ed_idx = df_tpx30[col].last_valid_index() # for any columns (stocks) if df_tpx30[col].isnull().any(): # New listing (æ–°è¦ä¸Šå ´) if st_idx != df_tpx30.index[0]: df_tpx30[col] = df_tpx30[col].fillna(df_tpx30[col][st_idx]) # Delisting (ä¸Šå ´å»ƒæ­¢) if df_tpx30.index[-1] != ed_idx: df_tpx30[col] = df_tpx30[col].fillna(df_tpx30[col][ed_idx]) ã“ã‚“ãªæ„Ÿã˜ã®ãƒ‘ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ãŒã§ãã‚Œã°æº–å‚™OKï¼\ndf_tpx30.tail() Date\t2914.T\t3382.T\t4063.T\t4452.T\t4502.T\t4503.T\t5401.T\t6301.T\t6501.T\t6502.T\t...\t8316.T\t8411.T\t8604.T\t8766.T\t8802.T\t9021.T\t9432.T\t9433.T\t9437.T\t9984.T\t2020-06-30\t2147.159091\t3710.272727\t12471.136364\t8774.045455\t4024.818182\t1824.386364\t1063.240909\t2220.500000\t3561.590909\t3275.681818\t...\t3172.181818\t1363.545455\t486.763636\t4795.681818\t1701.840909\t6502.181818\t2500.454545\t3184.863636\t2913.454545\t5287.318182 2020-07-31\t1933.785714\t3427.333333\t12802.619048\t8465.428571\t3763.000000\t1731.785714\t998.023810\t2239.047619\t3371.714286\t3450.714286\t...\t3029.809524\t1347.857143\t491.138096\t4699.238095\t1576.000000\t5405.428571\t2523.428571\t3289.142857\t2944.547619\t6311.333333 2020-08-31\t1995.300000\t3399.050000\t12785.500000\t8041.000000\t3967.150000\t1712.075000\t1009.300000\t2207.650000\t3489.050000\t3351.750000\t...\t3023.050000\t1402.250000\t531.764998\t4779.850000\t1644.575000\t5106.050000\t2570.875000\t3285.300000\t3066.400000\t6453.700000 2020-09-30\t1970.425000\t3357.000000\t13804.250000\t8053.550000\t3898.200000\t1622.550000\t1073.164999\t2352.600000\t3631.650000\t2941.300000\t...\t3090.425000\t1402.175000\t522.770003\t4871.900000\t1638.575000\t5578.100000\t2313.800000\t2851.150000\t2879.875000\t6278.600000 2020-10-31\t1989.095238\t3412.714286\t14189.761905\t7731.619048\t3568.904762\t1489.047619\t1070.538095\t2432.285714\t3605.000000\t2783.952381\t...\t2969.428571\t1311.571429\t487.642857\t4804.000000\t1611.214286\t4938.571429\t2237.142857\t2742.738095\t3882.095238\t6991.047619 ã‚ã¨ã¯ï¼Œè‡ªä½œã‚¯ãƒ©ã‚¹MarkowitzMinVarianceModel()ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆmodelã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿df_tpx30ã‚’é£Ÿã‚ã›ã¦ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œï¼\nfrom datetime import datetime # Const. ST_BACKTEST = datetime(2011,10,31) # Investment period (start date) ED_BACKTEST = datetime(2020,10,31) # Investment period (end date) # Params params = { \u0026quot;window_size\u0026quot;: 36, # åç›Šç‡ã®ç‰¹æ€§é‡(å¹³å‡ï¼Œåˆ†æ•£)ã®æ¨å®šã«ä½¿ã†æœŸé–“ (ä¾‹: é‹ç”¨æ™‚ã‹ã‚‰éå»36ã‚«æœˆ) \u0026quot;rebalance_freq\u0026quot;: 1, # ãƒªãƒãƒ©ãƒ³ã‚¹ã®é »åº¦ (1ã‹æœˆã”ã¨ã«ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªå†…ã®æŠ•è³‡æ¯”ç‡ã‚’å¤‰æ›´) \u0026quot;r_f\u0026quot;: 0.0001 * (1/12) # ãƒªã‚¹ã‚¯ãƒ•ãƒªãƒ¼ãƒ¬ãƒ¼ãƒˆ (æ—¥æœ¬å›½å‚µ10å¹´ç‰©åˆ©å›ã‚Š:0.01%ã‚’å˜åˆ©è¨ˆç®—ã§æœˆæ¬¡ã«å¤‰æ›) } # Data st = (ST_BACKTEST - relativedelta(months=params[\u0026quot;window_size\u0026quot;])).strftime('%Y-%m-%d') ed = ED_BACKTEST.strftime('%Y-%m-%d') df = df_tpx30[st:ed] params[\u0026quot;r_e\u0026quot;]= df_tpx[st:ed] # è¦æ±‚æœŸå¾…åç›Šç‡(r_e)ã¯åŒæ™‚æœŸã®TOPIX Indexã®åç›Šç‡ã¨ã™ã‚‹ (df_tpxä½œæˆã‚³ãƒ¼ãƒ‰ã¯çœç•¥) # Create model model = MarkowitzMinVarianceModel(df, **params) # Backtest by model df_bt = model.backtest() ã“ã“ã‹ã‚‰ã¯ï¼Œè‡ªä½œã‚¯ãƒ©ã‚¹MarkowitzMinVarianceModel()ã«ç”¨æ„ã—ãŸãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆè©•ä¾¡ç”¨ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ã†ï¼(åˆ†æã¯ç„¡é™å¤§)\nãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡ # Evaluate model.evaluate_backtest(logging=True) Portfolio Performance ======================= Returns per month sharpe ratio : 0.18788996 mean of returns : 0.00735206 std of returns : 0.03908527 risk-free rate : 0.00000833 capgain ratio : 1.04714952 Returns per year sharpe ratio : 0.65086993 mean of returns : 0.08822476 std of returns : 0.13539535 ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®åç›Šç‡ãƒ»ç´¯ç©åç›Šç‡ãƒ—ãƒ­ãƒƒãƒˆ fig = model.plot_returns() # Plot returns ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®æœˆæ¬¡åç›Šç‡åˆ†å¸ƒ fig = model.plot_returns_histgram() ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®å¹´æ¬¡ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ df_yearly = model.get_yearly_performance() df_yearly date\tror\tstd sharpe_ratio 2011-12-31\t-0.0001\t3.3888e-05\t-3.6736 2012-12-31\t-0.0444\t1.2435e-02\t-3.5695 2013-12-31\t0.5524\t6.5010e-02\t8.4973 2014-12-31\t0.2448\t5.2800e-02\t4.6357 2015-12-31\t0.0952\t4.1543e-02\t2.2923 2016-12-31\t-0.0970\t4.0639e-02\t-2.3871 2017-12-31\t0.2486\t3.0262e-02\t8.2144 2018-12-31\t-0.0097\t3.6705e-02\t-0.2644 2019-12-31\t0.0254\t3.1904e-02\t0.7947 2020-12-31\t-0.1793\t7.3461e-02\t-2.4404 ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªå†…ã®å„éŠ˜æŸ„ã®åç›Šç‡(%) df_bt_r = model.get_dfbt_r() # rate_of_returns df_bt_x = model.get_dfbt_x() # investment_ratio df1 = df_bt_r * df_bt_x # (rate_of_returns) Ã— (investment_ratio) df1 = df1.resample(\u0026quot;y\u0026quot;).sum() df1.columns = [c.replace(\u0026quot;.T\u0026quot;, \u0026quot;\u0026quot;) for c in df1.columns] df1 = df1.T * 100 # transpose \u0026amp;\u0026amp; convert as pct. df1.columns = [c.strftime('%Y') for c in df1.columns] plt.figure(figsize=(12,12)) sns.heatmap(df1, cmap=\u0026quot;RdBu\u0026quot;, center=0, annot=True, fmt=\u0026quot;.2f\u0026quot;, cbar=True) plt.show() æœ€å¾Œã¾ã§èª­ã‚“ã§ã„ãŸã ãï¼Œã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼\nstocks_topix30ã¯TOPIX Core30æ§‹æˆéŠ˜æŸ„ã®è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆã§ã™ï¼æ§‹æˆéŠ˜æŸ„ã¯æ¯å¹´10/31ã«æ›´æ–°ã•ã‚Œã¾ã™ï¼ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã‚’å†ç¾ã—ãŸã„å ´åˆstocks_topix30 = [2914, 3382, 4063, 4452, 4502, 4503, 5401, 6301, 6501, 6502, 6752, 6758, 6954, 7201, 7203, 7267, 7751, 8031, 8058, 8306, 8316, 8411, 8604, 8766, 8802, 9021, 9432, 9433, 9437, 9984]ã¨ã—ã¦ãã ã•ã„ï¼\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":1605139200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605139200,"objectID":"dd56f9d99d441913a4d17115ead32778","permalink":"https://yumauchiumi.com/post/2020-11-12-portfolio-optimization-by-python/","publishdate":"2020-11-12T00:00:00Z","relpermalink":"/post/2020-11-12-portfolio-optimization-by-python/","section":"post","summary":"Set-up the basic environment for backtesting with Python, select some stocks from the TSE TOPIX series, and build the minimum diversified portfolio.","tags":["Python","Portfolio","Optimization","Trading","Market"],"title":"Basic Portfolio Optimization with Python: Markowitz's Mean-Variance Model","type":"post"},{"authors":null,"categories":["Personal"],"content":" Simon Wood, a leading statistician in UK being famous for the text book of the GAMs introduced the lockdown reading list in his homepage. It looks interesting, so I\u0026rsquo;ll share it here. (most of them are Blackwell\u0026rsquo;s, books)\nCollapse (Jared Diamond) on how societies are destroyed, not by external forces, but by their failure to adapt their cultural norms to those forces. Thinking Fast and Slow (Daniel Kahneman) on the pitfalls of our intuitive reasoning, especially about risk and uncertainty. Mistakes were made, but not by me (Carol Tarvis and Elliot Aronson) on the psychology of sticking with bad decisions. The Parable of the Old Man and the Young by Wilfred Owen, on consequences of the above. Economics The User\u0026rsquo;s Guide (Ha-Joon Chang) on what you really need to know about economics, and how it isn\u0026rsquo;t just a scaled up version of household accounting. The Great Crash 1929 (Galbraith) a delightful disection of economic hubris (and the need for stabilizing controls that we long since did away with). The Rise and Fall of the Third Reich (William Shirer) detailing exactly how things went wrong in Germany after the Great Depression. Witch hunting in Scotland (Brian Levack) on the Scottish experience of the great European witchcraft panic (James I/VI wrote a treatise on Witchcraft). Wood and Thomas paper on the problems of prediction with disease models in the absense of direct validation data (the least impressive item here). ","date":1593475200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593475200,"objectID":"93cba2be93672db8c2b1de0e9060b5a1","permalink":"https://yumauchiumi.com/post/2020-06-30-sw-lockdown-reading-list/","publishdate":"2020-06-30T00:00:00Z","relpermalink":"/post/2020-06-30-sw-lockdown-reading-list/","section":"post","summary":"Simon Wood, a leading statistician in UK being famous for the text book of the GAMs introduced the lockdown reading list in his homepage. It looks interesting, so I\u0026rsquo;ll share it here.","tags":["Tips","Books"],"title":"Simon N Wood's \"lockdown reading list\"","type":"post"},{"authors":null,"categories":["Finance"],"content":" Self-discriptive system within time change.\nThe simplest example of self-discriptive systems is the exponential increase. This is because the simplest \u0026ldquo;change\u0026rdquo; of a variable $x$ is a first derivative of its time $dx/dt$, and the simplest form of the function \u0026ldquo;$f(x)$\u0026rdquo; determined by a variable $x$ is a linear one $Cx$.\nCase1: Growth is constant with the current state (state is invariant over time) $$ \\frac{dx}{dt} = C $$\nCase2: Growth is linear with the current state (state changes exponentially) $$ \\frac{dx}{dt} = Cx $$\nCase3: Growth is complex but relative to the current state (generalization) $$ \\frac{dx}{dt} = f(x) $$\nActual situations 1. Money/Capital creates new money/capital\nThis is the principle of capital markets and investment. Or what is called Capital Gain. Source of motivation on the lender side.\nã“ã‚Œã¯è³‡æœ¬å¸‚å ´ã¨æŠ•è³‡ã®åŸç†ï¼ã„ã‚ã‚†ã‚‹ã‚­ãƒ£ãƒ”ã‚¿ãƒ«ã‚²ã‚¤ãƒ³ï¼è²¸ã—æ‰‹ã®å‹•æ©Ÿã®æºæ³‰ï¼\n2. Trust/Credit creates new trust/credit\nThe most obvious example is banking. Actually, there is a word of credit creation. Lending is gradually increased based on credit. This is the principle of the borrower.\nã‚ã‹ã‚Šã‚„ã™ã„ä¾‹ã¯éŠ€è¡Œæ¥­ï¼ å®Ÿéš›ï¼Œä¿¡ç”¨å‰µé€ ã¨ã„ã†è¨€è‘‰ãŒã‚ã‚‹ãã‚‰ã„ï¼ä¿¡ç”¨ã‚’ã‚‚ã¨ã«ï¼Œå°‘ã—ãšã¤è²¸å‡ºã‚’å¢—ã‚„ã™ï¼ã“ã‚Œã¯å€Ÿã‚Šæ‰‹ã®åŸç†ï¼\nThe modern financial system is supported by the principle that capital and credit increase/decrease exponentially. This principle creates a dynamic phenomenon (spiral) that motivates lenders and borrowers and that \u0026ldquo;lending and borrowing exponentially increases/decreases.\u0026rdquo; This is inflation and deflation.\nç¾ä»£é‡‘èã‚·ã‚¹ãƒ†ãƒ ã¯ï¼Œã€Œè³‡æœ¬ã¨ä¿¡ç”¨ãŒæŒ‡æ•°å¢—åŠ /æ¸›å°‘ã™ã‚‹ã€ã¨ã„ã†åŸç†ã«ã‚ˆã£ã¦æ”¯ãˆã‚‰ã‚Œã‚‹ï¼ã“ã®åŸç†ã¯ï¼Œè²¸ã—æ‰‹ã¨å€Ÿã‚Šæ‰‹ã«å‹•æ©Ÿã‚’ä¸ãˆã€ŒæŒ‡æ•°å¢—åŠ /æ¸›å°‘ã™ã‚‹è²¸é‡‘ã¨å€Ÿé‡‘ã€ã¨ã„ã†ç¾è±¡ï¼ˆã‚¹ãƒ‘ã‚¤ãƒ©ãƒ«ï¼‰ã‚’ç”Ÿã‚€ï¼ã“ã‚ŒãŒã‚¤ãƒ³ãƒ•ãƒ¬ã¨ãƒ‡ãƒ•ãƒ¬ï¼\nA vested interest (a large corporation or a large political party) can be established as a vested interest by reproducing credit and achievement in the future by accumulating past credit and achievement.\næ—¢å¾—æ¨©ç›Šï¼ˆå¤§ä¼æ¥­ã‚„å¤§æ”¿å…šï¼‰ã¯ï¼Œéå»ã®ä¿¡ç”¨ã¨å®Ÿç¸¾ã®ç©ã¿é‡ã­ã«ã‚ˆã‚Šï¼Œå°†æ¥ã®ä¿¡ç”¨ã¨å®Ÿç¸¾ã‚’å†ç”Ÿç”£ã™ã‚‹ã“ã¨ã§ï¼Œæ—¢å¾—æ¨©ç›Šã¨ã—ã¦æˆç«‹ã™ã‚‹ï¼\nTo make matters difficult, \u0026ldquo;trust\u0026rdquo; and \u0026ldquo;achievement\u0026rdquo; are complementary. \u0026ldquo;Trust\u0026rdquo; provides an opportunity to unlock new achievements. The new â€œachievementâ€ repairs and strengthens trust. In other words, vested interests are invincible unless environmental changes occur.\nå„ä»‹ãªã“ã¨ã«ã€Œä¿¡é ¼ã€ã¨ã€Œå®Ÿç¸¾ã€ã¯ç›¸è£œé–¢ä¿‚ã«ã‚ã‚‹ï¼ã€Œä¿¡é ¼ã€ã¯æ–°ãŸãªå®Ÿç¸¾è§£é™¤ã¸ã®æ©Ÿä¼šã‚’æä¾›ã™ã‚‹ï¼æ–°ãŸãªã€Œå®Ÿç¸¾ã€ã¯ä¿¡é ¼ã‚’è£œä¿®ãƒ»å¼·åŒ–ã™ã‚‹ï¼ã¤ã¾ã‚Šï¼Œæ—¢å¾—æ¨©ç›Šã¯ç’°å¢ƒå¤‰åŒ–ãŒèµ·ããªã„é™ã‚Šç„¡æ•µï¼\nThe first step for new powers (ventures and youth) to scale up is to win trust or to make some achievements. In addition, it is even better as it causes environmental changes and innovation.\næ–°èˆˆå‹¢åŠ›ï¼ˆãƒ™ãƒ³ãƒãƒ£ãƒ¼ã‚„è‹¥è€…ï¼‰ãŒã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ã™ã‚‹ãŸã‚ã®ç¬¬ä¸€æ­©ã¯ã€Œä¿¡é ¼ã€ã‚’å‹ã¡å–ã‚‹ã‹ï¼Œã€Œå®Ÿç¸¾ã€ã§é»™ã‚‰ã›ã‚‹ã‹ï¼ åŠ ãˆã¦ï¼Œç’°å¢ƒå¤‰åŒ–ã‚„ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã‚’èµ·ã“ã™ã¨ãªãŠè‰¯ã„ï¼\nAs a result, The risk-loving youths first try to walk the cycle from the trust to the achievement, but The risk-averse youths first try to walk the cycle from the achievement to the trust.\nçµæœã¨ã—ã¦ï¼Œ ãƒªã‚¹ã‚¯é¸å¥½çš„ãªè‹¥è€…ã¯ï¼Œä¿¡é ¼ã‹ã‚‰å®Ÿç¸¾ã¸ã¨ã„ã†ã‚µã‚¤ã‚¯ãƒ«ã‚’æ­©ã‚‚ã†ã¨ã—ï¼Œ ãƒªã‚¹ã‚¯å›é¿çš„ãªè‹¥è€…ã¯ï¼Œå®Ÿç¸¾ã‹ã‚‰ä¿¡é ¼ã¸ã¨ã„ã†ã‚µã‚¤ã‚¯ãƒ«ã‚’æ­©ã‚‚ã†ã¨ã™ã‚‹ï¼\n4. Follower/Fan creates new follower/fan\nIn order for super popular corporate brands, entertainers, celebrities, and influencers to be born, their fans/followers need to create new fans/followers.\nè¶…äººæ°—ãªä¼æ¥­ãƒ–ãƒ©ãƒ³ãƒ‰ã‚„èŠ¸èƒ½äººï¼Œã‚»ãƒ¬ãƒ–ï¼Œã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ãŒç”Ÿã¾ã‚Œã‚‹ãŸã‚ã«ã¯ï¼Œå½¼ã‚‰ã®ãƒ•ã‚¡ãƒ³/ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ãŒæ–°ãŸãªãƒ•ã‚¡ãƒ³/ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ã‚’ç”Ÿã‚€å¿…è¦ãŒã‚ã‚‹ï¼\nNowadays, this mechanism has been strengthened in the entertainment industry where globalization by the Internet has been achieved, and in platforms where search and recommendation algorithms that strongly fit past trends are dominant.\nè¿‘å¹´ï¼Œã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã«ã‚ˆã‚‹ã‚°ãƒ­ãƒ¼ãƒãƒ«åŒ–ãŒé”æˆã•ã‚ŒãŸã‚¨ãƒ³ã‚¿ãƒ¡ç”£æ¥­ã‚„ï¼Œéå»ã®å‚¾å‘ã«å¼·ãfitã™ã‚‹æ¤œç´¢ã‚„æ¨è–¦ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒæ”¯é…çš„ãªãƒ—ãƒ©ãƒƒãƒˆãƒ›ãƒ¼ãƒ ã«ãŠã„ã¦ï¼Œã“ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã¯å¼·åŒ–ã•ã‚Œã¦ã„ã‚‹ï¼\nThe more followers/fans there are, the more followers/fans there are. The services with more registrants/subscribers tend to have more registrants/subscribers.\nãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼/ãƒ•ã‚¡ãƒ³ãŒå¤šã„äººã»ã©ï¼Œãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼/ãƒ•ã‚¡ãƒ³ãŒå¢—ãˆã‚„ã™ã„ï¼ ç™»éŒ²è€…/è³¼èª­è€…ã®å¤šã„ã‚µãƒ¼ãƒ“ã‚¹ã»ã©ï¼Œç™»éŒ²è€…/è³¼èª­è€…ãŒå¢—ãˆã‚„ã™ã„ï¼\n3. Popularity/Evaluation creates new popularity/evaluation\nFor products that sell well, the \u0026ldquo;selling\u0026rdquo; state itself becomes valuable.\nå£²ã‚Œã¦ã„ã‚‹ã‚‚ã®ã¯ï¼Œå£²ã‚Œã¦ã„ã‚‹ã¨ã„ã†ã“ã¨è‡ªä½“ãŒä¾¡å€¤ã«ãªã‚‹ï¼\nThis phenomenon is often explained by RenÃ© Girard\u0026rsquo;s triangular desire from the perspective of consumer sentiment, and by the network externality from the perspective of industrial organization theory.\nã“ã®ç¾è±¡ã¯ï¼Œæ¶ˆè²»è€…å¿ƒç†ã®è¦³ç‚¹ã‹ã‚‰ã¯ãƒ«ãƒãƒ»ã‚¸ãƒ©ãƒ¼ãƒ«ã®æ¬²æœ›ã®ä¸‰è§’å½¢ï¼Œç”£æ¥­çµ„ç¹”è«–ã®è¦³ç‚¹ã‹ã‚‰ã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¤–éƒ¨æ€§ã§ã‚ˆãèª¬æ˜ã•ã‚Œã‚‹ï¼\nAdditionally, this schema is often misused for stealth marketing, hype, information products, affiliates, etc.\nã¾ãŸï¼Œã“ã®ã‚¹ã‚­ãƒ¼ãƒã¯ï¼Œã—ã°ã—ã°ã‚¹ãƒ†ãƒï¼Œå½å®¢(ã‚µã‚¯ãƒ©)ï¼Œèª‡å¤§åºƒå‘Šï¼Œæƒ…å ±å•†æï¼Œã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆãªã©ã§æ‚ªç”¨ã•ã‚Œã‚‹ï¼\n","date":1591833600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591833600,"objectID":"9701ee598cf0a4fa23043c14d9af8bf3","permalink":"https://yumauchiumi.com/post/2020-06-11-recurrent-structure-in-capita/","publishdate":"2020-06-11T00:00:00Z","relpermalink":"/post/2020-06-11-recurrent-structure-in-capita/","section":"post","summary":"Self-discriptive system within time change. The simplest example of self-discriptive systems is the exponential increase. This is because the simplest \u0026ldquo;change\u0026rdquo; of a variable $x$ is a first derivative of","tags":["System","Capitalism"],"title":"Recursion of the Capital","type":"post"},{"authors":null,"categories":["Finance","Random"],"content":"\nä¸­å›½ã®TencentãŒæ—¥æœ¬ã®ACG(Anime, Comics, Game)ç”£æ¥­ã¸å¤§ããªé–¢å¿ƒã‚’å¯„ã›ã¦ã„ã‚‹ã‚‰ã—ã„ï¼ ã™ãªã‚ã¡ï¼Œã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã‚²ãƒ¼ãƒ ã®åˆ¶ä½œã¨ãƒ’ãƒƒãƒˆã‚·ãƒªãƒ¼ã‚ºã®ãƒ•ãƒ©ãƒ³ãƒãƒ£ã‚¤ã‚ºã«é–¢ã™ã‚‹æ—¥æœ¬ã®å°‚é–€çŸ¥è­˜ã‚’å¸åã—ãªãŒã‚‰ï¼Œ ã„ãã¤ã‹ã®ã‚¹ã‚¿ã‚¸ã‚ªã‚’è²·åã—ï¼Œæ½œåœ¨çš„ãªæŠ•è³‡ã«ã¤ã„ã¦äº¤æ¸‰ä¸­ã¨ã®ã“ã¨ï¼è©³ç´°ã¯ä¸‹è¨˜è¨˜äº‹ã‚’å‚ç…§ï¼\nTencent Targets Japan Anime, Manga to Jump-Start Global Growth (Bloomberg, June 9, 2020, 5:00 PM EDT)\n","date":1591747200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591747200,"objectID":"5a7dc29a1a65dddce9f0a2b4bf0e8bcd","permalink":"https://yumauchiumi.com/post/2020-06-10-tencent-acg/","publishdate":"2020-06-10T00:00:00Z","relpermalink":"/post/2020-06-10-tencent-acg/","section":"post","summary":"ä¸­å›½ã®TencentãŒæ—¥æœ¬ã®ACG(Anime, Comics, Game)ç”£æ¥­ã¸å¤§ããªé–¢å¿ƒã‚’å¯„ã›ã¦ã„ã‚‹ï¼","tags":["Tips"],"title":"Tencent's Strategy for the ACG and the Impact on Japan","type":"post"},{"authors":null,"categories":["Finance"],"content":"2020å¹´05æœˆç¾åœ¨ï¼Œã‚³ãƒ­ãƒŠã‚·ãƒ§ãƒƒã‚¯ã§ï¼Œæ™¯æ°—å¾Œé€€ãŒäºˆæ¸¬ã•ã‚Œã¦ã„ã‚‹ï¼ S\u0026amp;P 500 YTDã‚’ã¿ã‚‹ã¨ï¼Œ2018å¹´ãƒ»2019å¹´ã¨æ¯”è¼ƒã—ã¦ã‚‚ä½èª¿ã ï¼\nã‚¢ãƒ¡ãƒªã‚«æ ªã¯äºˆæƒ³ã«åã—ã¦é«˜ã„æ°´æº–ã‚’ç¶­æŒã—ã¦ã„ã‚‹ãŒï¼Œ6æœˆä»¥é™ã®çµ±è¨ˆçµæœã«ã‚ˆã£ã¦ã¯è½ã¡è¾¼ã‚€å¯èƒ½æ€§ã‚‚é«˜ã„ï¼ã“ã®ãƒã‚¹ãƒˆã§ã¯ï¼Œã‚¢ãƒ¡ãƒªã‚«ã®ãƒã‚¯ãƒ­çµŒæ¸ˆå²ã‚’æŒ¯ã‚Šè¿”ã‚Š,2021å¹´ä»¥é™ã®ã‚¢ãƒ¡ãƒªã‚«çµŒæ¸ˆã«ã¤ã„ã¦è€ƒãˆã¦ã¿ãŸã„ï¼\nFig: S\u0026amp;P 500 Index YTD Daily Performance Source: https://www.macrotrends.net/2490/sp-500-ytd-performance\nGDP USã®å®Ÿè³ªGDPå¹´é–“æ¨ç§»(1947 - 2020)ã‚’ã¿ã¦ã¿ã‚‹ï¼ã‚¤ãƒ³ãƒ•ãƒ¬ç‡ã‚’è£œæ­£ã™ã‚‹ã¨ï¼Œã»ã¼ç·šå½¢ã«å¢—åŠ ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ï¼\nFig: Real GDP, Billions of Chained 2012 Dollars, Annual Rate (1947 - 2020) Source: https://alfred.stlouisfed.org/series?seid=GDPC1\næ¬¡ã«ï¼Œæˆé•·ç‡(1æ¬¡å¾®åˆ†)ï¼USã®å››åŠæœŸã”ã¨ã®GDPæˆé•·ç‡(YOY)ã‚’ï¼Œæˆ¦å¾Œ(1947-2020)ã«é™å®šã—ã¦ã¿ã¦ã¿ã‚‹ï¼åŸºæœ¬çš„ã«ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã«ã¿ãˆã‚‹ãŒï¼Œèˆˆå‘³æ·±ã„ã®ã¯1985å¹´ã”ã‚ã‚’å¢ƒã«ï¼Œåˆ†æ•£ãŒå°ã•ããªã£ã¦ã„ã‚‹ã“ã¨ã ï¼\nFig: United States GDP Growth Rate, YOY (1947 - 2020) Source: https://tradingeconomics.com/united-states/gdp-growth\nFREDã®ã‚µã‚¤ãƒˆã«ã‚ã‚‹ï¼Œå¹´é–“ã®GDPæˆé•·ç‡(YOY)ã‚‚è¼‰ã›ã¦ãŠãï¼\nFig: Real GDP, Percent Change from Preceding Period (1930 - 2020) Source: https://fred.stlouisfed.org/graph/?g=oM2u\nEquity Market ç±³å›½æ ªå¼å¸‚å ´ã®æ­´å²ï¼ã¾ãšäº‹å®Ÿã¨ã—ã¦ï¼ŒUSã®æ ªå¼å¸‚å ´ã¯é•·æœŸçš„ã«ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰ã§ã‚ã‚‹ï¼ã¾ãŸï¼ŒS\u0026amp;P 500ã¨DJIAã¯é•·æœŸé–“ã§ã¿ã‚‹ã¨åŒã˜æŒ™å‹•ã‚’ç¤ºã™ï¼\nFig: S\u0026amp;P 500 Index (1928.01 - 2020.06) Source: https://www.macrotrends.net/2324/sp-500-historical-chart-data\nFig: Dow Jones Industrial Average (DJIA) (1915.02 - 2020.06) Source: https://www.macrotrends.net/2324/sp-500-historical-chart-data\nFig: NASDAQ Composite (1971.02 - 2020.06) Source: https://www.macrotrends.net/1320/nasdaq-historical-chart\nãã—ã¦ï¼Œç‹­ã„æ„å‘³ã§ã®åŠ¹ç‡çš„å¸‚å ´ä»®èª¬ã‚’æ”¯æŒã™ã‚‹ãªã‚‰ã°ï¼ŒçŸ­æœŸ(å¹´ã‚ã‚‹ã„ã¯æœˆå˜ä½)ã§ã®æ™¯æ°—å¾ªç’°ã¯å¹³æ»‘åŒ–ã•ã‚Œã‚‹ï¼ã¾ãŸäº‹å®Ÿã¨ã—ã¦ï¼Œã‚ã‚‰ã‚†ã‚‹çµŒæ¸ˆçµ±è¨ˆãŒã›ã„ãœã„100å¹´åˆ†ã—ã‹å­˜åœ¨ã—ãªã„ä»¥ä¸Šï¼Œæ™¯æ°—å¾ªç’°èª¬ã‚„åŠ¹ç‡çš„å¸‚å ´ä»®èª¬ã¯æ­£ã—ãæ¤œè¨¼ã§ããªã„ï¼\nS\u0026amp;P 500ã¯ï¼ŒNYSEã¨NSDAQã®ä¸Šå ´éŠ˜æŸ„ã‹ã‚‰(æµå‹•æ€§ã®é«˜ã„å¤§å‹æ ªã®)æ™‚ä¾¡ç·é¡ã‚’æŒ‡æ•°åŒ–ã—ãŸã‚‚ã®ãªã®ã§ï¼ŒDJIAã‚ˆã‚Šå®Ÿä½“çµŒæ¸ˆ(ä¼æ¥­éƒ¨é–€ã®åˆ©ç›Š)ã‚’åæ˜ ã—ã¦ã„ã‚‹(ã¨è¨€ã‚ã‚Œã¦ã„ã‚‹)ï¼\nã“ã“ã§ï¼ŒS\u0026amp;P 500ã®éå»10å¹´é–“(2010 - 2020)ã®æ¨ç§»ã‚’ã¿ã¦ã¿ã‚‹ï¼\nFig: S\u0026amp;P 500 Index, Daily Close (2010.06.15 - 2020.06.12) Source: https://fred.stlouisfed.org/graph/fredgraph.png?g=rtxI,\nFig: S\u0026amp;P 500 Index Change from Year Ago (2011.06.13 - 2020.06.12) Source: https://fred.stlouisfed.org/graph/fredgraph.png?g=rtyH\nS\u0026amp;P 500ã®æ—¥åˆ¥ã®å¤‰åŒ–é‡(YOY)ï¼ãƒã‚¤ãƒŠã‚¹ã¨ãªã£ãŸæœŸé–“ã¯2015/08~2016/07ï¼Œ2018/11~2019/05ï¼Œ2020/03~05ã®3å›ã§ã‚ã‚‹ï¼\nFig: S\u0026amp;P 500 Index Change, Daily Close (2010.06.15 - 2020.06.12) Source: https://fred.stlouisfed.org/graph/fredgraph.png?g=rtyj\nS\u0026amp;P 500ã®æ—¥åˆ¥ã®å¤‰åŒ–é‡(DOD)ï¼\nEconomic Policy æ ªä¾¡(cf. S\u0026amp;P 500)ã¯ï¼Œãƒã‚¯ãƒ­çµŒæ¸ˆ(GDP)ã«å¯¾ã™ã‚‹å…ˆè¡ŒæŒ‡æ¨™ã§ã‚ã‚‹ã¨åŒæ™‚ã«ï¼Œæ”¿åºœãŒçµŒæ¸ˆå¯¾ç­–ã‚’æ±ºå®šã™ã‚‹ãŸã‚ã®åŸå› ã¨ãªã‚‹ï¼ˆçµæœã§ã¯ãªã„ï¼‰ï¼S\u0026amp;P 500ã®æ­´å²ã¨ï¼Œãã®çµæœã¨ã—ã¦æ™‚ã®ç±³å›½æ”¿åºœãŒã©ã®ã‚ˆã†ãªçµŒæ¸ˆæ”¿ç­–ã‚’å®Ÿæ–½ã—ãŸã‹ï¼Œãã®æ”¿ç­–ã¯å…·ä½“çš„ã«ã©ã®ã‚ˆã†ãªãƒã‚¯ãƒ­çµŒæ¸ˆç†è«–ã«ã‚ˆã£ã¦è£ä»˜ã‘(ã‚¢ãƒ‰ãƒã‚¤ã‚¹)ã•ã‚ŒãŸã‹ï¼Œã‚’è€ƒãˆã¦ã¿ã‚‹ï¼\nS\u0026amp;P 500ã®å‰å¹´æ¯”å¤‰å‹•ç‡ï¼ˆ1929-2020/03ï¼‰ã‚’ã¿ã¦ã¿ã‚ˆã†ï¼\nFig: S\u0026amp;P 500 Historical Annual Returns (1928 - 2020) Source: https://www.macrotrends.net/2526/sp-500-historical-annual-returns\nS\u0026amp;P 500ã®çµ±è¨ˆãŒé–‹å§‹ã•ã‚Œã¦ã‹ã‚‰ï¼Œ1929-2019å¹´ã®ã¡ã‚‡ã†ã©100å¹´é–“ã§ï¼Œå¹´æ›ç®—ã§å‰å¹´æ¯”ãƒã‚¤ãƒŠã‚¹ã¨ãªã£ãŸå¹´ã¯22å›ã—ã‹ãªã„ï¼ãã—ã¦ï¼Œ2020å¹´ã¯23ç•ªç›®ã®å¹´ã«ãªã‚‹ã‹ã‚‚ã—ã‚Œãªã„ï¼å…¨22å›ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ã¿ã‚‹ï¼\nçµ±è¨ˆä¸Šã®ç•™æ„ç‚¹ï¼šS\u0026amp;P 500æŒ‡æ•°ã®ç™ºè¡Œå…ƒã§ã‚ã‚‹S\u0026amp;P Globalç¤¾ã®æ²¿é©:\n1941å¹´ï¼Œã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰çµ±è¨ˆç¤¾(Standard Statistics Bureau)ã¨ãƒ—ã‚¢ãƒ¼å‡ºç‰ˆç¤¾(H.V. and H.W. Poor Co.)ãŒåˆä½µã—ã¦S\u0026amp;Pç¤¾(Standard\u0026amp;Poor\u0026rsquo;s)ãŒèª•ç”Ÿï¼1957å¹´ï¼ŒS\u0026amp;P 500ãŒèª•ç”Ÿï¼1966å¹´ï¼Œãƒã‚°ãƒ­ã‚¦ãƒ’ãƒ«ç¤¾ãŒS\u0026amp;Pç¤¾ã‚’è²·åã—ï¼Œç¾åœ¨ã®é‹å–¶ä½“åˆ¶ (S\u0026amp;P Global Inc.; 1995å¹´ã¾ã§ã®æ—§å:The McGraw-Hill Companies)ã¨ãªã£ãŸï¼\nGreat Depression (1929-1936, 7 years) ã‚±ã‚¤ãƒ³ã‚ºï¼Œãƒ‹ãƒ¥ãƒ¼ãƒ‡ã‚£ãƒ¼ãƒ« æ”¿ç­– æœ‰åŠ¹éœ€è¦\n1929: (-11.91% YoY) ä¸–ç•Œææ…Œ 1930: (-28.48% YoY) ä¸–ç•Œææ…Œ 1931: (-47.07% YoY) ä¸–ç•Œææ…Œ 1932: (-15.15% YoY) ä¸–ç•Œææ…Œ 1934: (-05.94% YoY) ä¸–ç•Œææ…Œ World War â…¡ (1937-1945, 8 years) 1937: (-38.59% YoY) ç¬¬äºŒæ¬¡ä¸–ç•Œå¤§æˆ¦ 1939: (-05.45% YoY) ç¬¬äºŒæ¬¡ä¸–ç•Œå¤§æˆ¦ 1940: (-15.29% YoY) ç¬¬äºŒæ¬¡ä¸–ç•Œå¤§æˆ¦ 1941: (-17.86% YoY) ç¬¬äºŒæ¬¡ä¸–ç•Œå¤§æˆ¦ Post-war Prosperity (1945-1973, 28 years) 60så¾ŒåŠ-70så‰åŠã®ã‚¹ã‚¿ã‚°ãƒ•ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®çµæœã¨ã—ã¦ï¼Œ1972,73: ãƒ‹ã‚¯ã‚½ãƒ³ã‚·ãƒ§ãƒƒã‚¯(ãƒ–ãƒ¬ãƒˆãƒ³ãƒ»ã‚¦ãƒƒã‚ºå”å®šå´©å£Š)\n1946: (-11.87% YoY) æˆ¦å¾Œ 1953: (-06.62% YoY) 1957: (-14.31% YoY) 1960: (-02.97% YoY) 1962: (-11.81% YoY) 1966: (-13.09% YoY) 1969: (-11.36% YoY) Reaganomics (1974-1990, 16 years) æ–°è‡ªç”±ä¸»ç¾©ï¼Œã‚¨ãƒãƒ«ã‚®ãƒ¼è¦åˆ¶ã®ç·©å’Œï¼Œãƒ¬ãƒ¼ã‚¬ãƒãƒŸã‚¯ã‚¹(æ‰€å¾—æ¸›ç¨)ï¼Œãƒ—ãƒ©ã‚¶åˆæ„ï¼ŒåŒå­ã®èµ¤å­—(è²¡æ”¿èµ¤å­—ã¨è²¿æ˜“èµ¤å­—ã®æ‹¡å¤§)\n1973: (-17.37% YoY) ç¬¬ä¸€æ¬¡ã‚ªã‚¤ãƒ«ã‚·ãƒ§ãƒƒã‚¯ 1974: (-29.72% YoY) ç¬¬ä¸€æ¬¡ã‚ªã‚¤ãƒ«ã‚·ãƒ§ãƒƒã‚¯ 1977: (-11.50% YoY) 1981: (-09.73% YoY) ç¬¬äºŒæ¬¡ã‚ªã‚¤ãƒ«ã‚·ãƒ§ãƒƒã‚¯ New Economy (1990-2000, 10 years) æƒ…å ±é€šä¿¡æ¥­ã®ç‰½å¼•ï¼Œãƒ‰ãƒƒãƒˆã‚³ãƒ ãƒãƒ–ãƒ«ï¼Œ90sã®USGDPã¯69%å¢—ï¼ŒS\u0026amp;P 500ã¯3å€ã«ä¸Šæ˜‡\n1990: (-06.56% YoY) é€šè²¨å±æ©Ÿ 2000: (-10.14% YoY) ãƒ‰ãƒƒãƒˆã‚³ãƒ ãƒãƒ–ãƒ«å´©å£Š Financial Crisis (2001-2009, 8 years) 2001å¹´ã®åŒæ™‚å¤šç™ºãƒ†ãƒ­ï¼Œ2001-2007ã®ä½å®…ãƒãƒ–ãƒ«ã¨ã‚µãƒ–ãƒ—ãƒ©ã‚¤ãƒ ãƒ­ãƒ¼ãƒ³ã«ã‚ˆã‚‹é‡‘èå±æ©Ÿï¼ŒãƒŸãƒ³ã‚¹ã‚­ãƒ¼ã®é‡‘èä¸å®‰å®šä»®èª¬ï¼Œ\n2001: (-13.04% YoY) ãƒ‰ãƒƒãƒˆã‚³ãƒ ãƒãƒ–ãƒ«å´©å£Š 2002: (-23.37% YoY) ãƒ‰ãƒƒãƒˆã‚³ãƒ ãƒãƒ–ãƒ«å´©å£Š 2009: (-38.49% YoY) ãƒªãƒ¼ãƒãƒ³ã‚·ãƒ§ãƒƒã‚¯ Tech boom (2010-2019, 10years) è¥¿æµ·å²¸ãƒ†ãƒƒã‚¯æ ªã®ç‰½å¼•ï¼Œä¸­å›½çµŒæ¸ˆã®å°é ­\n2018: (-06.42% YoY) ä¸Šæµ·å±æ©Ÿ New Normal? (2020-????) 2020: (-3.34% YTD) ã‚³ãƒ­ãƒŠã‚·ãƒ§ãƒƒã‚¯ References \u0026amp; Source www.macrotrends.net - S\u0026amp;P 500, historical annual returns www.macrotrends.net - DJIA, 100 years historical chart us.spindices.com - S\u0026amp;P 500 us.spindices.com - Dow Jones Industrial Average tradingeconomics.com - United States GDP Growth Rate tradingeconomics.com - S\u0026amp;P 500 tradingeconomics.com - Dow Jones Industrial Average fred.stlouisfed.org - Real Gross Domestic Product fred.stlouisfed.org - S\u0026amp;P 500 Index fred.stlouisfed.org - Dow Jones Industrial Average Wikipedia - Economic history of the United States Wikipedia - List of recessions in the United States Wikipedia - List of economic expansions in the United States ","date":1590710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590710400,"objectID":"46be1c3180b33e0ee552ff09d82056d4","permalink":"https://yumauchiumi.com/post/2020-05-29-history_of_us_macro_economy/","publishdate":"2020-05-29T00:00:00Z","relpermalink":"/post/2020-05-29-history_of_us_macro_economy/","section":"post","summary":"ã‚¢ãƒ¡ãƒªã‚«ã®ãƒã‚¯ãƒ­çµŒæ¸ˆå²ã‚’æŒ¯ã‚Šè¿”ã‚Š,2021å¹´ä»¥é™ã®ã‚¢ãƒ¡ãƒªã‚«çµŒæ¸ˆã«ã¤ã„ã¦è€ƒãˆã¦ã¿ã‚‹ï¼","tags":["Macroeconomics","Market","Pricing"],"title":"United States Macroeconomic History","type":"post"},{"authors":null,"categories":["StatML"],"content":"ã‚·ã‚¹ãƒ†ãƒ ã¨å®šå¸¸çŠ¶æ…‹ å¤šãã®å‹•çš„ãƒ¢ãƒ‡ãƒ«ï¼ˆDynamic Model-Systemï¼‰ã¯ï¼Œå®šå¸¸çŠ¶æ…‹ï¼ˆtime-steady stateï¼‰ã«è‡³ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦è¨­è¨ˆã•ã‚Œã‚‹ï¼ã“ã“ã§ï¼Œå®šå¸¸çŠ¶æ…‹ã¨ã¯ï¼Œã€Œã‚ã‚‹å¤‰æ•°$X$ã«ä½œç”¨ã™ã‚‹ï¼Œä½•ã‚‰ã‹ã®æ™‚å¤‰é‡ï¼ˆparameter $\\theta_t$ï¼‰ã‚„é–¢æ•°$L_t(x; \\theta)$ï¼‰ãŒä¸€å®šå€¤ã«åæŸã™ã‚‹ã“ã¨ã€ã¨å®šç¾©ã—ã¦ãŠãï¼\nãã—ã¦ï¼Œç€ç›®ã—ã¦ã„ã‚‹å‹•çš„ãƒ¢ãƒ‡ãƒ«ãŒå®šå¸¸çŠ¶æ…‹ã«è‡³ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã¯ï¼Œã‚·ã‚¹ãƒ†ãƒ åŒå®šï¼ˆSystem identificationï¼‰ã¨å‘¼ã°ã‚Œï¼Œå®šå¸¸çŠ¶æ…‹ã‚’ç¤ºã—ãŸæ¦‚å¿µã¨ã—ã¦ï¼Œå‡è¡¡ï¼ˆequilibriumï¼‰ã‚„å¹³è¡¡ï¼ˆbalanceï¼‰ã¨å‘¼ã°ã‚Œã‚‹ç”¨èªãŒä½¿ã‚ã‚Œã‚‹ï¼\nåŒ–å­¦åå¿œã«ãŠã„ã¦ã¯ã€å¯é€†åå¿œã®ç”Ÿæˆç‰©ã®å¤‰åŒ–é‡ã¨å‡ºç™ºç‰©è³ªã®å¤‰åŒ–é‡ãŒåˆè‡´ã—ãŸçŠ¶æ…‹ã‚’æŒ‡ã™ã€‚åŒ–å­¦å¹³è¡¡ã‚’å‚ç…§ã€‚ åŠ›å­¦ã«ãŠã„ã¦ã¯ã€ç‰©ä½“ã«åŠ ã‚ã£ã¦ã„ã‚‹å…¨ã¦ã®åŠ›ã®åˆåŠ›ã¨åŠ›ã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã®å’ŒãŒã¨ã‚‚ã« 0 ã§ã‚ã‚‹çŠ¶æ…‹ã‚’å¹³è¡¡ã¨å‘¼ã¶ã€‚åŠ›å­¦çš„å¹³è¡¡ï¼ˆè‹±èª: Mechanical equilibriumï¼‰ã‚’å‚ç…§ã€‚ ç†±åŠ›å­¦ã«ãŠã„ã¦ã¯é€šå¸¸ã€ç†±å¹³è¡¡ã€åŠ›å­¦çš„å¹³è¡¡ã€åŒ–å­¦å¹³è¡¡ã®ä¸‰ã¤ã‚’åˆã‚ã›ã¦ã€ç†±åŠ›å­¦çš„å¹³è¡¡ã¨ã‚ˆã¶ã€‚ çµ±è¨ˆåŠ›å­¦ã«ãŠã„ã¦ã¯ã€ç³»ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼åˆ†å¸ƒãŒã€ãƒœãƒ«ãƒ„ãƒãƒ³åˆ†å¸ƒã«å¾“ã†ã“ã¨ã§ã‚ã‚‹ã€‚ç†±åŠ›å­¦çš„å¹³è¡¡ã‚’å‚ç…§ã€‚ ç‰©ç†åŒ–å­¦ã«ãŠã„ã¦ã¯ã€è¤‡æ•°ã®ç‰©è³ªç›¸ã‹ã‚‰æ§‹æˆã•ã‚Œã‚‹ç³»ã«ãŠã„ã¦ã€ç›¸é–“ã®ç‰©è³ªã®å‡ºå…¥ã‚ŠãŒåˆã„ç­‰ã—ã„çŠ¶æ…‹ã‚’æŒ‡ã™ã€‚ç›¸å¹³è¡¡ã‚’å‚ç…§ã€‚ é›»æ°—å·¥å­¦ã«ãŠã„ã¦ã¯ã€ä¿¡å·æºã¨è² è·ã®é–“ã®ã‚¤ãƒ³ãƒ”ãƒ¼ãƒ€ãƒ³ã‚¹ãŒåˆè‡´ã—ã¦ã„ã‚‹ã“ã¨ã‚’æŒ‡ã™ã€‚ã‚¤ãƒ³ãƒ”ãƒ¼ãƒ€ãƒ³ã‚¹å¹³è¡¡ã‚’å‚ç…§ã€‚ é›»æ°—å›è·¯ã«ãŠã„ã¦ã¯ã€ä¿¡å·å›è·¯ã®åŒæ–¹ãŒæ¥åœ°ç‚¹ã«æ¥ç¶šã•ã‚Œã¦ã„ãªã„ã“ã¨ã‚’æŒ‡ã™ã€‚å¹³è¡¡æ¥ç¶šã‚’å‚ç…§ã€‚ æƒ…å ±å·¥å­¦ã«ãŠã„ã¦ã¯ã€ãƒ‡ãƒ¼ã‚¿æœ¨æ§‹é€ ã®ä»»æ„ã®ç¯€ã«ãŠã„ã¦ãã®é…ä¸‹ã®ç¯€ç‚¹ã®æ•°ãŒç­‰ã—ã„çŠ¶æ…‹ã‚’æŒ‡ã™ã€‚ ç”Ÿæ…‹å­¦ã«ãŠã„ã¦ã¯ã€ç”Ÿç‰©ç¾¤é›†é–“ã®åˆ†å¸ƒã¨å€‹ä½“æ•°ã®å¤‰åŒ–ãŒç„¡ã„çŠ¶æ…‹ã‚’æŒ‡ã™ã€‚ ç”Ÿç†å­¦ã«ãŠã„ã¦ã¯ã€æ°´å¹³ã§ã‚ã‚‹ã“ã¨ã‚’èªçŸ¥ã™ã‚‹ã“ã¨ã‚’æŒ‡ã™ã€‚å¹³è¡¡æ„Ÿè¦šã‚’å‚ç…§ã€‚ çµŒæ¸ˆå­¦ã«ãŠã„ã¦ã¯ã€éœ€è¦ã¨ä¾›çµ¦ãŒé‡£ã‚Šåˆã£ã¦ä¾¡æ ¼ãŒä¸å‹•ã«ãªã‚‹ã“ã¨ãªã©ã‚’æŒ‡ã™ã€‚å‡è¡¡ã‚’å‚ç…§ã€‚ é€£ç«‹(å¾®åˆ†)æ–¹ç¨‹å¼ã§è¨˜è¿°ã§ãã‚‹ãŸã‚ï¼\nãªãœç·šå½¢ãƒ¢ãƒ‡ãƒ«ãŒæœ‰ç”¨ãªã®ã‹ï¼Ÿ ç­”ãˆã¯ã‚·ãƒ³ãƒ—ãƒ«ã§ï¼ŒTaylorå±•é–‹\n$$ \\begin{align} \\frac{d x_1}{d t} \u0026amp;= f_1(x_1, \\dots, x_n; \\theta_1) \\\\ \\frac{d x_2}{d t} \u0026amp;= f_2(x_1, \\dots, x_n; \\theta_1) \\\\ \u0026amp;\\vdots \\\\ \\frac{d x_n}{d t} \u0026amp;= f_n(x_1, \\dots, x_n; \\theta_1) \\end{align} $$\nã“ã®ãƒ¢ãƒ‡ãƒ«ãŒï¼Œå®šå¸¸çŠ¶æ…‹ã«ã„ã‚‹å ´åˆï¼Œ\n$$ \\begin{align} f_1 = f_2 = \\cdots = f_n = 0 \\end{align} $$\nãŒæˆã‚Šç«‹ã¤ã‹ã‚‰ï¼Œ$n$å€‹ã®å¤‰æ•°${\\bf x} = (x_1, x_2, \\dots, x_n)$ã«å¯¾ã—ã¦ï¼Œ$n$å€‹ã®æ–¹ç¨‹å¼ãŒå¾—ã‚‰ã‚Œã‚‹ï¼ã“ã®è§£ãŒã‚·ã‚¹ãƒ†ãƒ ã®å®šå¸¸åŒ–ã¨ãªã‚‹ï¼ã“ã‚Œã‚’${\\bf x}^* = (x_1^, x_2^, \\dots, x_n^)$ã¨ãŠãã¨ï¼Œ$f_1, f_2, \\dots, f_n$ã«å¯¾ã—ã¦ï¼Œç‚¹${\\bf x}^$ã®è¿‘å‚ã§Taylorå±•é–‹ãŒå¯èƒ½ã«ãªã‚‹ï¼\n$$ \\begin{align} \\frac{d x_1}{d t} = f_1(x_1, \\dots, x_n; \\theta_1) =\u0026amp; a_{11}(x_1 - x_1^) + a_{12} {(x_2 - x_2^)} + \\cdots a_{1n} {(x_n - x_n^)} + \\\\ \u0026amp; a_{111}{(x_1 - x_1^)}^2 + a_{112}(x_1 - x_1^)(x_2 - x_2^) + \\cdots + a_{11n}{(x_1 - x_1^)}^2 + \\\\ \u0026amp; ~~ \\vdots \\\\ \u0026amp; a_{11\\cdots1}{(x_1 - x_1^)}^n + \\cdots \\\\ \\frac{d x_2}{d t} = f_2(x_1, \\dots, x_n; \\theta_1) =\u0026amp; a_{21}(x_1 - x_1^) + a_{22} {(x_2 - x_2^)} + \\cdots a_{2n} {(x_n - x_n^)} + \\\\ \u0026amp; a_{211}{(x_1 - x_1^)}^2 + a_{212}(x_1 - x_1^)(x_2 - x_2^) + \\cdots + a_{21n}{(x_1 - x_1^)}^2 + \\\\ \u0026amp; ~~ \\vdots \\\\ \u0026amp; a_{21\\cdots1}{(x_1 - x_1^)}^n + \\cdots \\\\ \\\\ \u0026amp; ~~ \\vdots \\\\ \\\\\\ \\frac{d x_n}{d t} = f_n(x_1, \\dots, x_n; \\theta_1) =\u0026amp; a_{n1}(x_1 - x_1^) + a_{n2} {(x_2 - x_2^)} + \\cdots a_{nn} {(x_n - x_n^)} + \\\\ \u0026amp; a_{n11}{(x_1 - x_1^)}^2 + a_{n12}(x_1 - x_1^)(x_2 - x_2^) + \\cdots + a_{n1n}{(x_1 - x_1^)}^2 + \\\\ \u0026amp; ~~ \\vdots \\\\ \u0026amp; a_{n1\\cdots1}{(x_1 - x_1^)}^n + \\cdots \\\\ \\end{align} $$\nã¤ã¾ã‚Šï¼Œä»»æ„ã®å¾®åˆ†å¯èƒ½é–¢æ•°$f_1, f_2, \\dots, f_n$ã«ã‚ˆã£ã¦è¡¨ç¾ã•ã‚ŒãŸãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚’ã‚‚ã¤å‹•çš„ãƒ¢ãƒ‡ãƒ«ã¯ï¼Œï¼ˆå®šå¸¸è§£ã®è¿‘å‚ã§ã¯ï¼‰ä»»æ„ã®næ¬¡å¤šé …å¼ã«ã‚ˆã£ã¦è¿‘ä¼¼ã§ãã‚‹ï¼ã“ã‚Œã«ã‚ˆã‚Šï¼Œç·šå½¢ã‚·ã‚¹ãƒ†ãƒ ã®å¦¥å½“æ€§ãŒä¿è¨¼ã•ã‚Œã‚‹ï¼ä¸€èˆ¬è§£ã¯ï¼Œ\n$$ \\begin{align} x_1 =\u0026amp; ~ x_1^* + C_{11}e^{\\lambda_1 t} + C_{12}e^{\\lambda_2 t} + \\cdots C_{1n}e^{\\lambda_n t} + \\\\ \u0026amp; ~~~~~~~~~~ C_{111}e^{2\\lambda_1 t} + C_{112}e^{2\\lambda_2 t} + \\cdots + C_{11n}e^{2\\lambda_n t} + \\\\ \u0026amp; ~~~~~~~~~~ ~~ \\vdots \\\\ \u0026amp; ~~~~~~~~~~ C_{11\\cdots1}e^{n\\lambda_1 t} + C_{11\\cdots2}e^{n\\lambda_2 t} + \\cdots + C_{11\\cdots n}e^{n\\lambda_n t} \\\\ x_2 =\u0026amp; ~ x_2^* + C_{21}e^{\\lambda_1 t} + C_{22}e^{\\lambda_2 t} + \\cdots C_{2n}e^{\\lambda_n t} + \\\\ \u0026amp; ~~~~~~~~~~ C_{211}e^{2\\lambda_1 t} + C_{212}e^{2\\lambda_2 t} + \\cdots + C_{11n}e^{2\\lambda_n t} + \\\\ \u0026amp; ~~~~~~~~~~ ~~ \\vdots \\\\ \u0026amp; ~~~~~~~~~~ C_{21\\cdots1}e^{n\\lambda_1 t} + C_{21\\cdots2}e^{n\\lambda_2 t} + \\cdots + C_{21\\cdots n}e^{n\\lambda_n t} \\\\ \\\\\\ \u0026amp; ~~~~~~~~~~ ~~ \\vdots \\ \\ x_n =\u0026amp; ~ x_n^* + C_{n1}e^{\\lambda_1 t} + C_{n2}e^{\\lambda_2 t} + \\cdots C_{nn}e^{\\lambda_n t} + \\\\ \u0026amp; ~~~~~~~~~~ C_{n11}e^{2\\lambda_1 t} + C_{n12}e^{2\\lambda_2 t} + \\cdots + C_{n1n}e^{2\\lambda_n t} + \\\\ \u0026amp; ~~~~~~~~~~ ~~ \\vdots \\\\ \u0026amp; ~~~~~~~~~~ C_{n1\\cdots1}e^{n\\lambda_1 t} + C_{n1\\cdots2}e^{n\\lambda_2 t} + \\cdots + C_{n1\\cdots n}e^{n\\lambda_n t} \\\\ \\end{align} $$\nã¨ãªã‚‹ï¼\nãƒ¢ãƒ‡ãƒ«ã¨ã¯ä½•ã‹ï¼Ÿ ã¤ã¾ã‚Šï¼Œå¤šãã®åˆ†é‡ã«ãŠã„ã¦æ•°ç†ãƒ¢ãƒ‡ãƒ«ã¨ã‹è¨ˆé‡ãƒ¢ãƒ‡ãƒ«ã¨ã‹å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã¯ï¼Œä»¥ä¸‹ã®æ‰‹ç¶šãã‚’å¿…è¦ã¨ã™ã‚‹ï¼\nå®šå¸¸çŠ¶æ…‹ã«è‡³ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã™ã‚‹å‹•çš„ãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©ã™ã‚‹ ãƒ¢ãƒ‡ãƒ«ã®çŠ¶æ…‹å¤‰åŒ–ã‚’æœ€é©åŒ–å•é¡Œ(éç¨‹)ã¨ã—ã¦å®šå¼åŒ–ã™ã‚‹ï¼ å®šå¸¸çŠ¶æ…‹ã¸ã®åæŸãŒä¿è¨¼ã•ã‚ŒãŸæœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è€ƒãˆã‚‹ ç§»å‹•å¹³å‡ ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ Adamã«ç½®ã‘ã‚‹ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ  å¼·åŒ–å­¦ç¿’ã®å ±é…¬ ã‚²ãƒ¼ãƒ ç†è«–ã«ãŠã‘ã‚‹Ficticious Play æ ªä¾¡ã«ãŠã‘ã‚‹ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«åˆ†æç§»å‹•å¹³å‡ï¼ˆARMAï¼‰ (é‡‘èå·¥å­¦)ãƒãƒªãƒ¥ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã«ãŠã‘ã‚‹DCFæ³• ç§»å‹•å¹³å‡ã¨ã¯ä½•ã‹ï¼Ÿ\n$$ \\begin{align} m_t \u0026amp;= \\gamma \\cdot m_t âˆ’ 1+Î· \\cdot \\frac{\\partial L(w_t)}{\\partial w} \\\\ w_{t+1} \u0026amp;= w_t - m_t \\end{align} $$\n$$ m_t = g_t + \\gamma \\cdot g_{t-1} + \\gamma^2 \\cdot g_{t-2} \\cdots + \\gamma^t \\cdot g_{0} $$\n","date":1587686400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587686400,"objectID":"21fbd7d1785d6723151bf2b3dc3e018b","permalink":"https://yumauchiumi.com/post/2020-04-24-time-steady-state-on-system/","publishdate":"2020-04-24T00:00:00Z","relpermalink":"/post/2020-04-24-time-steady-state-on-system/","section":"post","summary":"å‹•çš„ã‚·ã‚¹ãƒ†ãƒ ã®å®šå¸¸çŠ¶æ…‹ï¼Œç·šå½¢æ€§ï¼Œç§»å‹•å¹³å‡ã«ã¤ã„ã¦ï¼","tags":["Tips","System"],"title":"Time-steady States on Systems","type":"post"},{"authors":null,"categories":["StatML"],"content":" State Space Model (SSM)\nState Space Model(SSM) is widely used in the field requiring the sequential estimation or online learning. This model is effective if you consider a system having two different variables; one completely represents the actual state but cannot be observed and the other partially represents the actual state but can be observed. Here, I call the former $x$ (state variable) and the latter $y$ (observation variable).\nIn SSM, we intruduce the following equations $F, H$ (or $f, h$) and identify them by observed data sample $[y_1, \\dots, y_t]$.\nEquation of each state $x_t$ :\n$$ \\begin{aligned} x_{t+1} \u0026amp;= F(x_t) ~~ (\\text{Deterministic process}) \\\\ x_{t+1} \u0026amp;\\sim f(\\cdot\\vert x_t) ~~ (\\text{Stochastic process}) \\end{aligned} $$\nEquation of each observation $y_t$ :\n$$ \\begin{aligned} y_t \u0026amp;= H(x_t) ~~ (\\text{Deterministic process}) \\\\ y_t \u0026amp;\\sim h(\\cdot \\vert x_t) ~~ (\\text{Stochastic process}) \\end{aligned} $$\nPerticle filter\nFor each $i$ in $[1 \\dots M]$\n(Prediction)\nDerive prediction distribution $f(x_t \\vert \\cdot)$ depends on particles $\\hat{x}_{t-1}$.\nSample $x^{i}_{t \\vert t-1} ~~~ (i = 1, \\dots, M)$ following $f(x_t \\vert \\cdot)$.\n$$ \\begin{align} x^{i}_{t \\vert t-1} \\sim f(x_t \\vert \\hat{x}_{t-1}) \\end{align} $$\n(Likelihood)\nDerive the likelihood of $x^i_{t \\vert t-1}$ from given sample data $y_t$ based on $h(\\cdot)$\n$$ w^i_t \\sim h(y_t \\vert x^i_{t \\vert t-1}) $$\n(Resampling)\nResampe $\\hat{x}^i_{t \\vert t-1}$ based on the likelihood $w^i_t ~~~ (i=1,\\dots,M)$ .\nDerive the filter distribution $p(x_t \\vert y_{1:t})$ for any $x_t$: $$ \\begin{aligned} p(x_t \\vert y_{1:t}) \u0026amp;\\approx \\frac{1}{M} \\sum_{i=1}^{M} \\delta(x_t - \\hat{x}^i_{t \\vert t-1}) \\\\ \u0026amp;\\approx \\sum_{i=1}^{M} \\frac{}{\\sum_{i=1}^{M} } \\delta(x_t - \\hat{x}^i_{t \\vert t-1}) \\end{aligned} $$\n","date":1584489600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584489600,"objectID":"f552b8fa81a0587f981381f6b2396798","permalink":"https://yumauchiumi.com/post/2020-03-18-particle_filter/","publishdate":"2020-03-18T00:00:00Z","relpermalink":"/post/2020-03-18-particle_filter/","section":"post","summary":"State Space Model (SSM)\nState Space Model(SSM) is widely used in the field requiring the sequential estimation or online learning. This model is effective if you consider a system having two different variables; one completely represents the actual state but cannot be observed and the other partially represents the actual state but can be observed.","tags":["Bayes","System"],"title":"State Space Model \u0026 Particle Filter","type":"post"},{"authors":null,"categories":["StatML"],"content":"å¹³å‡å ´è¿‘ä¼¼ã¨è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ ã‚ã‚‹å¤‰æ•°$X$ã®ã¨ã‚Šã†ã‚‹ã™ã¹ã¦ã®çŠ¶æ…‹(å®Ÿç¾å€¤)$x$ã«å¯¾ã—ã¦ï¼Œä½•ã‚‰ã‹ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°$\\phi(x)$ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ã™ã‚‹ï¼ã“ã®ã¨ãï¼Œå¤‰æ•°$X$ã®Gibbsåˆ†å¸ƒï¼ˆBoltzmannåˆ†å¸ƒï¼‰:\n$$ \\begin{aligned} p(x) \u0026amp;= \\frac{\\exp (- \\beta \\phi(x))}{\\int_X \\exp (- \\beta \\phi(x))} = \\frac{\\exp (- \\beta \\phi(x))}{Z^{\\phi}(\\beta)} \\end{aligned} $$\nã‚’è€ƒãˆã‚‹ï¼ã“ã®ã¨ãï¼ŒGibbsåˆ†å¸ƒ$p(x)$ã¨ä»»æ„ã®è¿‘ä¼¼åˆ†å¸ƒ$q(x)$ã¨ã®KL-divergence:\n$$ \\begin{aligned} D_{KL}(q \\vert\\vert p) := \\int_{X} q(x) \\log \\frac{q(x)}{p(x)} \\end{aligned} $$\nã¯ä»¥ä¸‹ã®ã‚ˆã†ã«åˆ†è§£ã§ãã‚‹ï¼\n$$ \\begin{aligned} D_{KL}(q \\vert\\vert p) \u0026amp;= \\beta \\int_X q(x)\\phi(x) - \\left\\{ - \\int_X q(x)\\log q(x) \\right\\} + \\log \\int_X \\exp(-\\beta \\phi(x)) \\\\ \u0026amp;= \\beta~ \\mathbb{E}_{x \\sim q}[\\phi(x)] - H_q(X) + \\log Z^{\\phi}(\\beta) \\\\ \u0026amp;= \\beta~ (\\text{Internal energy}) - (\\text{Entropy}) + (\\text{Const.}) \\\\ \\end{aligned} $$\nã„ã¾ï¼Œè¿‘ä¼¼åˆ†å¸ƒ$q(x)$ã«å¯¾ã™ã‚‹æ±é–¢æ•°ã¨ã—ã¦ï¼Œè‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼:\n$$ F^{\\phi}(q) := \\mathbb{E}_{x \\sim q}[\\phi(x)] - \\frac{1}{\\beta}H_q(X) ~~~ (\\text{Free energy}) $$\nã‚’å®šç¾©ã™ã‚Œã°ï¼Œ\n$$ D_{KL}(q \\vert\\vert p) = \\beta~ F^{\\phi}(q) + \\log Z^{\\phi}(\\beta) $$\nã¨ãªã‚‹ã‹ã‚‰ï¼Œ$q(x)$ã«ã‚ˆã‚‹$p(x)$ã®è¿‘ä¼¼å•é¡Œã¯æ¬¡å¼ã§è¡¨ç¾ã§ãã‚‹ï¼\n$$ \\begin{aligned} \\underset{q}{\\rm min} ~ D_{KL}(p \\vert\\vert q) \u0026amp;= \\underset{q}{\\rm min} ~ F^{\\phi}(q) \\end{aligned} $$\nã¾ãŸï¼è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼$F^{\\phi}(q)$ã®æœ€å°å€¤ã¯ï¼Œ\n$$ {F^{\\phi}}^{*}(q) = - \\frac{1}{\\beta} \\log \\int_X \\exp (-\\beta \\phi(x)) = - \\frac{1}{\\beta} \\log Z^{\\phi}(\\beta) $$\nã¨ãªã‚‹ï¼ã™ãªã‚ã¡ï¼Œ\n$$ \\begin{aligned} {F^{\\phi}}(q) = - \\frac{1}{\\beta} \\log Z^{\\phi}(\\beta) ~~ \\Leftrightarrow ~~ D_{KL}(p \\vert\\vert q) = 0 ~~ \\Leftrightarrow ~~ p(\\cdot) \\equiv\tq(\\cdot) \\end{aligned} $$\nã¨ãªã‚‹ï¼\nç†±åŠ›å­¦(çµ±è¨ˆåŠ›å­¦)ã¨ã®é–¢ä¿‚ æ¸©åº¦$T$ï¼Œå†…éƒ¨ã‚¨ãƒãƒ«ã‚®ãƒ¼$U$ï¼Œã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼$S$ã«å¯¾ã—ã¦ï¼ŒHelmholtzã®è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼$F$ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«å®šç¾©ã•ã‚Œã‚‹ï¼\n$$ F = U - TS $$\n$F^{\\phi}(q)$ã®å®šç¾©å¼ã§ï¼Œ$F = F^{\\phi}(q)$ï¼Œ$U = \\mathbb{E}_{x \\sim q}[\\phi(x)]$ï¼Œ$S = H_q(X)$ã¨ãŠã‘ã°ï¼Œ\n$$ \\begin{aligned} \\beta F \u0026amp;= \\beta U - S \\\\ F \u0026amp;= U - \\frac{1}{\\beta} S \\end{aligned} $$\nã¨ãªã‚‹ã‹ã‚‰ï¼Œæ±é–¢æ•°$F^{\\phi}(q)$ã¯ï¼Œç†±åŠ›å­¦ã«ãŠã‘ã‚‹Helmholtzã®è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼$F$ã¨é¡ä¼¼ã—ãŸå½¢å¼ã‚’æŒã£ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ï¼ãªãŠï¼ŒBayesç†è«–ã«ãŠã„ã¦å®šæ•°$\\beta$ã¯ã€Œé€†æ¸©åº¦ã€ã¨å‘¼ã°ã‚Œã‚‹ãŒï¼Œã“ã‚Œã¯æ¸©åº¦$T$ã«ç”±æ¥ã™ã‚‹ï¼\nBayesè„³ã‚„FEPã¨ã®é–¢ä¿‚ ç¥çµŒç§‘å­¦ã®åˆ†é‡ã§K.Fristonã«ã‚ˆã£ã¦æå”±ã•ã‚ŒãŸè‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼åŸç†(Free energy principle, FEP)ã¯ï¼Œä¸Šã«ã‚ã‚‹æ±é–¢æ•°$F^{\\phi}(q)$ã‚’å¤‰åˆ†æ¨è«–ã‚’çµ„ã¿åˆã‚ã›ãŸã‚‚ã®ã§ã‚ã‚‹ï¼ˆã¨è§£é‡ˆã§ãã‚‹ï¼‰ï¼ã“ã“ã§ã¯ï¼ŒELBOã¨ã®é–¢ä¿‚ã«ã®ã¿è§¦ã‚Œã¦ãŠãï¼\nELBOã®å®šç¾©å¼:\n$$ \\begin{align} (\\text{Evidence}) \u0026amp;= \\log p(y) \\\\ \u0026amp;\\geq \\mathbb{E}_{\\theta \\sim q}\\left[ \\log p(y, \\theta) \\right] - \\mathbb{E}_{\\theta \\sim q} \\left[ \\log q(\\theta) \\right] \\\\ \u0026amp;= \\mathcal{L}_{ELBO}(q) \\\\ \u0026amp;= (\\text{Evidence Lower Bound}) \\end{align} $$\nã¨Fristonã®Cellè«–æ–‡(2009)ã«ã‚ã‚‹è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®å®šç¾©å¼\n$$ F(y) = - \\mathbb{E}_{\\theta \\sim q}[\\log p(y,\\theta)] + \\mathbb{E}_{\\theta \\sim q}[\\log q(\\theta)] $$\nã‚’æ¯”ã¹ã‚‹ã¨ï¼Œä»¥ä¸‹ã®é–¢ä¿‚ãŒå¾—ã‚‰ã‚Œã‚‹ï¼\n$$ \\begin{aligned} (\\text{Surprise}) \u0026amp;= - \\log p(y) \\\\ \u0026amp;\\leq - \\mathbb{E}_{\\theta \\sim q}[\\log p(y,\\theta)] + \\mathbb{E}_{\\theta \\sim q}[\\log q(\\theta)] \\\\ \u0026amp;= -\\mathcal{L}_{ELBO}(q) \\\\ \u0026amp;= F(y) \\\\ \u0026amp;= (\\text{Free energy}) \\end{aligned} $$\nã¤ã¾ã‚Šï¼ŒFristonã®è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼$F(y)$ã¯ã€Œè„³ã®å¤–éƒ¨ç’°å¢ƒ$Y$ã«å¯¾ã™ã‚‹è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿${\\{y_t\\}}_{t=1}^{n}$ã®å¯¾æ•°å°¤åº¦ä¸‹é™(ELBO)ã«$-1$ã‚’ã‹ã‘ãŸã‚‚ã®ã€ã§ã‚ã‚‹ï¼ãªãŠï¼ŒBayesæ¨è«–ã§ã¯,å¯¾æ•°å°¤åº¦$\\log p(y)$ã‚’ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹(Evidence)ã¨ã„ã„ï¼Œæƒ…å ±ç†è«–ã§ã¯è² ã®å¯¾æ•°å°¤åº¦$-\\log p(y)$ã‚’ã‚µãƒ—ãƒ©ã‚¤ã‚º(Surprise)ã¨ã„ã†ï¼\nFristonã®Cellè«–æ–‡(2009)ã«ã‚ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¡Œå‹•$\\alpha$ã‚„è„³ã®å†…éƒ¨çŠ¶æ…‹$\\mu$ã®æ›´æ–°å¼:\n$$ \\begin{align} \\alpha^{*} \u0026amp;= \\underset{\\alpha}{\\rm argmin} ~ F(y) \\\\ \\mu^{*} \u0026amp;= \\underset{\\mu}{\\rm argmin} ~ F(y) \\end{align} $$\nã«ãŠã‘ã‚‹$F(y)$ã®æœ€å°åŒ–ã¯ï¼Œã€Œè„³ã®å¤–éƒ¨ç’°å¢ƒ$Y$ã«å¯¾ã™ã‚‹è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿${\\{y_t\\}}_{t=1}^{n}$ã®å¯¾æ•°å°¤åº¦(Evidence)ã€ã‚’æœ€å¤§åŒ–ã™ã‚‹éç¨‹ã‚’è¡¨ã—ã¦ã„ã‚‹ï¼ELBOã¨FEPã®é–¢ä¿‚ã‚’ã¾ã¨ã‚ã‚‹ã¨ä»¥ä¸‹ã®è¡¨ã®ã‚ˆã†ã«ãªã‚‹ï¼\nåŸç† Jensenã®ä¸ç­‰å¼ Bayesæ¨è«– ELBO Evidence: $ \\log p(y)$ ã®æœ€å¤§åŒ– $\\text{Evidence} \\geq \\mathcal{L}_{ELBO}$ ä¸‹é™$\\mathcal{L}_{ELBO}$ã‚’æœ€å¤§åŒ– FEP Surprise: $- \\log p(y)$ ã®æœ€å°åŒ– $\\text{Surprise} \\leq F$ ä¸Šé™$F$ã‚’æœ€å°åŒ– åŸ·ç­†æ™‚ã®å€‹äººçš„ãªç†è§£ã¨ã—ã¦ã¯ï¼ŒFEPã«ãŠã‘ã‚‹å„å¤‰æ•°$\\theta, \\mu, y, \\alpha$ã®æ›´æ–°è¦å‰‡ã¯ï¼Œã€Œè¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ãŸæœ€å°¤æ¨å®šã€ãã®ã‚‚ã®ã ã¨æ€ã£ã¦ã„ã‚‹ï¼è«–æ–‡ã§æå”±ã•ã‚Œã¦ã„ã‚‹è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼$F(y)$æœ€å°åŒ–ã¯ï¼ŒVariational Bayesã«ãŠã‘ã‚‹ELBOæœ€å¤§åŒ–ã¨åŒã˜ã§ã‚ã‚‹ã‹ã‚‰ï¼Œã‚€ã—ã‚4ã¤ã®å¤‰æ•°é–“ã®ãƒ«ãƒ¼ãƒ—æ§‹é€ ï¼ˆã‚°ãƒ©ãƒ•è¡¨ç¾ï¼‰ã®æ–¹ãŒé‡è¦ãªã®ã ã‚ã†ï¼\nFristonã®Natureè«–æ–‡(2010)ã§ã¯ï¼Œè‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼$F(y)$ã®å®šç¾©ãŒã‚ˆã‚Šè¤‡é›‘åŒ–ã—ã¦ãŠã‚Šï¼Œã‚ˆãç†è§£ã—ã¦ã„ãªã„ï¼\n","date":1583798400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583798400,"objectID":"44558e929078445c7ddf4f8e3db92205","permalink":"https://yumauchiumi.com/post/2020-03-10-free_energy_on_bayes_inference/","publishdate":"2020-03-10T00:00:00Z","relpermalink":"/post/2020-03-10-free_energy_on_bayes_inference/","section":"post","summary":"Fristonã®è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼åŸç†ã¨ELBOã®ç­‰ä¾¡æ€§ã«ã¤ã„ã¦","tags":["Bayes"],"title":"Free energy and Bayes inference","type":"post"},{"authors":null,"categories":["Tips"],"content":" In Git, commit messages are very imoportant to avoid confused commit log in your branch. But in some small projects that you develope alone, thinking about every commit message might be dull.\nI usualy use the following script to send local data to the remote repository. Please try it.\n#!/bin/bash echo -e \u0026quot;\\033[0;32mDeploying updates to GitHub...\\033[0m\u0026quot; # Go To .git root directory cd ~/workspace/{project_name} # Add all changes to git. git add . # Commit changes. msg=\u0026quot;update repo `date`\u0026quot; if [ $# -eq 1 ] then msg=\u0026quot;$1\u0026quot; fi git commit -m \u0026quot;$msg\u0026quot; # Push source and build repos. git push origin master ","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"813e3de4b4d1b414e68a4ba3b298a8c2","permalink":"https://yumauchiumi.com/post/2020-03-01-bash_script_for_git_push/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/post/2020-03-01-bash_script_for_git_push/","section":"post","summary":"In Git, commit messages are very imoportant to avoid confused commit log in your branch. But in some small projects that you develope alone, thinking about every commit message might be dull.","tags":["Git","Shell"],"title":"Bash script for git push","type":"post"},{"authors":null,"categories":["StatML"],"content":" Evidence Lower Bound (ELBO) is widely used in variational inference. Recently, according to the massive success of DeepLearning and related models, variational inference (and its technic) gains exposure in the filed of representation learning. For instance, stochastic generative models such as VAE and GAN are famous for their variational aspects.\nELBO Evidence Lower Bound (ELBO) is a lower bound of Log likelihood of $X$ (Evidence) in the model. The below inequality holds based on Cauchy-Schwartz inequality because of the convexity of log function.\n$$ \\begin{aligned} (\\text{Evidence}) \u0026amp;= \\log p(x) \\\\ \u0026amp;= \\log \\int_{Z} p(x,z) \\\\ \u0026amp;= \\log \\int_{Z} p(x,z) \\frac{q(z)}{q(z)} \\\\ \u0026amp;= \\log \\int_{Z} q(z) \\frac{p(x,z)}{q(z)} \\\\ \u0026amp;= \\log \\mathbb{E}{z \\sim q} \\left[ \\frac{p(x,z)}{q(z)} \\right] \\\\ \u0026amp;\\geq \\mathbb{E}{z \\sim q} \\left[ \\log \\frac{p(x,z)}{q(z)} \\right] \\\\ \u0026amp;= \\mathbb{E}_{z \\sim q} \\left[ \\log p(x,z) \\right] + H_q(Z) \\\\ \u0026amp;= ELBO(q) ~~~ (\\text{Evidence Lower Bound, ELBO}) \\end{aligned} $$\nSo that, we can obtain the optimization formula below.\n$$ \\begin{aligned} \\underset{\\theta}{\\rm max} ~ \\log p_{\\theta}(x) \u0026amp;= \\underset{q}{\\rm max} ~ ELBO(q) \\end{aligned} $$\nKL-divergence and ELBO $$ \\begin{aligned} D_{KL}( q(z) \\vert\\vert p(z \\vert x) ) \u0026amp;= \\int_{Z} q(z) \\frac{q(z)}{p(z \\vert x)} \\\\ \u0026amp;= - H_q(Z) - \\mathbb{E}_{z \\sim q} \\left[ \\log p(z|x) \\right] \\end{aligned} $$\nELBO is considered as the difference between Log likelihood $\\log p(x)$ and KL-divergence $D_{KL}( q(z) \\vert\\vert p(z \\vert x) )$ as below.\n$$ \\begin{align} ELBO \u0026amp;= \\mathbb{E}_{z \\sim q} \\left[ \\log p(x,z) \\right] + H_q(Z) \\\\ \u0026amp;= \\mathbb{E}_{z \\sim q} \\left[ \\log p(x) + \\log p(z|x) \\right] + H_q(Z) \\\\ \u0026amp;= \\mathbb{E}_{z \\sim q} \\left[ \\log p(x) \\right] + \\mathbb{E}_{z \\sim q} \\left[ \\log p(z|x) \\right] + H_q(Z) \\\\ \u0026amp;= \\log p(x) + H_q(Z) + \\mathbb{E}_{z \\sim q} \\left[ \\log p(z \\vert x) \\right] \\\\ \u0026amp;= \\log p(x) - D_{KL}( q(z) \\vert\\vert p(z \\vert x) ) \\end{align} $$\nSo that, we can obtain the below relation.\n$$ \\begin{align} \\underset{\\theta}{\\rm max} ~ \\log p_{\\theta}(x) \u0026amp;= \\underset{q}{\\rm max} ~ ELBO(q) \\\\ \u0026amp;= \\underset{q}{\\rm min} ~ D_{KL}( q(z) \\vert\\vert p(z|x) ) \\end{align} $$ \\\n","date":1582502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582502400,"objectID":"4c6e4097764feef99ecf700b24345c9b","permalink":"https://yumauchiumi.com/post/2020-02-24-deriving-elbo/","publishdate":"2020-02-24T00:00:00Z","relpermalink":"/post/2020-02-24-deriving-elbo/","section":"post","summary":"Evidence Lower Bound (ELBO) is widely used in variational inference. Recently, according to the massive success of DeepLearning and related models, variational inference (and its technic) gains exposure in the filed of representation learning.","tags":["Bayes"],"title":"Deriving ELBO","type":"post"},{"authors":null,"categories":["StatML"],"content":"In this post, I introduce you the Counterfactual Regret Minimization (CFR Algorithm). It is mainly used for the algorithm to figure out the optimal strategy of a extensive-form game with incomplete information such as Poker and Mahjong.\nExtensive-form Game Set, variables $N: $ set of players $i \\in N$: player $A :$ set of actions $a \\in A: $ action $H: $set of sequences $h \\in H: $ sequences (= possible history of actions, $h = (a_1, \\dots, a_t$) $Z \\subseteq H: $ set of terminal histories. $Z = {z \\in H \\vert \\forall h \\in H, z \\notin h }$ $z \\in Z$: sea Function, relations $u_i: Z \\to \\mathbb{R}: $ utility function of player $i$ $\\sigma_i: A \\to [0,1]$ a strategy of player $i$, probability distribution on action set $A$. $\\sigma~: A^N \\to [0,1]$ a strategy profile, $\\sigma := (\\sigma_1, \\dots, \\sigma_N)$ $\\pi^{\\sigma}i: H \\to [0,1]: $ probability of history $h$ under a strategy $$\\sigma$ of player $i$ $\\pi^{\\sigma}: H^N \\to [0,1]: $ probability of history $h$ under a strategy profile $\\sigma$ Then, you can also interplate $u_i$ as the function mapping a storategy profile $\\sigma$ to its utility.\n$$ \\begin{align} u_i(\\sigma) \u0026amp;= \\sum_{h \\in Z} u_i(h) \\pi^{\\sigma}(h) \\\\ \u0026amp;= \\sum_{h \\in Z} u_i(h) \\prod_{i \\in N} \\pi^{\\sigma}_i(h) \\end{align} $$ Nash equilibrium Definition: $(\\text{Nash equilibrium})$\nIn $N$-player extensive game, a strategy profile $\\acute{\\sigma} := (\\acute{\\sigma_1}, \\dots, \\acute{\\sigma_N})$ is the Nash equilibrium if and only if the followings holds.\n$$ \\begin{aligned} u_1(\\acute{\\sigma_1}, \\dots, \\acute{\\sigma_N}) \u0026amp;\\geq \\underset{\\sigma_1}{\\rm max} ~ u_1(\\sigma_1, \\acute{\\sigma_{-1}}) \\\\ u_2(\\acute{\\sigma_1}, \\dots, \\acute{\\sigma_N}) \u0026amp;\\geq \\underset{\\sigma_2}{\\rm max} ~ u_2(\\sigma_2, \\acute{\\sigma_{-2}}) \\\\ \u0026amp;~ \\vdots \\\\ u_N(\\acute{\\sigma_1}, \\dots, \\acute{\\sigma_N}) \u0026amp;\\geq \\underset{\\sigma_N}{\\rm max} ~ u_N(\\sigma_N, \\acute{\\sigma_{-N}}) \\end{aligned} $$\nDefinition: $\\text{(}\\varepsilon\\text{-Nash equilibrium)}$\nIn $N$-player extensive game, a strategy profile $\\acute{\\sigma} := (\\acute{\\sigma_1}, \\dots, \\acute{\\sigma_N})$ is the $\\varepsilon$-Nash equilibrium if and only if the followings holds when $\\forall \\varepsilon \\geq 0$ is given.\n$$ \\begin{aligned} u_1(\\acute{\\sigma_1}, \\dots, \\acute{\\sigma_N}) + \\varepsilon \u0026amp;\\geq \\underset{\\sigma_1}{\\rm max} ~ u_1(\\sigma_1, \\acute{\\sigma_{-1}}) \\\\ u_2(\\acute{\\sigma_1}, \\dots, \\acute{\\sigma_N}) + \\varepsilon \u0026amp;\\geq \\underset{\\sigma_2}{\\rm max} ~ u_2(\\sigma_2, \\acute{\\sigma_{-2}}) \\\\ \u0026amp;~ \\vdots \\\\ u_N(\\acute{\\sigma_1}, \\dots, \\acute{\\sigma_N}) + \\varepsilon \u0026amp;\\geq \\underset{\\sigma_N}{\\rm max} ~ u_N(\\sigma_N, \\acute{\\sigma_{-N}}) \\end{aligned} $$\nRegret matching Average overall regret of player $i$ at time $T$ï¼š $$ R_i^T := \\underset{\\sigma_i^}{\\rm max} ~ \\frac{1}{T} \\sum_{t=1}^{T} \\left( u_i(\\sigma_i^, \\sigma_{-i}^{t}) - u_i(\\sigma_i^t, \\sigma_{-i}^{t}) \\right) $$\nAverage strategy for player $i$ from time $1$ to $T$ï¼š $$ \\begin{align} \\overline{\\sigma}_i^t(I)(a) \u0026amp;:= \\frac{\\sum_{t=1}^{T} \\pi_i^{\\sigma^t}(I) \\cdot \\sigma^t(I)(a)}{\\sum_{t=1}^{T} \\pi_i^{\\sigma^t}(I)} \\\\ \u0026amp;= \\frac{\\sum_{t=1}^{T} \\sum_{h \\in I} \\pi_i^{\\sigma^t}(h) \\cdot \\sigma^t(h)(a)}{\\sum_{t=1}^{T} \\sum_{h \\in I} \\pi_i^{\\sigma^t}(h)} \\end{align} $$\nIf the average overall regret holds $R_i^T \\leq \\varepsilon$, the average strategy $\\overline{\\sigma}_i^t(I)(a) $ is $2 \\varepsilon$-Nash equilibrium for player $i$ in time $t$. So that, in order to derive Nash equilibrium, we should minimize the average overall regret $R_i^T$ or its upper bound $\\varepsilon$ according to $R_i^T \\to 0 ~~ (\\varepsilon \\to 0)$.\nCFR Algorithm Counterfactual utilityï¼š $$ \\begin{align} u_i(\\sigma, I) = \\frac{\\sum_{h \\in H, h\u0026rsquo; \\in Z} \\pi_{-i}^{\\sigma}(h)\\pi^{\\sigma}(h,h\u0026rsquo;)u_i(h) }{\\pi_{-i}^{\\sigma}(I)} \\end{align} $$\nimmediate counteractual regret of action $a$ in Information set $I$: $$ \\begin{aligned} R_{i,imm}^{T}(I, a) := \\frac{1}{T} \\sum_{t=1}^{T} \\pi_{-i}^{\\sigma^t}(I) \\left( u_i(\\sigma^t_{I \\to a}, I) - u_i(\\sigma^t, I) \\right) \\end{aligned} $$\nImmediate counterfactual regret of Information set $I$ï¼š $$ \\begin{aligned} R_{i,imm}^{T}(I) \u0026amp;:= \\underset{a \\in A(I)}{\\rm max} ~ \\frac{1}{T} \\sum_{t=1}^{T} \\pi_{-i}^{\\sigma^t}(I) \\left( u_i(\\sigma^t_{I \\to a}, I) - u_i(\\sigma^t, I) \\right) \\end{aligned} $$\nThe following inequality holds for the average overall regret $R_i^T $ and the immediate counterfactual regret $R_{i,imm}^{T}(I)$:\n$$ \\begin{aligned} R_i^T \\leq \\sum_{I \\in \\mathcal{I}i} \u0026amp;R{i,imm}^{T,+}(I) \\\\ where ~~~ \u0026amp;R_{i,imm}^{T, +}(I) := max(R_{i,imm}^{T}(I), 0) \\end{aligned} $$\nSo that, we obtain the sufficient condition of $R_{i,imm}^{T}(I)$ for the average strategy $\\overline{\\sigma}_i^t(I)(a)$ to become a Nash equilibrium strategy as below.\n$$ \\sum_{I \\in \\mathcal{I}i} R{i,imm}^{T,+}(I) \\to 0 ~~~ \\Rightarrow ~~~ R_i^T \\to 0 ~~~ \\Rightarrow ~~~ \\varepsilon \\to 0. $$\nNow all we need is to minimize the immediate counterfactual regret $R_{i,imm}^{T}(I)$.\nIn addition, as can be seen from the above formula, the computational complexity of the CFR algorithm depends on the number of information sets $I$. Also, to avoid the complete search of game tree (searching all information sets $I$), subsequent algorithms such as CFR + propose an abstraction of the game state.\nPython code to run CFR algorithm for Kuhn Poker import numpy as np # Number of actions a player can take at a decision node. _N_ACTIONS = 2 _N_CARDS = 3 def main(): \u0026quot;\u0026quot;\u0026quot; Run iterations of counterfactual regret minimization algorithm. \u0026quot;\u0026quot;\u0026quot; i_map = {} # map of information sets n_iterations = 10000 expected_game_value = 0 for _ in range(n_iterations): expected_game_value += cfr(i_map) for _, v in i_map.items(): v.next_strategy() expected_game_value /= n_iterations display_results(expected_game_value, i_map) def cfr(i_map, history=\u0026quot;\u0026quot;, card_1=-1, card_2=-1, pr_1=1, pr_2=1, pr_c=1): \u0026quot;\u0026quot;\u0026quot; Counterfactual regret minimization algorithm. Parameters ---------- i_map: dict Dictionary of all information sets. history : [{'r', 'c', 'b'}], str A string representation of the game tree path we have taken. Each character of the string represents a single action: 'r': random chance action 'c': check action 'b': bet action card_1 : (0, 2), int player A's card card_2 : (0, 2), int player B's card pr_1 : (0, 1.0), float The probability that player A reaches `history`. pr_2 : (0, 1.0), float The probability that player B reaches `history`. pr_c: (0, 1.0), float The probability contribution of chance events to reach `history`. \u0026quot;\u0026quot;\u0026quot; if is_chance_node(history): return chance_util(i_map) if is_terminal(history): return terminal_util(history, card_1, card_2) n = len(history) is_player_1 = n % 2 == 0 info_set = get_info_set(i_map, card_1 if is_player_1 else card_2, history) strategy = info_set.strategy if is_player_1: info_set.reach_pr += pr_1 else: info_set.reach_pr += pr_2 # Counterfactual utility per action. action_utils = np.zeros(_N_ACTIONS) for i, action in enumerate([\u0026quot;c\u0026quot;, \u0026quot;b\u0026quot;]): next_history = history + action if is_player_1: action_utils[i] = -1 * cfr(i_map, next_history, card_1, card_2, pr_1 * strategy[i], pr_2, pr_c) else: action_utils[i] = -1 * cfr(i_map, next_history, card_1, card_2, pr_1, pr_2 * strategy[i], pr_c) # Utility of information set. util = sum(action_utils * strategy) regrets = action_utils - util if is_player_1: info_set.regret_sum += pr_2 * pr_c * regrets else: info_set.regret_sum += pr_1 * pr_c * regrets return util def is_chance_node(history): \u0026quot;\u0026quot;\u0026quot; Determine if we are at a chance node based on tree history. \u0026quot;\u0026quot;\u0026quot; return history == \u0026quot;\u0026quot; def chance_util(i_map): expected_value = 0 n_possibilities = 6 for i in range(_N_CARDS): for j in range(_N_CARDS): if i != j: expected_value += cfr(i_map, \u0026quot;rr\u0026quot;, i, j, 1, 1, 1/n_possibilities) return expected_value/n_possibilities def is_terminal(history): possibilities = { \u0026quot;rrcc\u0026quot;: True, \u0026quot;rrcbc\u0026quot;: True, \u0026quot;rrcbb\u0026quot;: True, \u0026quot;rrbc\u0026quot;: True, \u0026quot;rrbb\u0026quot;: True} return history in possibilities def terminal_util(history, card_1, card_2): n = len(history) card_player = card_1 if n % 2 == 0 else card_2 card_opponent = card_2 if n % 2 == 0 else card_1 if history == \u0026quot;rrcbc\u0026quot; or history == \u0026quot;rrbc\u0026quot;: # Last player folded. The current player wins. return 1 elif history == \u0026quot;rrcc\u0026quot;: # Showdown with no bets return 1 if card_player \u0026gt; card_opponent else -1 # Showdown with 1 bet assert(history == \u0026quot;rrcbb\u0026quot; or history == \u0026quot;rrbb\u0026quot;) return 2 if card_player \u0026gt; card_opponent else -2 def card_str(card): if card == 0: return \u0026quot;J\u0026quot; elif card == 1: return \u0026quot;Q\u0026quot; elif card == 2: return \u0026quot;K\u0026quot; def get_info_set(i_map, card, history): \u0026quot;\u0026quot;\u0026quot; Retrieve information set from dictionary. \u0026quot;\u0026quot;\u0026quot; key = card_str(card) + \u0026quot; \u0026quot; + history info_set = None if key not in i_map: info_set = InformationSet(key) i_map[key] = info_set return info_set return i_map[key] class InformationSet(): def __init__(self, key): self.key = key self.regret_sum = np.zeros(_N_ACTIONS) self.strategy_sum = np.zeros(_N_ACTIONS) self.strategy = np.repeat(1/_N_ACTIONS, _N_ACTIONS) self.reach_pr = 0 self.reach_pr_sum = 0 def next_strategy(self): self.strategy_sum += self.reach_pr * self.strategy self.strategy = self.calc_strategy() self.reach_pr_sum += self.reach_pr self.reach_pr = 0 def calc_strategy(self): \u0026quot;\u0026quot;\u0026quot; Calculate current strategy from the sum of regret. \u0026quot;\u0026quot;\u0026quot; strategy = self.make_positive(self.regret_sum) total = sum(strategy) if total \u0026gt; 0: strategy = strategy / total else: n = _N_ACTIONS strategy = np.repeat(1/n, n) return strategy def get_average_strategy(self): \u0026quot;\u0026quot;\u0026quot; Calculate average strategy over all iterations. This is the Nash equilibrium strategy. \u0026quot;\u0026quot;\u0026quot; strategy = self.strategy_sum / self.reach_pr_sum # Purify to remove actions that are likely a mistake strategy = np.where(strategy \u0026lt; 0.001, 0, strategy) # Re-normalize total = sum(strategy) strategy /= total return strategy def make_positive(self, x): return np.where(x \u0026gt; 0, x, 0) def __str__(self): strategies = ['{:03.2f}'.format(x) for x in self.get_average_strategy()] return '{} {}'.format(self.key.ljust(6), strategies) def display_results(ev, i_map): print('player 1 expected value: {}'.format(ev)) print('player 2 expected value: {}'.format(-1 * ev)) print() print('player 1 strategies:') sorted_items = sorted(i_map.items(), key=lambda x: x[0]) for _, v in filter(lambda x: len(x[0]) % 2 == 0, sorted_items): print(v) print() print('player 2 strategies:') for _, v in filter(lambda x: len(x[0]) % 2 == 1, sorted_items): print(v) if __name__ == \u0026quot;__main__\u0026quot;: main() ","date":1581292800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581292800,"objectID":"b0463a81c533ce0008fd98312178c36b","permalink":"https://yumauchiumi.com/post/2020-02-10-counterfactual-regret-minimization/","publishdate":"2020-02-10T00:00:00Z","relpermalink":"/post/2020-02-10-counterfactual-regret-minimization/","section":"post","summary":"In this post, I introduce you the Counterfactual Regret Minimization (CFR Algorithm). It is mainly used for the algorithm to figure out the optimal strategy of a extensive-form game with incomplete information such as Poker and Mahjong.","tags":["RL","Game Theory","Regret","Python"],"title":"Counterfactual Regret Minimization","type":"post"},{"authors":null,"categories":["Random"],"content":" In this note, I describe how to install NVIDIA GPU and set up CUDA/cuDNN on Ubuntu 16.04LTS machine that has been clean booted. Also, I write down some linux commands used in debugging, since knowing your machine in detail would lead to resolving some errors related to the machine environment. This article could be updated from time to time.\nExample: My Ubuntu GPU machine (2020/01/10)\nOS : Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-145-generic x86_64) RAM(16GB) : Memory: Kingston 8GB 288-Pin DDR4 SDRAM DDR4 2133 (PC4 17000) x2 ROM(250GB): SSD: Samsung SSD 750 EVO 250GB (/dev/sda) HDD: Seagate Barracuda ST2000DM001 Desktop SATA (/dev/sdb) CPU(x8) : Intel Core i7-6700 CPU @ 3.40GHz GPU(x1) : NVIDIA Geforce GTX 1080 NVIDIA CUDA : 10.0.130 (/usr/local/cuda-10.0/) NVIDIA cuDNN : 7.4.2.24 (/usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.2) Python3 : 3.6.9 (/usr/bin/python3.6) Python2 : 2.7.12 (/usr/bin/python) tensorflow 1.13.1 ($HOME/.local/lib/python3.6/site-packages) tensorflow-gpu 1.13.1 ($HOME/.local/lib/python3.6/site-packages) keras 2.2.4 ($HOME/.local/lib/python3.6/site-packages) pytorch 1.2.0 ($HOME/.local/lib/python3.6/site-packages) Table of contents\nOperating System Checking Linux OS Checking Linux distribution Checking Linux kernel Storage (ROM) Checking ROM devices Checking the number of files Checking disk space Memory (RAM) Checking RAM devices Checking memory space CPU Checking CPU devices GPU Checking GPU devices NVIDIA driver \u0026amp; CUDA/cuDNN Installing NVIDIA driver Installing NVIDIA CUDA Installing NVIDIA cuDNN I/O Checking X11 display manager (DM) Operating System Checking Linux OS uname command shows 1.OS Name, 2.Hostname, 3.Release, 4.Version, 5,Hardware Architecture, 6,CPU type, 7.Platform, 8.OS Name, respectively,\n$ uname -a Linux XXXX 4.4.0-145-generic #171-Ubuntu SMP Tue Mar 26 12:43:40 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux Checking Linux distribution /etc/issue contains information about Linux distribution.\n$ cat /etc/issue Ubuntu 16.04.6 LTS \\n \\l /etc/lsb-release contains the same information.\n$ cat /etc/lsb-release DISTRIB_ID=Ubuntu DISTRIB_RELEASE=16.04 DISTRIB_CODENAME=xenial DISTRIB_DESCRIPTION=\u0026quot;Ubuntu 16.04.6 LTS\u0026quot; /etc/os-release contains the same information.\n$ cat /etc/os-release NAME=â€œUbuntuâ€ VERSION=â€œ16.04.6 LTS (Xenial Xerus)â€ ID=ubuntu ID_LIKE=debian PRETTY_NAME=â€œUbuntu 16.04.6 LTSâ€ VERSION_ID=â€œ16.04\u0026quot; HOME_URL=â€œhttp://www.ubuntu.com/â€ SUPPORT_URL=â€œhttp://help.ubuntu.com/â€ BUG_REPORT_URL=â€œhttp://bugs.launchpad.net/ubuntu/â€ VERSION_CODENAME=xenial UBUNTU_CODENAME=xenial Checking Linux kernel /proc/version contains information about Linux kernel.\n$ cat /proc/version Linux version 4.4.0-159-generic (buildd@lgw01-amd64-042) (gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10) ) #187-Ubuntu SMP Thu Aug 1 16:28:06 UTC 2019 Storage (ROM) Storage device (HDD, SSD) and file systems.\nChecking ROM devices df commad shows information about ROM (HDD) devices\n$ df -h Filesystem Size Used Avail Use% Mounted on udev 7.8G 0 7.8G 0% /dev tmpfs 1.6G 46M 1.6G 3% /run /dev/sda1 214G 165G 39G 81% / tmpfs 7.9G 208K 7.9G 1% /dev/shm tmpfs 5.0M 4.0K 5.0M 1% /run/lock tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup /dev/loop3 384K 384K 0 100% /snap/patchelf/93 /dev/loop1 384K 384K 0 100% /snap/patchelf/87 none 7.9G 2.5M 7.9G 1% /tmp/guest-qyuodw tmpfs 1.6G 64K 1.6G 1% /run/user/998 /dev/loop4 90M 90M 0 100% /snap/core/8213 /dev/loop0 90M 90M 0 100% /snap/core/8268 tmpfs 1.6G 0 1.6G 0% /run/user/1001 Checking the number of files wc command shows the number of files under the current dir.\n$ du -hsc * 689M\tResearch 4.0K\tbuild 106M\tdataset 4.0K\tdocker 9.3M\tgym 50M\tkaggle 2.6M\tlatent.gif 2.0G\topencv 122G\tworkspace 4.0K\tãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ 4.0K\tãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ 4.0K\tãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ— 4.0K\tãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ 4.0K\tãƒ“ãƒ‡ã‚ª 4.0K\tãƒ”ã‚¯ãƒãƒ£ 4.0K\tãƒŸãƒ¥ãƒ¼ã‚¸ãƒƒã‚¯ 4.0K\tå…¬é–‹ 125G\tåˆè¨ˆ ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡ã‚’ç¢ºèªã—ãŸã„ df -hã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ã†\nã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç›´ä¸‹ã«ã‚ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãŠã‚ˆã³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡ã¨ãã®åˆè¨ˆã‚’è¡¨ç¤ºã™ã‚‹\n$ du -hsc * 689M\tResearch 4.0K\tbuild 106M\tdataset 4.0K\tdocker 9.3M\tgym 50M\tkaggle 2.6M\tlatent.gif 2.0G\topencv 122G\tworkspace 4.0K\tãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ 4.0K\tãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ 4.0K\tãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ— 4.0K\tãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ 4.0K\tãƒ“ãƒ‡ã‚ª 4.0K\tãƒ”ã‚¯ãƒãƒ£ 4.0K\tãƒŸãƒ¥ãƒ¼ã‚¸ãƒƒã‚¯ 4.0K\tå…¬é–‹ ãƒ¡ãƒ¢ãƒªï¼ˆRAMï¼‰ ãƒ¡ãƒ¢ãƒªãƒ‡ãƒã‚¤ã‚¹ã‚’ç¢ºèªã—ãŸã„ /proc/meminfoã‚’ã¿ã‚‹\nãƒ¡ãƒ¢ãƒªã®è©³ç´°æƒ…å ±ãŒè¡¨ç¤ºã•ã‚Œã‚‹\n$ cat /proc/meminfo MemTotal: 16377200 kB MemFree: 3077848 kB MemAvailable: 15767804 kB Buffers: 363052 kB Cached: 12274992 kB SwapCached: 66936 kB Active: 8048088 kB Inactive: 4689560 kB Active(anon): 25860 kB Inactive(anon): 86584 kB ... HugePages_Total: 0 HugePages_Free: 0 HugePages_Rsvd: 0 HugePages_Surp: 0 Hugepagesize: 2048 kB DirectMap4k: 1907316 kB DirectMap2M: 14815232 kB DirectMap1G: 0 kB ãƒ¡ãƒ¢ãƒªã®ç©ºãå®¹é‡ã‚’ç¢ºèªã—ãŸã„ freeã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ã†\n$ free total used free shared buff/cache available Mem: 16377148 2470228 314496 17140 13592424 13460232 Swap: 16720892 431568 16289324 vmstatã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ã†\n$ vmstat procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 431584 267696 944212 12638044 0 2 389 15 0 0 6 2 91 0 0 topã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ã†\n$ top top - 15:55:05 up 64 days, 23:12, 5 users, load average: 1.00, 1.04, 1.07 Tasks: 232 total, 2 running, 230 sleeping, 0 stopped, 0 zombie %Cpu(s): 9.1 us, 3.5 sy, 0.0 ni, 86.9 id, 0.5 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 16377148 total, 271964 free, 2527528 used, 13577656 buff/cache KiB Swap: 16720892 total, 16289228 free, 431664 used. 13403420 avail Mem ... CPU CPUãƒ‡ãƒã‚¤ã‚¹ã‚’ç¢ºèªã—ãŸã„ /proc/cpuinfoã‚’ã¿ã‚‹\nCPUã®ã‚³ã‚¢ã”ã¨ã«è©³ç´°æƒ…å ±ãŒè¡¨ç¤ºã•ã‚Œã‚‹\n$ cat /proc/cpuinfo processor : 0 vendor_id : GenuineIntel cpu family : 6 model : 94 model name : Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz stepping : 3 microcode : 0xc6 cpu MHz : 800.062 cache size : 8192 KB physical id : 0 siblings : 8 ... processor : 1 vendor_id : GenuineIntel cpu family : 6 model : 94 model name : Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz ... GPU GPUãƒ‡ãƒã‚¤ã‚¹ã®ç¢ºèª lswsã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ã†\n$ sudo lshw -C display *-display è©³ç´°: VGA compatible controller è£½å“: GP104 [GeForce GTX 1080] ãƒ™ãƒ³ãƒ€ãƒ¼: NVIDIA Corporation ç‰©ç†ID: 0 ãƒã‚¹æƒ…å ±: pci@0000:01:00.0 ãƒãƒ¼ã‚¸ãƒ§ãƒ³: a1 å¹…: 64 bits ã‚¯ãƒ­ãƒƒã‚¯: 33MHz æ€§èƒ½: pm msi pciexpress vga_controller bus_master cap_list rom è¨­å®š: driver=nvidia latency=0 ãƒªã‚½ãƒ¼ã‚¹: irq:317 ãƒ¡ãƒ¢ãƒªãƒ¼:de000000-deffffff ãƒ¡ãƒ¢ãƒªãƒ¼:c0000000-cfffffff ãƒ¡ãƒ¢ãƒªãƒ¼:d0000000-d1ffffff IOãƒãƒ¼ãƒˆ:e000(ã‚µã‚¤ã‚º=128) ãƒ¡ãƒ¢ãƒªãƒ¼:df000000-df07ffff lspciã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ã†\nLinuxã«æ­è¼‰ã•ã‚Œã¦ã„ã‚‹PCIãƒã‚¹ã®æƒ…å ±ã‚’è¡¨ç¤ºã™ã‚‹ï¼\n$ lspci | grep -i nvidia 01:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1080] (rev a1) 01:00.1 Audio device: NVIDIA Corporation GP104 High Definition Audio Controller (rev a1) NVIDIAãƒ‰ãƒ©ã‚¤ãƒã¨CUDA/cuDNNã®å°å…¥ NVIDIAãƒ‰ãƒ©ã‚¤ãƒã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« 1.ä¸‹è¨˜ãƒªãƒ³ã‚¯ã‹ã‚‰ï¼Œè‡ªåˆ†ã®GPUã«ã‚ã†ãƒ‰ãƒ©ã‚¤ãƒã‚’æ¤œç´¢ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ï¼\nhttps://www.nvidia.co.jp/Download/index.aspx?lang=jp\nãŸã¨ãˆã°ï¼ŒGPUã€ŒNVIDIA GeForce 1080ã€ã«å¯¾å¿œã—ãŸãƒ‰ãƒ©ã‚¤ãƒã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚‹ï¼\nï¼\næ–°ã—ãGPUãƒ‰ãƒ©ã‚¤ãƒï¼ˆNVIDIAãƒ‰ãƒ©ã‚¤ãƒï¼‰ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å‰ã«ï¼Œæ—¢ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹GPUãƒ‰ãƒ©ã‚¤ãƒã‚’ç¢ºèªã™ã‚‹ï¼ aptã«NVIDIAãƒ‰ãƒ©ã‚¤ãƒã‚’æä¾›ã—ã¦ã„ã‚‹xorg-edgersãƒ¬ãƒã‚¸ãƒˆãƒªã‚’è¿½åŠ ã™ã‚‹ï¼\naptã§NVIDIAãƒ‰ãƒ©ã‚¤ãƒã€Œnvidia-396ã€ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ï¼Œãƒã‚·ãƒ³ã‚’å†èµ·å‹•ï¼\nCUDAã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« ï¼ˆæ³¨æ„ï¼‰CUDAãƒ»cuDNNãƒ»tensorFlow-gpuã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’åˆã‚ã›ã‚‹å¿…è¦ãŒã‚ã‚‹ï¼\nCUDAã®å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚ˆãèª­ã‚€ï¼\nCUDA Toolkit Documentation https://docs.nvidia.com/cuda/index.html\nä¸‹è¨˜ãƒªãƒ³ã‚¯ã‹ã‚‰ï¼ŒNVIDIAãƒ‰ãƒ©ã‚¤ãƒã«å¯¾å¿œã™ã‚‹CUDAã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¢ºèªã™ã‚‹\nCUDA Toolkit Documentation \u0026gt; Release Notes \u0026gt; 1. CUDA Toolkit Major Components \u0026gt; CUDA Driver https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html\nä¸‹è¨˜ãƒªãƒ³ã‚¯ã‹ã‚‰ï¼Œtensorflow-gpuã«å¯¾å¿œã™ã‚‹cuDNN/CUDAã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¢ºèªã™ã‚‹\nTensorFlow (Linux) - ãƒ†ã‚¹ãƒˆæ¸ˆã¿ã®ãƒ“ãƒ«ãƒ‰è¨­å®š\nhttps://www.tensorflow.org/install/source#linux\nCUDAãƒ»cuDNNãƒ»tensorFlow-gpuã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèªã‚’çµ‚ãˆãŸï¼\nä»Šå›ã¯ï¼Œä»¥ä¸‹ã§ç’°å¢ƒæ§‹ç¯‰ã‚’ã™ã‚‹ï¼\nPython 3.6.9 tensorflow-gpu 1.13.1 CUDA 10.0 cuDNN 7.4 ä¸‹è¨˜ãƒªãƒ³ã‚¯ã‹ã‚‰ï¼Œè‡ªåˆ†ã®ç’°å¢ƒã«ã‚ã£ãŸã€ŒCUDA Toolkitãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã€ã‚’ç¢ºèªã—ï¼Œãƒã‚·ãƒ³ã¸ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ï¼\nCUDA Toolkit Archive https://developer.nvidia.com/cuda-toolkit-archive\nä»Šå›ã¯ï¼ŒCUDA10.0ã§ï¼Œãƒã‚·ãƒ³ã®ç’°å¢ƒã¨ã—ã¦ï¼Œä»¥ä¸‹ã‚’é¸æŠï¼\nOperating System: Linux Architecture: x86_64 Distribution: Ubuntu Version: 16.04 Installer Type: deb [network] ï¼ˆæ³¨æ„ï¼‰https://developer.nvidia.com/cuda-downloadsã¯ï¼Œæœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ãªã®ã§ï¼Œã“ã“ã‹ã‚‰å®‰æ˜“ã«CUDAã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã¯ã„ã‘ãªã„ï¼ç‰¹ã«ï¼Œtensorflow-gpuã¯ï¼Œæœ€æ–°ã®CUDA Toolkitã«å¯¾å¿œã—ã¦ã„ãªã„ã®ã§æ³¨æ„ã™ã‚‹ï¼CUDAã¨Tensorflow-gpuã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒã‚ã£ã¦ã„ãªã„ã¨ï¼ŒãŸã¨ãˆã°ImportError: libcublas.so.10.0ãŒç™ºç”Ÿã™ã‚‹ï¼\nå¯¾å¿œã™ã‚‹CUDA Toolkitï¼ˆCUDA 10.0ï¼‰ã®.debãƒ•ã‚¡ã‚¤ãƒ«(network)ã¯ã€Œcuda-repo-ubuntu1604_10.0.130-1_amd64.debã€ã¨ãªã‚‹ï¼ ã“ã®.debãƒ•ã‚¡ã‚¤ãƒ«ã‚’wgetã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ã£ã¦ï¼Œãƒã‚·ãƒ³ã¸ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ï¼\nãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸCUDA Toolkitãƒ‘ãƒƒã‚±ãƒ¼ã‚¸(.deb)ã‚’ï¼Œãƒã‚·ãƒ³ã¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ dpkgã‚³ãƒãƒ³ãƒ‰ã§CUDA Toolkitãƒ‘ãƒƒã‚±ãƒ¼ã‚¸(.deb)ã‚’cudaãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¨ã—ã¦ä¿å­˜ã—ã¾ã™ï¼ã•ã‚‰ã«ï¼Œaptã‚³ãƒãƒ³ãƒ‰ã§cudaãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ï¼ æ³¨æ„ï¼šå…¬å¼ã«æ›¸ã‹ã‚Œã¦ã„ã‚‹sudo apt-get install cudaã‚’å®Ÿè¡Œã™ã‚‹ã¨è‡ªå‹•çš„ã«æœ€æ–°ç‰ˆã®CUDAãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã‚‹ï¼\nã“ã‚Œã§CUDA Toolkitï¼ˆCUDA 10.0ï¼‰ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¯å®Œäº†ï¼\næ¬¡ã«ï¼Œç’°å¢ƒå¤‰æ•°ï¼ˆPATHï¼‰ã‚’è¨­å®šã™ã‚‹ï¼\ncuDNNã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« PATHãƒã‚§ãƒƒã‚¯ ãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤ X11ãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤ãƒãƒãƒ¼ã‚¸ãƒ£(DM)ã‚’ç¢ºèª /etc/X11/default-display-managerã‚’ã¿ã‚‹\n","date":1579132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579132800,"objectID":"1269f2e0e4402c1d9168bd4342744f90","permalink":"https://yumauchiumi.com/post/2020-01-16-setup-gpu-machine-for-ml/","publishdate":"2020-01-16T00:00:00Z","relpermalink":"/post/2020-01-16-setup-gpu-machine-for-ml/","section":"post","summary":"In this note, I describe how to install NVIDIA GPU and set up CUDA/cuDNN on Ubuntu 16.04LTS machine that has been clean booted. Also, I write down some linux commands","tags":["Env"],"title":"Setting up a GPU machine for Machine Learning","type":"post"},{"authors":null,"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://yumauchiumi.com/about/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"About Me","tags":null,"title":"About","type":"widget_page"},{"authors":null,"categories":["Random"],"content":"Prof. George M. Whitesides is a top-level researcher on chemistry and also known as one of the highest h-index researchers in the world. He explains his unique writing techniques called Outline Method in the article, \u0026ldquo;Whitesides\u0026rsquo; Group: Writing a Paper\u0026rdquo;. I\u0026rsquo;ve translated that into Japanese and publish it here.\nOriginal paper This browser does not support PDFs. Please download the PDF to view it: Download PDF.\nJapanese translation This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n","date":1574640000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574640000,"objectID":"981967c8ac223875067a00abfb3c335b","permalink":"https://yumauchiumi.com/post/2019-11-25-whitesides_outline_method/","publishdate":"2019-11-25T00:00:00Z","relpermalink":"/post/2019-11-25-whitesides_outline_method/","section":"post","summary":"Prof. George M. Whitesides is a top-level researcher on chemistry and also known as one of the highest h-index researchers in the world. He explains his unique writing techniques called Outline Method in the article, \u0026ldquo;Whitesides\u0026rsquo; Group: Writing a Paper\u0026rdquo;.","tags":["Tips","Book"],"title":"Whiteside's Outline Method","type":"post"},{"authors":null,"categories":["StatML"],"content":" â€‹\tEM algorithm is an algorithm for deriving the maximum likelihood estimator (MLE), which is generally applied to statistical methods for incomplete data. Originally, the concept of â€œincomplete data and complete dataâ€ was established to handle missing data, but by extending the definition, it can be applied to cut data, censored data, mixed distribution models, Robust distribution models, and latent data. It can also be applied to variable models, and Bayesian modeling.\nâ€‹\tAlso, a number of statistical approach for clustering and unsupervised learning (eg, k-means, Gaussian mixture models) can be generalized as EM algorithms when focusing on the computational process. In addition, researches on analyzing the EM algorithm from the viewpoint of information geometry has been active, and applying EM algorithm to the stochastic model including an exponential family can be summarized in the form of e-projection / m-projection.\n1. Statistical inference Objectives: To find out the probability distribution $q(x)$ that a certain variable $x \\in X$ follows.\nNamely, when considering a stochastic model $p(x \\vert \\theta)$ determined by the parameter $\\theta \\in \\Theta$ and detecting the optimal parameter $\\theta^{*} \\in \\Theta$ from dataset $ \\mathcal{D} := {\\{x_i\\}}_{i=1}^{n}$, the follwing Approximation holds.\n$$ \\begin{align} x \\sim q(x) \\approx p(x|\\theta) \\end{align} $$\nThis is called a statistical inference (or statistical estimation).\n2. Maximum likelihood estimation The most basic algorithm for statistical inference is maximum likelihood estimation (MLE). A log likelihood function of the stochastic model $p(x \\vert \\theta)$ is defined as\n$$ \\begin{align} \\ell(\\theta | x) := \\log p(x | \\theta) \\end{align} $$\nand an empirical objective function of $\\theta$:\n$$ \\begin{align} J(\\theta) := \\frac{1}{n} \\sum_{i=1}^{n} \\ell(\\theta | x_i) \\end{align} $$\nthat depends on dataset $ \\mathcal{D} := {{x_i}}_{i=1}^{n}$ can be obtained, MLE of parameter $\\theta$ is derived as follows.\n$$ \\begin{align} \\hat{\\theta}_{MLE} = \\underset{\\theta \\in \\Theta}{\\rm argmax} ~ J(\\theta) \\end{align} $$\n3. EM algorithm Let\u0026rsquo;s define the following data categories.\nComplete data $Y \\in \\mathcal{Y}$ : not observable but completely follows the true distribution $p(y)$ Imcomplete data $X \\in \\mathcal{X}$ : observable but not completely follows the true distribution $p(x)$ In general, the relationship complete data $y$ and incomplete data $x$ is a one-to-many relationship. but here, as a convenient assumption, I introduce a latent variable $z \\in Z$ to express this constraint, that is assume $y = [x, z]$ holds. Considering the stochastic model for the complete data $x$,\n$$ \\begin{align} p(y | \\theta) = p(x,z | \\theta) \\end{align} $$\nComplete data $\\{X,Z\\} \\in \\mathcal{X \\times Z}$ : not observable but completely follows the true distribution $p(x,z)$ Imcomplete data $X \\in \\mathcal{X}$ : observable but not completely follows the true distribution $p(x)$ data sample $x_i$ cannot be observed and its likelihood $p(x_i \\vert \\theta)$ cannot be calculated. However, for pair data sample $\\{x_i, z_i\\}$ can be observed and its likelihood $p(x_i, z_i \\vert \\theta)$ can be calculated.\n$$ \\begin{align} p(x_i | \\theta) \u0026amp;= \\int_{Z} p(x_i, z_i | \\theta) ~ dz \\ \\end{align} $$\nBy using this formula, the estimated value of $\\hat{\\theta}_{MLE}$ can be obtained by approximating $p(x,z \\vert \\theta)$, the likelihood function of complete data $\\{x, z\\}$. The procedure to derive the estimated value of $\\hat{\\theta}_{MLE}$ is called EM algorithm because it is an iterative method that repeats E-step and M-step alternately.\nEM algorithm\nInitialize $\\theta$ with $\\theta^{0}$.\nFor each step $t$:\nE Step: Update the expectation value $Q$.\n$$ \\begin{aligned} Q(\\theta | \\theta^{(t)}) \u0026amp;= \\mathbb{E}_{z \\sim p(z \\vert x, \\theta^{(t)})} \\left[ \\log p(x, z \\vert \\theta) \\right] \\\\ \u0026amp;\\simeq \\sum_{i=1}^{n} p(z_i \\vert x_i, \\theta^{(t)}) \\log p(x_i, z_i \\vert \\theta) \\\\ \u0026amp;= \\sum_{i=1}^{n} p(z_i \\vert x_i, \\theta^{(t)}) \\log p(z_i \\vert x_i, \\theta) + Const. \\end{aligned}$$\nM Step: Derive the optimal parameter ${\\theta}^{(t+1)}$ that maximize $Q$ value.\n$$\\begin{aligned} {\\theta}^{(t+1)} \u0026amp;= \\underset{\\theta \\in \\Theta}{\\rm argmax} ~ Q(\\theta \\vert {\\theta}^{(t)}) \\end{aligned}$$\nConsider the convergence value $\\theta^{(\\infty)}$ as the algorithm output $\\hat{\\theta}_{EM}$.\nAs a result, the estimated value of $\\hat{\\theta}_{MLE}$ is derived as $\\hat{\\theta}_{EM}$ and the following holds.\n$$ \\begin{align} \\hat{\\theta}_{MLE} \\approx \\hat{\\theta}_{EM}, ~~~ p(x|\\hat{\\theta}_{MLE} ) \\approx p(x|\\hat{\\theta}_{EM} ) \\end{align} $$\nAlso, the summarized formula of calculations in E step and M step is as follows.\nFor each step $t$:\nEM Step:\n$$\\begin{align} \\theta^{(t+1)} \u0026amp;= \\underset{\\theta \\in \\Theta}{\\rm argmax} ~ \\mathbb{E}_{z \\sim p(z \\vert x, \\theta^{(t)})} \\left[ \\log p(x, z \\vert \\theta) \\right] \\\\ \u0026amp;= \\underset{\\theta \\in \\Theta}{\\rm argmax} ~ \\sum_{i=1}^{n} p(z_i \\vert x_i, \\theta^{(t)}) \\log p(x_i, z_i \\vert \\theta) \\end{align} $$\nReferences PRML Chapter 9: Mixture models and EM 9ç«  EMã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  - ã€Œ21 ä¸–ç´€ã®çµ±è¨ˆç§‘å­¦ã€ç¬¬ III å·» æ—¥æœ¬çµ±è¨ˆå­¦ä¼š, 2008 è§£èª¬ EMã‚¢ãƒ«ã‚³ã‚™ãƒªã‚¹ã‚™ãƒ ã®å¹¾ä½•å­¦ - èµ¤ç©‚æ˜­å¤ªéƒ, é›»å­æŠ€è¡“ç·åˆç ”ç©¶æ‰€ EMã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ç¥çµŒå›è·¯ç¶², 2000, çµ±è¨ˆæ•°ç†ç ”ç©¶æ‰€ ","date":1569369600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569369600,"objectID":"69426175cd5772c606e672386dab8632","permalink":"https://yumauchiumi.com/post/2019-09-25-em-algorithm/","publishdate":"2019-09-25T00:00:00Z","relpermalink":"/post/2019-09-25-em-algorithm/","section":"post","summary":"â€‹ EM algorithm is an algorithm for deriving the maximum likelihood estimator (MLE), which is generally applied to statistical methods for incomplete data. Originally, the concept of â€œin","tags":["Bayes"],"title":"EM Algorithm","type":"post"},{"authors":null,"categories":["Finance"],"content":" 1. è³‡æœ¬ä¸»ç¾©ã®åŸç† è³‡æœ¬ä¸»ç¾© - Capitalism\nè³‡æœ¬ä¸»ç¾©ã¨ã¯ä½•ã‹ï¼Ÿã‚’è€ƒãˆã‚‹éš›ã«ã¯ï¼Œè³‡æœ¬ä¸»ç¾©ã§ã¯ãªã„ã‚‚ã®ã¯ä½•ã‹ï¼Ÿã‚’è€ƒãˆï¼Œãã®å·®åˆ†ã‚’ã¾ã¨ã‚ç›´ã›ã°è‰¯ã„ï¼è³‡æœ¬ä¸»ç¾©ã®å¯¾ç¾©èªã¯ï¼Œå…±ç”£ä¸»ç¾©ï¼ˆç¤¾ä¼šä¸»ç¾©ï¼‰ã«ãªã‚‹ï¼ã“ã‚Œã‚‰ã‚’æ’ä»–çš„ã«æ¯”è¼ƒã™ã‚Œã°ï¼Œãã®å®šç¾©ï¼ˆæ”¿æ²»çš„æ¦‚å¿µã§ã‚ã‚‹\u0026quot;è‡ªç”±\u0026quot;ã‚„\u0026quot;æ°‘ä¸»ä¸»ç¾©\u0026quot;ã«ã¤ã„ã¦ã¯é™¤å¤–ã™ã‚‹ï¼‰ã¯ï¼Œæ¬¡ã®ã‚ˆã†ã«ã¾ã¨ã‚ã‚‰ã‚Œã‚‹ã¨æ€ã†ï¼\nè³‡æœ¬ä¸»ç¾©ï¼šå€‹äººãŒè³‡æœ¬ã‚’æ‰€æœ‰ã§ãã‚‹ å…±ç”£ä¸»ç¾©ï¼šå€‹äººãŒè³‡æœ¬ã‚’æ‰€æœ‰ã§ããªã„ éå¸¸ã«ã‚·ãƒ³ãƒ—ãƒ«ãªå®šç¾©ã ãŒï¼Œåƒ•ã¯æ°—ã«å…¥ã£ã¦ã„ã‚‹ã—ï¼Œã“ã®å®šç¾©ã‚’ä½¿ã£ã¦æ–‡æ„è§£é‡ˆã«è‹¦é›£ã—ãŸã“ã¨ã¯ãªã„ï¼è¦ã™ã‚‹ã«ï¼Œä½•ã‚‰ã‹ã®æ–‡è„ˆã§ã€Œè³‡æœ¬ä¸»ç¾©ã€ã¨ã„ã†ãƒ¯ãƒ¼ãƒ‰ãŒä½¿ã‚ã‚ŒãŸå ´åˆï¼Œãã“ã§ã¯ç§çš„è²¡ç”£ã®æ‰€æœ‰ã‚’èªã‚ã‚‹ã‹ãƒ»èªã‚ãªã„ã‹ãŒè¨€åŠã•ã‚Œã¦ã„ã‚‹ã®ã ï¼\nNote:\nåŸç†ã¨ã—ã¦ã®ã€Œè³‡æœ¬ä¸»ç¾©/å…±ç”£ä¸»ç¾©ã€2å…ƒè«–ã¯éå¸¸ã«ã‚·ãƒ³ãƒ—ãƒ«ã ãŒï¼Œå®Ÿéš›ã®äººé¡å²ã§ã¯ï¼Œã“ã‚Œã‚‰ã‚’ç¾å®Ÿç¤¾ä¼šã¸åˆ¶åº¦ãƒ»ã‚·ã‚¹ãƒ†ãƒ ã¨ã—ã¦å°å…¥ã—ï¼Œã“ã®åŸç†ã‚’é§†å‹•ã•ã›ã‚‹ãŸã‚ã«ï¼Œæ§˜ã€…ãªæ³•çš„ãƒ»æ”¿æ²»çš„ãªã—ãã¿ãŒææ¡ˆã•ã‚Œã¦ã„ã‚‹ï¼ã—ã‹ã—ï¼Œå†ç¾æ€§ãŒãªã„ç¾è±¡(=å²å®Ÿ)ã«ã¤ã„ã¦ã¯é»™ã‚‹ã®ãŒç§‘å­¦ã®ãƒ«ãƒ¼ãƒ«ãªã®ã§ï¼Œã“ã“ã§ã¯ç«‹ã¡å…¥ã‚‰ãªã„ã“ã¨ã¨ã™ã‚‹ï¼\nè³‡æœ¬ - Capital\nè³‡æœ¬ã¨ã¯ï¼Œå¯Œã‚’ç”Ÿç”£ã™ã‚‹ãŸã‚ã®æ§‹æˆè¦ç´ ã‚’æŒ‡ã™æŠ½è±¡æ¦‚å¿µã§ã‚ã‚‹ï¼\nè³‡æœ¬ï¼šç”Ÿç”£è¦ç´  è³‡æœ¬ã¯ï¼Œåºƒç¾©ã§ã¯ç”Ÿç”£è¦ç´ ã‚’æŒ‡ã™ç”¨èªã§ã‚ã‚Šï¼Œä»¥é™ã§æ‰±ã†ãƒ’ãƒˆãƒ»ãƒ¢ãƒãƒ»ã‚«ãƒã¯å…¨ã¦è³‡æœ¬ã¨ã¿ãªã™ã“ã¨ã‚‚ã§ãã‚‹ï¼ã—ã‹ã—ï¼Œå®Ÿéš›ã«ã¯ï¼Œãã®ç¤¾ä¼šçš„å½¹å‰²ã‚„æ­´å²çš„æ„å‘³ã«æ³¨æ„ã—ã¦ï¼Œã“ã‚Œã‚‰ã‚’åˆ†ã‘ã¦æ‰±ã†ã“ã¨ãŒå¤šã„ï¼ã“ã‚Œã¯çµŒæ¸ˆå­¦ãŒï¼Œè‡ªç„¶ç§‘å­¦ã§ã¯ãªãç¤¾ä¼šç§‘å­¦ãŸã‚‹æ‰€ä»¥ã§ã‚ã‚‹ï¼ã™ãªã‚ã¡ï¼ŒçµŒæ¸ˆå­¦ãŒæ‰±ã†ç¯„ç–‡ã§ã‚ã‚‹ã€Œäººé–“ç¤¾ä¼šã€ã®åˆ¶åº¦ãƒ»ã‚·ã‚¹ãƒ†ãƒ ã«ã¯ï¼Œæ³•ã‚„æ”¿æ²»ãŒè¨­è¨ˆç†å¿µã¨ã—ã¦æ·±ãé–¢ã‚ã£ã¦ãŠã‚Šï¼Œå®Ÿéš›çš„ãƒ»å®Ÿç”¨çš„ãªåˆ†æã‚’è¡Œã†éš›ã«ã¯ï¼Œã“ã‚Œã‚‰ã¯åˆ†é›¢ä¸å¯èƒ½ã§ã‚ã‚‹ã¨ã„ã†ã“ã¨ã§ã‚ã‚‹ï¼\nNote:\nè³‡æœ¬ã«ã‚ˆã£ã¦ï¼Œæˆ‘ã€…ãŒç”Ÿç”£ã—ã¦ã„ã‚‹ã‚‚ã®(=å¯Œ)ã¨ã¯ä½•ã‹ï¼Ÿã¨ã„ã†ç–‘å•ãŒæ®‹ã‚‹ï¼çµè«–ã‹ã‚‰è¨€ã†ã¨ï¼Œåƒ•ã¯ã¾ã ç†è§£ã§ãã¦ã„ãªã„ï¼ˆè©³ã—ã„æ–¹ã«æ•™ãˆã¦æ¬²ã—ã„ï¼‰ï¼ä»¥ä¸‹ã«ç¾æ™‚ç‚¹ã§ã®ä¸€å¿œã®è§£é‡ˆã‚’ã¾ã¨ã‚ã¦ãŠã\nã‚¢ãƒ€ãƒ ãƒ»ã‚¹ãƒŸã‚¹ã®æ™‚ä»£ã«ï¼Œå½“æ™‚ã®ç†è«–å®¶ã¯ï¼ˆæŠ½è±¡çš„ãªæ¦‚å¿µã¨ã—ã¦ã®ï¼‰ã€Œå¯Œã€ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«æ€è€ƒã‚’é‡ã­ãŸï¼ãã®çµæœï¼Œå›½å®¶ã«ãŠã‘ã‚‹ã€Œå¯Œã€ã‚’èª¬æ˜ã™ã‚‹éš›ã«ï¼Œè¦³æ¸¬å¯èƒ½ãªã‚‚ã®ã®ã†ã¡ï¼Œã‚‚ã£ã¨ã‚‚å¦¥å½“ãªæŒ‡æ¨™ã¯ï¼Œè³‡æœ¬(=ç”Ÿç”£è¦ç´ )ã ã£ãŸï¼ãã®ãŸã‚ã€Œå›½å®¶ã®å¯Œâˆå›½å®¶ã®ç”Ÿç”£åŠ›ã€ã¨ä»®å®šã—ï¼Œãã®å›½ã®è³‡æœ¬ã«æ³¨ç›®ã—ãŸï¼\nãªãŠï¼Œã€Œè¦³æ¸¬ã§ããªã„äº‹å®Ÿã€ã«ã¤ã„ã¦ã¯å•é¡Œã®ç¯„ç–‡å¤–ã¨ã™ã‚‹ã‹ï¼Œã‚ã‚‹ã„ã¯å¦¥å½“ãªä»®å®šã‚’ãŠã„ã¦æ¢æ±‚ã‚’é€²ã‚ã‚‹ã®ãŒçµŒé¨“ç§‘å­¦ã®ä½œæ³•ãªã®ã§ï¼Œã“ã“ã§ã¯ç«‹ã¡å…¥ã‚‰ãªã„ã“ã¨ã¨ã™ã‚‹ï¼\nå–å¼•ã¨å¸‚å ´ - Trading \u0026amp; markets\næˆ‘ã€…ãŒï¼ŒçµŒæ¸ˆå­¦çš„è¦–ç‚¹ã§ç¤¾ä¼šã‚’ã¿ã‚‹ã¨ãï¼Œãã‚Œã¯ã™ãªã‚ã¡ï¼Œæˆ‘ã€…ãŒã€Œå–å¼•ã€ã«ç€ç›®ã™ã‚‹ã“ã¨ã‚’æ„å‘³ã™ã‚‹ï¼ã€Œå–å¼•ã€ã¨ã¯ã€Œè³‡æœ¬(=ç”Ÿç”£è¦ç´ )ã®æ‰€æœ‰æ¨©ã®äº¤æ›ã€ã§ã‚ã‚Šï¼Œå–å¼•ã®å‚¾å‘ã‚’åˆ†æã™ã‚‹ã“ã¨ã§ï¼ŒçµŒæ¸ˆã®ãƒŸã‚¯ãƒ­ãªçŠ¶æ…‹ã‚’æ¨å®šã™ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼ã•ã‚‰ã«ï¼Œå–å¼•ã®é›†åˆã«å¯¾ã—ã¦ã€Œå¸‚å ´ã€ã¨ã„ã†æ¦‚å¿µã‚’ä¸ãˆï¼Œå–å¼•ã®çµ±è¨ˆçš„å‚¾å‘ã‚’åˆ†æã™ã‚‹ã“ã¨ã§ï¼ŒçµŒæ¸ˆã®ãƒã‚¯ãƒ­ãªçŠ¶æ…‹ã‚’æ¨å®šã™ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼\n2. è³‡æœ¬ä¸»ç¾©ç¤¾ä¼šã«å¯¾ã™ã‚‹å€‹äººã®é©åˆæˆ¦ç•¥ ã“ã“ã‹ã‚‰ã¯ï¼Œè³‡æœ¬ä¸»ç¾©ã®åŸç†ã‚’è¸ã¾ãˆãŸä¸Šã§ï¼Œã‚ã‚Œã‚ã‚Œå€‹äººãŒå–ã‚‹ã¹ãæˆ¦ç•¥ã«ã¤ã„ã¦è€ƒãˆã¦ã¿ã‚ˆã†ã¨æ€ã†ï¼ã“ã“ã§ã„ã†ã€Œæˆ¦ç•¥ã€ã¨ã¯ï¼Œã€Œè¡Œå‹•é¸æŠã€ã¨åŒç¾©ã§ã‚ã‚‹ï¼ãªãŠï¼Œå®Ÿéš›ã®ç¤¾ä¼šã§è€ƒæ…®ã™ã¹ãå€‹åˆ¥å…·ä½“çš„ãªæ¡ä»¶ã¯è€ƒãˆãšï¼Œã‚ãã¾ã§è³‡æœ¬ä¸»ç¾©ã®\u0026quot;åŸç†\u0026quot;ã‚’è¡¨ç¾ã™ã‚‹ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ï¼Œã‚²ãƒ¼ãƒ ã‚’å°å…¥ã—ã¦ã„ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ã»ã—ã„ï¼\nè³‡æœ¬ä¸»ç¾©ã‚²ãƒ¼ãƒ  - The capitalist\u0026rsquo;s game\nå€‹äººã«ã¨ã£ã¦é‡è¦ãªå•é¡Œã¯ï¼ŒãƒŸã‚¯ãƒ­ãªç¤¾ä¼šçŠ¶æ³ã«å¯¾ã—ã¦ï¼Œç§‘å­¦çš„ãªè¦–ç‚¹ã‚’ã‚‚ã¡ï¼Œåˆç†çš„ã«æ„æ€æ±ºå®šãƒ»è¡Œå‹•é¸æŠã‚’è¡Œã†ã“ã¨ã§ã‚ã‚‹ï¼ã“ã“ã§ã¯ï¼Œè³‡æœ¬ä¸»ç¾©ã®åŸç†ã‚’è¸ã¾ãˆãŸä»¥ä¸‹ã®ã‚ˆã†ãªã‚²ãƒ¼ãƒ ã‚’è€ƒãˆã¦ï¼Œå€‹äººã®é©åˆæˆ¦ç•¥ã‚’è€ƒãˆã¦ã¿ã‚‹ï¼\nç¤¾ä¼šï¼šã‚²ãƒ¼ãƒ ä¸–ç•Œ\nå€‹äººï¼šãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼\nè³‡æœ¬ï¼šãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æ‰€æœ‰ã‚¢ã‚¤ãƒ†ãƒ  è³‡æœ¬ä¸»ç¾©çµŒæ¸ˆï¼šã‚²ãƒ¼ãƒ ãƒ«ãƒ¼ãƒ«\nå–å¼•ï¼šè³‡æœ¬ã®æ‰€æœ‰æ¨©ã®äº¤æ› å¸‚å ´ï¼šå–å¼•ã®é›†åˆ ä¾¡å€¤ï¼šå¸‚å ´ã‹ã‚‰çµ±è¨ˆçš„ã«æ¨å®šã•ã‚Œã‚‹å„è³‡æœ¬ã®è©•ä¾¡å€¤ ã“ã®ã‚²ãƒ¼ãƒ ã«ãŠã‘ã‚‹å„ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ç›®æ¨™ã¯ï¼Œã€Œå¸‚å ´ä¾¡å€¤ã€ãŒå¸¸ã«æœ€å¤§ã¨ãªã‚‹ã‚ˆã†ã«æ‰€æœ‰ã™ã‚‹è³‡æœ¬ã‚’é¸æŠã—ç¶šã‘ã‚‹ã“ã¨ã¨ãªã‚‹ï¼ã“ã“ã§ï¼Œã€Œå¸‚å ´ä¾¡å€¤ã€ã¨ã¯ï¼Œã‚²ãƒ¼ãƒ å‚åŠ è€…ã«ã‚ˆã£ã¦è¡Œã‚ã‚Œã‚‹å–å¼•ï¼ˆè³‡æœ¬ã®æ‰€æœ‰æ¨©ã®äº¤æ›ï¼‰ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã‹ã‚‰ï¼Œãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§çµ±è¨ˆçš„ã«æ¨å®šã•ã‚ŒãŸï¼Œå„è³‡æœ¬ã®è©•ä¾¡å€¤ã§ã‚ã‚‹ï¼\nNote:\nè³‡æœ¬ä¾¡å€¤ã®å®šç¾©ã«ã€Œè©•ä¾¡å€¤ã€ã¨ã„ã†è¡¨ç¾ã‚’ä½¿ã£ãŸãŒï¼Œã“ã‚Œã¯ä¸€ä½“ä½•ãªã®ã‹ï¼Ÿå„è³‡æœ¬ã®è©•ä¾¡å€¤ã¯ï¼Œå–å¼•ãŒè¡Œã‚ã‚Œã‚‹å¸‚å ´ã«ã‚ˆã£ã¦ç•°ãªã‚‹ãŒï¼Œãã®ç®—å®šãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã¯ã€Œéœ€è¦ã€ã¨ã€Œä¾›çµ¦ã€ã¨ã„ã†æŠ½è±¡æ¦‚å¿µã§èª¬æ˜ã•ã‚Œã‚‹ï¼ä¾‹ã¨ã—ã¦ï¼Œæ ªå¼å¸‚å ´ã§ã¯ï¼Œä¼æ¥­ã®æ‰€æœ‰æ¨©ï¼ˆæ ªå¼ï¼‰ã«å¯¾ã—ã¦è©•ä¾¡å€¤ï¼ˆæ ªä¾¡ï¼‰ã‚’ä¸ãˆã¦ã„ã‚‹ãŒï¼Œã“ã‚Œã¯éœ€è¦ï¼ˆè²·ã„æ³¨æ–‡ï¼‰ã¨ä¾›çµ¦ï¼ˆå£²ã‚Šæ³¨æ–‡ï¼‰ã«ã‚ˆã£ã¦ç®—å®šã•ã‚Œã‚‹ï¼\nä»£è¡¨çš„ãªè³‡æœ¬: ãƒ’ãƒˆãƒ»ãƒ¢ãƒãƒ»ã‚«ãƒ\nè³‡æœ¬ã®åˆ†é¡ã«ã¤ã„ã¦ã¯ï¼Œæ³•çš„åˆ†é¡ã‚„ä¼šè¨ˆå­¦çš„åˆ†é¡ãªã©ç¨®ã€…ã§ã‚ã‚‹ï¼ã“ã“ã§ã¯ï¼Œè³‡æœ¬ã‚’ãƒ’ãƒˆãƒ»ãƒ¢ãƒãƒ»ã‚«ãƒã«åˆ†ã‘ã¦ï¼Œãã®å…·ä½“ä¾‹ã‚’è€ƒãˆã¦ã¿ã‚‹ï¼\nï¼Šä»£è¡¨çš„ãªè³‡æœ¬ï¼ˆ=ç§çš„è²¡ç”£ï¼‰\nãƒ’ãƒˆï¼ˆå€‹äººï¼‰ï¼šæ³•çš„åˆ¶ç´„ã‚’æ ¹æ‹ ã¨ã—ãŸä¾¡å€¤ã‚’æŒã¤ï¼\nâ€‹\tcf) çµŒæ¸ˆå–å¼•ã«ãŠã‘ã‚‹è¡Œå‹•ä¸»ä½“ã€€cf) æ³•äºº\nãƒ¢ãƒï¼ˆæ¶ˆè²»è²¡ï¼Œç”Ÿç”£è²¡ï¼‰ï¼šç‰©ç†çš„åˆ¶ç´„ã‚’æ ¹æ‹ ã¨ã—ãŸä¾¡å€¤ã‚’ã‚‚ã¤ï¼\nâ€‹\tcf) ã‚µãƒ¼ãƒ“ã‚¹ã‚‚å«ã‚€ã€€exï¼‰é‡‘ãƒ»çŸ³æ²¹ãƒ»åœŸåœ°ã€€ã‚«ãƒï¼ˆç¾é‡‘ï¼Œæœ‰ä¾¡è¨¼åˆ¸ï¼‰ï¼šãƒ¢ãƒãƒ»ãƒ’ãƒˆã¨ã®äº¤æ›å¯èƒ½æ€§ã«ã‚ˆã‚Šä¾¡å€¤ã‚’æŒã¤ï¼\nï¼Šä»£è¡¨çš„ãªå¸‚å ´ï¼ˆ=ä¾¡å€¤ã‚’æ¸¬ã‚‹è©•ä¾¡é–¢æ•°ï¼‰\nãƒ’ãƒˆï¼šåŠ´åƒå¸‚å ´/é›‡ç”¨å¸‚å ´ ãƒ¢ãƒï¼šè²¡å¸‚å ´ã€€ã‚«ãƒï¼šè³‡æœ¬å¸‚å ´/é‡‘èå¸‚å ´ ï¼Šå¤‰æ›è¦å‰‡\nãƒ’ãƒˆâ†’ã‚«ãƒï¼ˆåŠ´åƒï¼‰\nã‚«ãƒâ†’ãƒ¢ãƒï¼ˆæ¶ˆè²»ï¼‰\nãƒ¢ãƒâ†’ãƒ’ãƒˆï¼ˆæ•™è‚²ï¼‰ï¼šã“ã“ã§ã„ã†ãƒ¢ãƒ/ã‚µãƒ¼ãƒ“ã‚¹ã¨ã¯ï¼ŒçŸ¥è­˜ã‚„ã‚¹ã‚­ãƒ«ã‚’æŒ‡ã™\nãƒ¢ãƒâ†’ã‚«ãƒï¼ˆå£²å´ï¼‰ï¼šã»ã¨ã‚“ã©ã®ãƒ¢ãƒã¯ï¼Œè©•ä¾¡ã•ã‚Œãªã„ï¼ˆä¸­å¤å“å¸‚å ´ï¼‰\nâ†’é©åˆ‡ãªè¡Œå‹•é¸æŠ\nï¼Šå¸‚å ´ã«æ ¹ã–ã—ãŸä¿æœ‰è³‡æœ¬ã®ãƒã‚¸ã‚·ãƒ§ãƒ‹ãƒ³ã‚°\nãƒ’ãƒˆï¼ˆå°±è·ï¼Œè»¢è·ï¼Œå‰¯æ¥­ï¼‰\nãƒ¢ãƒï¼ˆã›ã©ã‚Šï¼Œè»Šã‚„å®¶ï¼Œéºç”£ç›¸ç¶šï¼‰\nã‚«ãƒï¼ˆãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªè¨­è¨ˆï¼‰\nâ†’é©åˆ‡ãªç¾çŠ¶åˆ†æ\nï¼Šç”Ÿç”£åŠ›(=ä¾¡å€¤)ã¯ï¼ŒçµŒéæ™‚é–“ã«å¯¾ã—ã¦æˆé•·/æ¸›è¡°ã™ã‚‹\nãƒ’ãƒˆã®ä¾¡å€¤ï¼šUP/DOWNï¼ˆçŸ¥è­˜ã¨æŠ€è¡“ï¼Œç®¡ç†ï¼Œäººé–“é–¢ä¿‚ã®è³ª/é‡ï¼‰\nãƒ¢ãƒã®ä¾¡å€¤ï¼šUP/DOWNï¼ˆéª¨è‘£å“ã‚„å¤é…’ï¼ŒçµŒå¹´åŠ£åŒ–ï¼‰\nã‚«ãƒã®ä¾¡å€¤ï¼šUP/DOWNï¼ˆç‰©ä¾¡ã¨é‡‘åˆ©ï¼ŒçµŒæ¸ˆæˆé•·ã¨æ ªä¾¡ï¼‰\nâ†’é©åˆ‡ãªæœªæ¥äºˆæ¸¬\nï¼Šãªãœï¼Œé‡‘èå¸‚å ´ã«ç€ç›®ã™ã‚‹ã¹ãã‹ï¼Ÿ\nã‚«ãƒã¯ï¼Œæ³•çš„ãƒ»ç‰©ç†çš„åˆ¶ç´„ã®å½±éŸ¿ãŒã‚‚ã£ã¨ã‚‚å°ã•ã„ï¼ ãƒ’ãƒˆã¯æ³•çš„åˆ¶ç´„ã‚’å—ã‘ã‚‹ï¼šEx.)åŠ´åƒæ™‚é–“ã®ä¸Šé™ï¼Œå‹¤å‹™åœ°ã®é¸æŠ ãƒ¢ãƒã¯ç‰©ç†çš„åˆ¶ç´„ã‚’å—ã‘ã‚‹ï¼šEx.)çŸ³æ²¹åŸ‹è”µé‡ã®ä¸Šé™ï¼Œäººå£ä¸Šé™ â†’ã‚«ãƒã®å¸‚å ´ä¾¡å€¤ã¯ï¼Œå®šå¸¸çŠ¶æ…‹ã«åæŸã—ãªã„ãŸã‚ï¼Œã‚‚ã£ã¨ã‚‚å¤‰åŒ–ãŒæ¿€ã—ã„ï¼\n","date":1561075200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561075200,"objectID":"202c2d8d41279ef86362d59a029b53ec","permalink":"https://yumauchiumi.com/post/2020-04-05-adaptation-strategies-for-capitalism/","publishdate":"2019-06-21T00:00:00Z","relpermalink":"/post/2020-04-05-adaptation-strategies-for-capitalism/","section":"post","summary":"Thinking of adaptation strategies for the capitalism","tags":["System","Capitalism"],"title":"A Capitalist's Game","type":"post"},{"authors":null,"categories":["Random"],"content":"Have you ever want to login to keio.jp automatically? Don\u0026rsquo;t you think it is cool? At least I think so and I write down the way to achieve that with Python.\n","date":1561075200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561075200,"objectID":"ca9f10c0f3d78d0df17137a403cb2bff","permalink":"https://yumauchiumi.com/post/2019-06-21-keiojp-auto-login/","publishdate":"2019-06-21T00:00:00Z","relpermalink":"/post/2019-06-21-keiojp-auto-login/","section":"post","summary":"Have you ever want to login to keio.jp automatically? Don\u0026rsquo;t you think it is cool? At least I think so and I write down the way to achieve that with Python.","tags":["Tips","Python"],"title":"Automatically Login to keio.jp using Selenium on Python","type":"post"},{"authors":null,"categories":["Dev"],"content":"TL;DR Use getLocalGraphicsEnvironment().getAllFonts(); method defined in java.awt.GraphicsEnvironment class.\nCf. https://docs.oracle.com/javase/jp/8/docs/api/java/awt/GraphicsEnvironment.html Code Sample (Java):\nimport java.awt.Font; import java.awt.GraphicsEnvironment; public class Main { public static void main(String[] args) throws Exception { Font[] fonts = GraphicsEnvironment.getLocalGraphicsEnvironment().getAllFonts(); for (int i = 0; i \u0026lt; fonts.length; i++) { System.out.print(fonts[i].getFontName() + \u0026quot;, \u0026quot;); System.out.print(fonts[i].getFamily() + \u0026quot;, \u0026quot;); System.out.print(fonts[i].getName()); System.out.println(); } } } Output Result:\nSerif, Serif, Serif, SansSerif, SansSerif, SansSerif, Monospaced, Monospaced, Monospaced, Dialog, Dialog, Dialog, DialogInput, DialogInput, DialogInput, .SFNSText, .SF NS Text, .SFNSText, .SFNSText-Bold, .SF NS Text, .SFNSText-Bold, AlBayan, Al Bayan, AlBayan, AlBayan-Bold, Al Bayan, AlBayan-Bold, AlNile, Al Nile, AlNile, AlNile-Bold, Al Nile, AlNile-Bold, AlTarikh, Al Tarikh, AlTarikh, AmericanTypewriter, American Typewriter, AmericanTypewriter, AmericanTypewriter-Bold, American Typewriter, AmericanTypewriter-Bold, AmericanTypewriter-Condensed, American Typewriter, AmericanTypewriter-Condensed, AmericanTypewriter-CondensedBold, American Typewriter, AmericanTypewriter-CondensedBold, AmericanTypewriter-CondensedLight, American Typewriter, AmericanTypewriter-CondensedLight, AmericanTypewriter-Light, American Typewriter, AmericanTypewriter-Light, AmericanTypewriter-Semibold, American Typewriter, AmericanTypewriter-Semibold, AndaleMono, Andale Mono, AndaleMono, Apple-Chancery, Apple Chancery, Apple-Chancery, AppleBraille, Apple Braille, AppleBraille, AppleBraille-Outline6Dot, Apple Braille, AppleBraille-Outline6Dot, AppleBraille-Outline8Dot, Apple Braille, AppleBraille-Outline8Dot, AppleBraille-Pinpoint6Dot, Apple Braille, AppleBraille-Pinpoint6Dot, AppleBraille-Pinpoint8Dot, Apple Braille, AppleBraille-Pinpoint8Dot, AppleColorEmoji, Apple Color Emoji, AppleColorEmoji, AppleGothic, AppleGothic, AppleGothic, AppleMyungjo, AppleMyungjo, AppleMyungjo, AppleSDGothicNeo-Bold, Apple SD Gothic Neo, AppleSDGothicNeo-Bold, AppleSDGothicNeo-ExtraBold, Apple SD Gothic Neo, AppleSDGothicNeo-ExtraBold, AppleSDGothicNeo-Heavy, Apple SD Gothic Neo, AppleSDGothicNeo-Heavy, AppleSDGothicNeo-Light, Apple SD Gothic Neo, AppleSDGothicNeo-Light, AppleSDGothicNeo-Medium, Apple SD Gothic Neo, AppleSDGothicNeo-Medium, AppleSDGothicNeo-Regular, Apple SD Gothic Neo, AppleSDGothicNeo-Regular, AppleSDGothicNeo-SemiBold, Apple SD Gothic Neo, AppleSDGothicNeo-SemiBold, AppleSDGothicNeo-Thin, Apple SD Gothic Neo, AppleSDGothicNeo-Thin, AppleSDGothicNeo-UltraLight, Apple SD Gothic Neo, AppleSDGothicNeo-UltraLight, AppleSymbols, Apple Symbols, AppleSymbols, Arial-Black, Arial Black, Arial-Black, Arial-BoldItalicMT, Arial, Arial-BoldItalicMT, Arial-BoldMT, Arial, Arial-BoldMT, Arial-ItalicMT, Arial, Arial-ItalicMT, ArialHebrew, Arial Hebrew, ArialHebrew, ArialHebrew-Bold, Arial Hebrew, ArialHebrew-Bold, ArialHebrew-Light, Arial Hebrew, ArialHebrew-Light, ArialHebrewScholar, Arial Hebrew Scholar, ArialHebrewScholar, ArialHebrewScholar-Bold, Arial Hebrew Scholar, ArialHebrewScholar-Bold, ArialHebrewScholar-Light, Arial Hebrew Scholar, ArialHebrewScholar-Light, ArialMT, Arial, ArialMT, ArialNarrow, Arial Narrow, ArialNarrow, ArialNarrow-Bold, Arial Narrow, ArialNarrow-Bold, ArialNarrow-BoldItalic, Arial Narrow, ArialNarrow-BoldItalic, ArialNarrow-Italic, Arial Narrow, ArialNarrow-Italic, ArialRoundedMTBold, Arial Rounded MT Bold, ArialRoundedMTBold, ArialUnicodeMS, Arial Unicode MS, ArialUnicodeMS, Athelas-Bold, Athelas, Athelas-Bold, Athelas-BoldItalic, Athelas, Athelas-BoldItalic, Athelas-Italic, Athelas, Athelas-Italic, Athelas-Regular, Athelas, Athelas-Regular, Avenir-Black, Avenir, Avenir-Black, Avenir-BlackOblique, Avenir, Avenir-BlackOblique, Avenir-Book, Avenir, Avenir-Book, Avenir-BookOblique, Avenir, Avenir-BookOblique, Avenir-Heavy, Avenir, Avenir-Heavy, Avenir-HeavyOblique, Avenir, Avenir-HeavyOblique, Avenir-Light, Avenir, Avenir-Light, Avenir-LightOblique, Avenir, Avenir-LightOblique, Avenir-Medium, Avenir, Avenir-Medium, Avenir-MediumOblique, Avenir, Avenir-MediumOblique, Avenir-Oblique, Avenir, Avenir-Oblique, Avenir-Roman, Avenir, Avenir-Roman, AvenirNext-Bold, Avenir Next, AvenirNext-Bold, AvenirNext-BoldItalic, Avenir Next, AvenirNext-BoldItalic, AvenirNext-DemiBold, Avenir Next, AvenirNext-DemiBold, AvenirNext-DemiBoldItalic, Avenir Next, AvenirNext-DemiBoldItalic, AvenirNext-Heavy, Avenir Next, AvenirNext-Heavy, AvenirNext-HeavyItalic, Avenir Next, AvenirNext-HeavyItalic, AvenirNext-Italic, Avenir Next, AvenirNext-Italic, AvenirNext-Medium, Avenir Next, AvenirNext-Medium, AvenirNext-MediumItalic, Avenir Next, AvenirNext-MediumItalic, AvenirNext-Regular, Avenir Next, AvenirNext-Regular, AvenirNext-UltraLight, Avenir Next, AvenirNext-UltraLight, AvenirNext-UltraLightItalic, Avenir Next, AvenirNext-UltraLightItalic, AvenirNextCondensed-Bold, Avenir Next Condensed, AvenirNextCondensed-Bold, AvenirNextCondensed-BoldItalic, Avenir Next Condensed, AvenirNextCondensed-BoldItalic, AvenirNextCondensed-DemiBold, Avenir Next Condensed, AvenirNextCondensed-DemiBold, AvenirNextCondensed-DemiBoldItalic, Avenir Next Condensed, AvenirNextCondensed-DemiBoldItalic, AvenirNextCondensed-Heavy, Avenir Next Condensed, AvenirNextCondensed-Heavy, AvenirNextCondensed-HeavyItalic, Avenir Next Condensed, AvenirNextCondensed-HeavyItalic, AvenirNextCondensed-Italic, Avenir Next Condensed, AvenirNextCondensed-Italic, AvenirNextCondensed-Medium, Avenir Next Condensed, AvenirNextCondensed-Medium, AvenirNextCondensed-MediumItalic, Avenir Next Condensed, AvenirNextCondensed-MediumItalic, AvenirNextCondensed-Regular, Avenir Next Condensed, AvenirNextCondensed-Regular, AvenirNextCondensed-UltraLight, Avenir Next Condensed, AvenirNextCondensed-UltraLight, AvenirNextCondensed-UltraLightItalic, Avenir Next Condensed, AvenirNextCondensed-UltraLightItalic, Ayuthaya, Ayuthaya, Ayuthaya, Baghdad, Baghdad, Baghdad, BanglaMN, Bangla MN, BanglaMN, BanglaMN-Bold, Bangla MN, BanglaMN-Bold, BanglaSangamMN, Bangla Sangam MN, BanglaSangamMN, BanglaSangamMN-Bold, Bangla Sangam MN, BanglaSangamMN-Bold, Baskerville, Baskerville, Baskerville, Baskerville-Bold, Baskerville, Baskerville-Bold, Baskerville-BoldItalic, Baskerville, Baskerville-BoldItalic, Baskerville-Italic, Baskerville, Baskerville-Italic, Baskerville-SemiBold, Baskerville, Baskerville-SemiBold, Baskerville-SemiBoldItalic, Baskerville, Baskerville-SemiBoldItalic, Beirut, Beirut, Beirut, BigCaslon-Medium, Big Caslon, BigCaslon-Medium, BodoniOrnamentsITCTT, Bodoni Ornaments, BodoniOrnamentsITCTT, BodoniSvtyTwoITCTT-Bold, Bodoni 72, BodoniSvtyTwoITCTT-Bold, BodoniSvtyTwoITCTT-Book, Bodoni 72, BodoniSvtyTwoITCTT-Book, BodoniSvtyTwoITCTT-BookIta, Bodoni 72, BodoniSvtyTwoITCTT-BookIta, BodoniSvtyTwoOSITCTT-Bold, Bodoni 72 Oldstyle, BodoniSvtyTwoOSITCTT-Bold, BodoniSvtyTwoOSITCTT-Book, Bodoni 72 Oldstyle, BodoniSvtyTwoOSITCTT-Book, BodoniSvtyTwoOSITCTT-BookIt, Bodoni 72 Oldstyle, BodoniSvtyTwoOSITCTT-BookIt, BodoniSvtyTwoSCITCTT-Book, Bodoni 72 Smallcaps, BodoniSvtyTwoSCITCTT-Book, BradleyHandITCTT-Bold, Bradley Hand, BradleyHandITCTT-Bold, BrushScriptMT, Brush Script MT, BrushScriptMT, Chalkboard, Chalkboard, Chalkboard, Chalkboard-Bold, Chalkboard, Chalkboard-Bold, ChalkboardSE-Bold, Chalkboard SE, ChalkboardSE-Bold, ChalkboardSE-Light, Chalkboard SE, ChalkboardSE-Light, ChalkboardSE-Regular, Chalkboard SE, ChalkboardSE-Regular, Chalkduster, Chalkduster, Chalkduster, Charter-Black, Charter, Charter-Black, Charter-BlackItalic, Charter, Charter-BlackItalic, Charter-Bold, Charter, Charter-Bold, Charter-BoldItalic, Charter, Charter-BoldItalic, Charter-Italic, Charter, Charter-Italic, Charter-Roman, Charter, Charter-Roman, Cochin, Cochin, Cochin, Cochin-Bold, Cochin, Cochin-Bold, Cochin-BoldItalic, Cochin, Cochin-BoldItalic, Cochin-Italic, Cochin, Cochin-Italic, ComicSansMS, Comic Sans MS, ComicSansMS, ComicSansMS-Bold, Comic Sans MS, ComicSansMS-Bold, Copperplate, Copperplate, Copperplate, Copperplate-Bold, Copperplate, Copperplate-Bold, Copperplate-Light, Copperplate, Copperplate-Light, CorsivaHebrew, Corsiva Hebrew, CorsivaHebrew, CorsivaHebrew-Bold, Corsiva Hebrew, CorsivaHebrew-Bold, Courier, Courier, Courier, Courier-Bold, Courier, Courier-Bold, Courier-BoldOblique, Courier, Courier-BoldOblique, Courier-Oblique, Courier, Courier-Oblique, CourierNewPS-BoldItalicMT, Courier New, CourierNewPS-BoldItalicMT, CourierNewPS-BoldMT, Courier New, CourierNewPS-BoldMT, CourierNewPS-ItalicMT, Courier New, CourierNewPS-ItalicMT, CourierNewPSMT, Courier New, CourierNewPSMT, DFKaiShu-SB-Estd-BF, BiauKai, DFKaiShu-SB-Estd-BF, DFWaWaSC-W5, Wawati SC, DFWaWaSC-W5, DFWaWaTC-W5, Wawati TC, DFWaWaTC-W5, DINAlternate-Bold, DIN Alternate, DINAlternate-Bold, DINCondensed-Bold, DIN Condensed, DINCondensed-Bold, Damascus, Damascus, Damascus, DamascusBold, Damascus, DamascusBold, DamascusLight, Damascus, DamascusLight, DamascusMedium, Damascus, DamascusMedium, DamascusSemiBold, Damascus, DamascusSemiBold, DecoTypeNaskh, DecoType Naskh, DecoTypeNaskh, DevanagariMT, Devanagari MT, DevanagariMT, DevanagariMT-Bold, Devanagari MT, DevanagariMT-Bold, DevanagariSangamMN, Devanagari Sangam MN, DevanagariSangamMN, DevanagariSangamMN-Bold, Devanagari Sangam MN, DevanagariSangamMN-Bold, Didot, Didot, Didot, Didot-Bold, Didot, Didot-Bold, Didot-Italic, Didot, Didot-Italic, DiwanKufi, Diwan Kufi, DiwanKufi, DiwanMishafi, Mishafi, DiwanMishafi, DiwanMishafiGold, Mishafi Gold, DiwanMishafiGold, DiwanThuluth, Diwan Thuluth, DiwanThuluth, EuphemiaUCAS, Euphemia UCAS, EuphemiaUCAS, EuphemiaUCAS-Bold, Euphemia UCAS, EuphemiaUCAS-Bold, EuphemiaUCAS-Italic, Euphemia UCAS, EuphemiaUCAS-Italic, FZLTTHB--B51-0, Lantinghei TC, FZLTTHB--B51-0, FZLTTHK--GBK1-0, Lantinghei SC, FZLTTHK--GBK1-0, FZLTXHB--B51-0, Lantinghei TC, FZLTXHB--B51-0, FZLTXHK--GBK1-0, Lantinghei SC, FZLTXHK--GBK1-0, FZLTZHB--B51-0, Lantinghei TC, FZLTZHB--B51-0, FZLTZHK--GBK1-0, Lantinghei SC, FZLTZHK--GBK1-0, Farah, Farah, Farah, Farisi, Farisi, Farisi, Futura-Bold, Futura, Futura-Bold, Futura-CondensedExtraBold, Futura, Futura-CondensedExtraBold, Futura-CondensedMedium, Futura, Futura-CondensedMedium, Futura-Medium, Futura, Futura-Medium, Futura-MediumItalic, Futura, Futura-MediumItalic, GB18030Bitmap, GB18030 Bitmap, GB18030Bitmap, GeezaPro, Geeza Pro, GeezaPro, GeezaPro-Bold, Geeza Pro, GeezaPro-Bold, Geneva, Geneva, Geneva, Georgia, Georgia, Georgia, Georgia-Bold, Georgia, Georgia-Bold, Georgia-BoldItalic, Georgia, Georgia-BoldItalic, Georgia-Italic, Georgia, Georgia-Italic, GillSans, Gill Sans, GillSans, GillSans-Bold, Gill Sans, GillSans-Bold, GillSans-BoldItalic, Gill Sans, GillSans-BoldItalic, GillSans-Italic, Gill Sans, GillSans-Italic, GillSans-Light, Gill Sans, GillSans-Light, GillSans-LightItalic, Gill Sans, GillSans-LightItalic, GillSans-SemiBold, Gill Sans, GillSans-SemiBold, GillSans-SemiBoldItalic, Gill Sans, GillSans-SemiBoldItalic, GillSans-UltraBold, Gill Sans, GillSans-UltraBold, GujaratiMT, Gujarati MT, GujaratiMT, GujaratiMT-Bold, Gujarati MT, GujaratiMT-Bold, GujaratiSangamMN, Gujarati Sangam MN, GujaratiSangamMN, GujaratiSangamMN-Bold, Gujarati Sangam MN, GujaratiSangamMN-Bold, GurmukhiMN, Gurmukhi MN, GurmukhiMN, GurmukhiMN-Bold, Gurmukhi MN, GurmukhiMN-Bold, GurmukhiSangamMN, Gurmukhi Sangam MN, GurmukhiSangamMN, GurmukhiSangamMN-Bold, Gurmukhi Sangam MN, GurmukhiSangamMN-Bold, HannotateSC-W5, Hannotate SC, HannotateSC-W5, HannotateSC-W7, Hannotate SC, HannotateSC-W7, HannotateTC-W5, Hannotate TC, HannotateTC-W5, HannotateTC-W7, Hannotate TC, HannotateTC-W7, HanziPenSC-W3, HanziPen SC, HanziPenSC-W3, HanziPenSC-W5, HanziPen SC, HanziPenSC-W5, HanziPenTC-W3, HanziPen TC, HanziPenTC-W3, HanziPenTC-W5, HanziPen TC, HanziPenTC-W5, Helvetica, Helvetica, Helvetica, Helvetica-Bold, Helvetica, Helvetica-Bold, Helvetica-BoldOblique, Helvetica, Helvetica-BoldOblique, Helvetica-Light, Helvetica, Helvetica-Light, Helvetica-LightOblique, Helvetica, Helvetica-LightOblique, Helvetica-Oblique, Helvetica, Helvetica-Oblique, HelveticaNeue, Helvetica Neue, HelveticaNeue, HelveticaNeue-Bold, Helvetica Neue, HelveticaNeue-Bold, HelveticaNeue-BoldItalic, Helvetica Neue, HelveticaNeue-BoldItalic, HelveticaNeue-CondensedBlack, Helvetica Neue, HelveticaNeue-CondensedBlack, HelveticaNeue-CondensedBold, Helvetica Neue, HelveticaNeue-CondensedBold, HelveticaNeue-Italic, Helvetica Neue, HelveticaNeue-Italic, HelveticaNeue-Light, Helvetica Neue, HelveticaNeue-Light, HelveticaNeue-LightItalic, Helvetica Neue, HelveticaNeue-LightItalic, HelveticaNeue-Medium, Helvetica Neue, HelveticaNeue-Medium, HelveticaNeue-MediumItalic, Helvetica Neue, HelveticaNeue-MediumItalic, HelveticaNeue-Thin, Helvetica Neue, HelveticaNeue-Thin, HelveticaNeue-ThinItalic, Helvetica Neue, HelveticaNeue-ThinItalic, HelveticaNeue-UltraLight, Helvetica Neue, HelveticaNeue-UltraLight, HelveticaNeue-UltraLightItalic, Helvetica Neue, HelveticaNeue-UltraLightItalic, Herculanum, Herculanum, Herculanum, HiraKakuPro-W3, Hiragino Kaku Gothic Pro, HiraKakuPro-W3, HiraKakuPro-W6, Hiragino Kaku Gothic Pro, HiraKakuPro-W6, HiraKakuProN-W3, Hiragino Kaku Gothic ProN, HiraKakuProN-W3, HiraKakuProN-W6, Hiragino Kaku Gothic ProN, HiraKakuProN-W6, HiraKakuStd-W8, Hiragino Kaku Gothic Std, HiraKakuStd-W8, HiraKakuStdN-W8, Hiragino Kaku Gothic StdN, HiraKakuStdN-W8, HiraMaruPro-W4, Hiragino Maru Gothic Pro, HiraMaruPro-W4, HiraMaruProN-W4, Hiragino Maru Gothic ProN, HiraMaruProN-W4, HiraMinPro-W3, Hiragino Mincho Pro, HiraMinPro-W3, HiraMinPro-W6, Hiragino Mincho Pro, HiraMinPro-W6, HiraMinProN-W3, Hiragino Mincho ProN, HiraMinProN-W3, HiraMinProN-W6, Hiragino Mincho ProN, HiraMinProN-W6, HiraginoSans-W0, Hiragino Sans, HiraginoSans-W0, HiraginoSans-W1, Hiragino Sans, HiraginoSans-W1, HiraginoSans-W2, Hiragino Sans, HiraginoSans-W2, HiraginoSans-W3, Hiragino Sans, HiraginoSans-W3, HiraginoSans-W4, Hiragino Sans, HiraginoSans-W4, HiraginoSans-W5, Hiragino Sans, HiraginoSans-W5, HiraginoSans-W6, Hiragino Sans, HiraginoSans-W6, HiraginoSans-W7, Hiragino Sans, HiraginoSans-W7, HiraginoSans-W8, Hiragino Sans, HiraginoSans-W8, HiraginoSans-W9, Hiragino Sans, HiraginoSans-W9, HiraginoSansCNS-W3, Hiragino Sans CNS, HiraginoSansCNS-W3, HiraginoSansCNS-W6, Hiragino Sans CNS, HiraginoSansCNS-W6, HiraginoSansGB-W3, Hiragino Sans GB, HiraginoSansGB-W3, HiraginoSansGB-W6, Hiragino Sans GB, HiraginoSansGB-W6, HoeflerText-Black, Hoefler Text, HoeflerText-Black, HoeflerText-BlackItalic, Hoefler Text, HoeflerText-BlackItalic, HoeflerText-Italic, Hoefler Text, HoeflerText-Italic, HoeflerText-Ornaments, Hoefler Text, HoeflerText-Ornaments, HoeflerText-Regular, Hoefler Text, HoeflerText-Regular, ITFDevanagari-Bold, ITF Devanagari, ITFDevanagari-Bold, ITFDevanagari-Book, ITF Devanagari, ITFDevanagari-Book, ITFDevanagari-Demi, ITF Devanagari, ITFDevanagari-Demi, ITFDevanagari-Light, ITF Devanagari, ITFDevanagari-Light, ITFDevanagari-Medium, ITF Devanagari, ITFDevanagari-Medium, ITFDevanagariMarathi-Bold, ITF Devanagari Marathi, ITFDevanagariMarathi-Bold, ITFDevanagariMarathi-Book, ITF Devanagari Marathi, ITFDevanagariMarathi-Book, ITFDevanagariMarathi-Demi, ITF Devanagari Marathi, ITFDevanagariMarathi-Demi, ITFDevanagariMarathi-Light, ITF Devanagari Marathi, ITFDevanagariMarathi-Light, ITFDevanagariMarathi-Medium, ITF Devanagari Marathi, ITFDevanagariMarathi-Medium, Impact, Impact, Impact, InaiMathi, InaiMathi, InaiMathi, InaiMathi-Bold, InaiMathi, InaiMathi-Bold, IowanOldStyle-Black, Iowan Old Style, IowanOldStyle-Black, IowanOldStyle-BlackItalic, Iowan Old Style, IowanOldStyle-BlackItalic, IowanOldStyle-Bold, Iowan Old Style, IowanOldStyle-Bold, IowanOldStyle-BoldItalic, Iowan Old Style, IowanOldStyle-BoldItalic, IowanOldStyle-Italic, Iowan Old Style, IowanOldStyle-Italic, IowanOldStyle-Roman, Iowan Old Style, IowanOldStyle-Roman, IowanOldStyle-Titling, Iowan Old Style, IowanOldStyle-Titling, JCHEadA, HeadLineA, JCHEadA, JCfg, PilGi, JCfg, JCkg, GungSeo, JCkg, JCsmPC, PCMyungjo, JCsmPC, Kailasa, Kailasa, Kailasa, Kailasa-Bold, Kailasa, Kailasa-Bold, KannadaMN, Kannada MN, KannadaMN, KannadaMN-Bold, Kannada MN, KannadaMN-Bold, KannadaSangamMN, Kannada Sangam MN, KannadaSangamMN, KannadaSangamMN-Bold, Kannada Sangam MN, KannadaSangamMN-Bold, Kefa-Bold, Kefa, Kefa-Bold, Kefa-Regular, Kefa, Kefa-Regular, KhmerMN, Khmer MN, KhmerMN, KhmerMN-Bold, Khmer MN, KhmerMN-Bold, KhmerSangamMN, Khmer Sangam MN, KhmerSangamMN, Klee-Demibold, Klee, Klee-Demibold, Klee-Medium, Klee, Klee-Medium, KohinoorBangla-Bold, Kohinoor Bangla, KohinoorBangla-Bold, KohinoorBangla-Light, Kohinoor Bangla, KohinoorBangla-Light, KohinoorBangla-Medium, Kohinoor Bangla, KohinoorBangla-Medium, KohinoorBangla-Regular, Kohinoor Bangla, KohinoorBangla-Regular, KohinoorBangla-Semibold, Kohinoor Bangla, KohinoorBangla-Semibold, KohinoorDevanagari-Bold, Kohinoor Devanagari, KohinoorDevanagari-Bold, KohinoorDevanagari-Light, Kohinoor Devanagari, KohinoorDevanagari-Light, KohinoorDevanagari-Medium, Kohinoor Devanagari, KohinoorDevanagari-Medium, KohinoorDevanagari-Regular, Kohinoor Devanagari, KohinoorDevanagari-Regular, KohinoorDevanagari-Semibold, Kohinoor Devanagari, KohinoorDevanagari-Semibold, KohinoorTelugu-Bold, Kohinoor Telugu, KohinoorTelugu-Bold, KohinoorTelugu-Light, Kohinoor Telugu, KohinoorTelugu-Light, KohinoorTelugu-Medium, Kohinoor Telugu, KohinoorTelugu-Medium, KohinoorTelugu-Regular, Kohinoor Telugu, KohinoorTelugu-Regular, KohinoorTelugu-Semibold, Kohinoor Telugu, KohinoorTelugu-Semibold, Kokonor, Kokonor, Kokonor, Krungthep, Krungthep, Krungthep, KufiStandardGK, KufiStandardGK, KufiStandardGK, LaoMN, Lao MN, LaoMN, LaoMN-Bold, Lao MN, LaoMN-Bold, LaoSangamMN, Lao Sangam MN, LaoSangamMN, LiGothicMed, Apple LiGothic, LiGothicMed, LiHeiPro, LiHei Pro, LiHeiPro, LiSongPro, LiSong Pro, LiSongPro, LiSungLight, Apple LiSung, LiSungLight, Lucida Bright Demibold, Lucida Bright, Lucida Bright Demibold, Lucida Bright Demibold Italic, Lucida Bright, Lucida Bright Demibold Italic, Lucida Bright Italic, Lucida Bright, Lucida Bright Italic, Lucida Bright Regular, Lucida Bright, Lucida Bright Regular, Lucida Sans Demibold, Lucida Sans, Lucida Sans Demibold, Lucida Sans Regular, Lucida Sans, Lucida Sans Regular, Lucida Sans Typewriter Bold, Lucida Sans Typewriter, Lucida Sans Typewriter Bold, Lucida Sans Typewriter Regular, Lucida Sans Typewriter, Lucida Sans Typewriter Regular, LucidaBright, Lucida Bright, LucidaBright, LucidaBright-Demi, Lucida Bright, LucidaBright-Demi, LucidaBright-DemiItalic, Lucida Bright, LucidaBright-DemiItalic, LucidaBright-Italic, Lucida Bright, LucidaBright-Italic, LucidaGrande, Lucida Grande, LucidaGrande, LucidaGrande-Bold, Lucida Grande, LucidaGrande-Bold, LucidaSans, Lucida Sans, LucidaSans, LucidaSans-Demi, Lucida Sans, LucidaSans-Demi, LucidaSans-Typewriter, Lucida Sans Typewriter, LucidaSans-Typewriter, LucidaSans-TypewriterBold, Lucida Sans Typewriter, LucidaSans-TypewriterBold, Luminari-Regular, Luminari, Luminari-Regular, MLingWaiMedium-SC, LingWai SC, MLingWaiMedium-SC, MLingWaiMedium-TC, LingWai TC, MLingWaiMedium-TC, MalayalamMN, Malayalam MN, MalayalamMN, MalayalamMN-Bold, Malayalam MN, MalayalamMN-Bold, MalayalamSangamMN, Malayalam Sangam MN, MalayalamSangamMN, MalayalamSangamMN-Bold, Malayalam Sangam MN, MalayalamSangamMN-Bold, Marion-Bold, Marion, Marion-Bold, Marion-Italic, Marion, Marion-Italic, Marion-Regular, Marion, Marion-Regular, MarkerFelt-Thin, Marker Felt, MarkerFelt-Thin, MarkerFelt-Wide, Marker Felt, MarkerFelt-Wide, Menlo-Bold, Menlo, Menlo-Bold, Menlo-BoldItalic, Menlo, Menlo-BoldItalic, Menlo-Italic, Menlo, Menlo-Italic, Menlo-Regular, Menlo, Menlo-Regular, MicrosoftSansSerif, Microsoft Sans Serif, MicrosoftSansSerif, Monaco, Monaco, Monaco, MonotypeGurmukhi, Gurmukhi MT, MonotypeGurmukhi, Mshtakan, Mshtakan, Mshtakan, MshtakanBold, Mshtakan, MshtakanBold, MshtakanBoldOblique, Mshtakan, MshtakanBoldOblique, MshtakanOblique, Mshtakan, MshtakanOblique, Muna, Muna, Muna, MunaBlack, Muna, MunaBlack, MunaBold, Muna, MunaBold, MyanmarMN, Myanmar MN, MyanmarMN, MyanmarMN-Bold, Myanmar MN, MyanmarMN-Bold, MyanmarSangamMN, Myanmar Sangam MN, MyanmarSangamMN, MyanmarSangamMN-Bold, Myanmar Sangam MN, MyanmarSangamMN-Bold, Nadeem, Nadeem, Nadeem, NanumBrush, Nanum Brush Script, NanumBrush, NanumGothic, Nanum Gothic, NanumGothic, NanumGothicBold, Nanum Gothic, NanumGothicBold, NanumGothicExtraBold, Nanum Gothic, NanumGothicExtraBold, NanumMyeongjo, Nanum Myeongjo, NanumMyeongjo, NanumMyeongjoBold, Nanum Myeongjo, NanumMyeongjoBold, NanumMyeongjoExtraBold, Nanum Myeongjo, NanumMyeongjoExtraBold, NanumPen, Nanum Pen Script, NanumPen, NewPeninimMT, New Peninim MT, NewPeninimMT, NewPeninimMT-Bold, New Peninim MT, NewPeninimMT-Bold, NewPeninimMT-BoldInclined, New Peninim MT, NewPeninimMT-BoldInclined, NewPeninimMT-Inclined, New Peninim MT, NewPeninimMT-Inclined, Noteworthy-Bold, Noteworthy, Noteworthy-Bold, Noteworthy-Light, Noteworthy, Noteworthy-Light, NotoNastaliqUrdu, Noto Nastaliq Urdu, NotoNastaliqUrdu, Optima-Bold, Optima, Optima-Bold, Optima-BoldItalic, Optima, Optima-BoldItalic, Optima-ExtraBlack, Optima, Optima-ExtraBlack, Optima-Italic, Optima, Optima-Italic, Optima-Regular, Optima, Optima-Regular, OriyaMN, Oriya MN, OriyaMN, OriyaMN-Bold, Oriya MN, OriyaMN-Bold, OriyaSangamMN, Oriya Sangam MN, OriyaSangamMN, OriyaSangamMN-Bold, Oriya Sangam MN, OriyaSangamMN-Bold, Osaka, Osaka, Osaka, Osaka-Mono, Osaka, Osaka-Mono, PTMono-Bold, PT Mono, PTMono-Bold, PTMono-Regular, PT Mono, PTMono-Regular, PTSans-Bold, PT Sans, PTSans-Bold, PTSans-BoldItalic, PT Sans, PTSans-BoldItalic, PTSans-Caption, PT Sans Caption, PTSans-Caption, PTSans-CaptionBold, PT Sans Caption, PTSans-CaptionBold, PTSans-Italic, PT Sans, PTSans-Italic, PTSans-Narrow, PT Sans Narrow, PTSans-Narrow, PTSans-NarrowBold, PT Sans Narrow, PTSans-NarrowBold, PTSans-Regular, PT Sans, PTSans-Regular, PTSerif-Bold, PT Serif, PTSerif-Bold, PTSerif-BoldItalic, PT Serif, PTSerif-BoldItalic, PTSerif-Caption, PT Serif Caption, PTSerif-Caption, PTSerif-CaptionItalic, PT Serif Caption, PTSerif-CaptionItalic, PTSerif-Italic, PT Serif, PTSerif-Italic, PTSerif-Regular, PT Serif, PTSerif-Regular, Palatino-Bold, Palatino, Palatino-Bold, Palatino-BoldItalic, Palatino, Palatino-BoldItalic, Palatino-Italic, Palatino, Palatino-Italic, Palatino-Roman, Palatino, Palatino-Roman, Papyrus, Papyrus, Papyrus, Papyrus-Condensed, Papyrus, Papyrus-Condensed, Phosphate-Inline, Phosphate, Phosphate-Inline, Phosphate-Solid, Phosphate, Phosphate-Solid, PingFangHK-Light, PingFang HK, PingFangHK-Light, PingFangHK-Medium, PingFang HK, PingFangHK-Medium, PingFangHK-Regular, PingFang HK, PingFangHK-Regular, PingFangHK-Semibold, PingFang HK, PingFangHK-Semibold, PingFangHK-Thin, PingFang HK, PingFangHK-Thin, PingFangHK-Ultralight, PingFang HK, PingFangHK-Ultralight, PingFangSC-Light, PingFang SC, PingFangSC-Light, PingFangSC-Medium, PingFang SC, PingFangSC-Medium, PingFangSC-Regular, PingFang SC, PingFangSC-Regular, PingFangSC-Semibold, PingFang SC, PingFangSC-Semibold, PingFangSC-Thin, PingFang SC, PingFangSC-Thin, PingFangSC-Ultralight, PingFang SC, PingFangSC-Ultralight, PingFangTC-Light, PingFang TC, PingFangTC-Light, PingFangTC-Medium, PingFang TC, PingFangTC-Medium, PingFangTC-Regular, PingFang TC, PingFangTC-Regular, PingFangTC-Semibold, PingFang TC, PingFangTC-Semibold, PingFangTC-Thin, PingFang TC, PingFangTC-Thin, PingFangTC-Ultralight, PingFang TC, PingFangTC-Ultralight, PlantagenetCherokee, Plantagenet Cherokee, PlantagenetCherokee, Raanana, Raanana, Raanana, RaananaBold, Raanana, RaananaBold, SIL-Hei-Med-Jian, Hei, SIL-Hei-Med-Jian, SIL-Kai-Reg-Jian, Kai, SIL-Kai-Reg-Jian, STBaoliSC-Regular, Baoli SC, STBaoliSC-Regular, STBaoliTC-Regular, Baoli TC, STBaoliTC-Regular, STFangsong, STFangsong, STFangsong, STHeiti, STHeiti, STHeiti, STHeitiSC-Light, Heiti SC, STHeitiSC-Light, STHeitiSC-Medium, Heiti SC, STHeitiSC-Medium, STHeitiTC-Light, Heiti TC, STHeitiTC-Light, STHeitiTC-Medium, Heiti TC, STHeitiTC-Medium, STIXGeneral-Bold, STIXGeneral, STIXGeneral-Bold, STIXGeneral-BoldItalic, STIXGeneral, STIXGeneral-BoldItalic, STIXGeneral-Italic, STIXGeneral, STIXGeneral-Italic, STIXGeneral-Regular, STIXGeneral, STIXGeneral-Regular, STIXIntegralsD-Bold, STIXIntegralsD, STIXIntegralsD-Bold, STIXIntegralsD-Regular, STIXIntegralsD, STIXIntegralsD-Regular, STIXIntegralsSm-Bold, STIXIntegralsSm, STIXIntegralsSm-Bold, STIXIntegralsSm-Regular, STIXIntegralsSm, STIXIntegralsSm-Regular, STIXIntegralsUp-Bold, STIXIntegralsUp, STIXIntegralsUp-Bold, STIXIntegralsUp-Regular, STIXIntegralsUp, STIXIntegralsUp-Regular, STIXIntegralsUpD-Bold, STIXIntegralsUpD, STIXIntegralsUpD-Bold, STIXIntegralsUpD-Regular, STIXIntegralsUpD, STIXIntegralsUpD-Regular, STIXIntegralsUpSm-Bold, STIXIntegralsUpSm, STIXIntegralsUpSm-Bold, STIXIntegralsUpSm-Regular, STIXIntegralsUpSm, STIXIntegralsUpSm-Regular, STIXNonUnicode-Bold, STIXNonUnicode, STIXNonUnicode-Bold, STIXNonUnicode-BoldItalic, STIXNonUnicode, STIXNonUnicode-BoldItalic, STIXNonUnicode-Italic, STIXNonUnicode, STIXNonUnicode-Italic, STIXNonUnicode-Regular, STIXNonUnicode, STIXNonUnicode-Regular, STIXSizeFiveSym-Regular, STIXSizeFiveSym, STIXSizeFiveSym-Regular, STIXSizeFourSym-Bold, STIXSizeFourSym, STIXSizeFourSym-Bold, STIXSizeFourSym-Regular, STIXSizeFourSym, STIXSizeFourSym-Regular, STIXSizeOneSym-Bold, STIXSizeOneSym, STIXSizeOneSym-Bold, STIXSizeOneSym-Regular, STIXSizeOneSym, STIXSizeOneSym-Regular, STIXSizeThreeSym-Bold, STIXSizeThreeSym, STIXSizeThreeSym-Bold, STIXSizeThreeSym-Regular, STIXSizeThreeSym, STIXSizeThreeSym-Regular, STIXSizeTwoSym-Bold, STIXSizeTwoSym, STIXSizeTwoSym-Bold, STIXSizeTwoSym-Regular, STIXSizeTwoSym, STIXSizeTwoSym-Regular, STIXVariants-Bold, STIXVariants, STIXVariants-Bold, STIXVariants-Regular, STIXVariants, STIXVariants-Regular, STKaiti, STKaiti, STKaiti, STKaitiSC-Black, Kaiti SC, STKaitiSC-Black, STKaitiSC-Bold, Kaiti SC, STKaitiSC-Bold, STKaitiSC-Regular, Kaiti SC, STKaitiSC-Regular, STKaitiTC-Black, Kaiti TC, STKaitiTC-Black, STKaitiTC-Bold, Kaiti TC, STKaitiTC-Bold, STKaitiTC-Regular, Kaiti TC, STKaitiTC-Regular, STLibianSC-Regular, Libian SC, STLibianSC-Regular, STLibianTC-Regular, Libian TC, STLibianTC-Regular, STSong, STSong, STSong, STSongti-SC-Black, Songti SC, STSongti-SC-Black, STSongti-SC-Bold, Songti SC, STSongti-SC-Bold, STSongti-SC-Light, Songti SC, STSongti-SC-Light, STSongti-SC-Regular, Songti SC, STSongti-SC-Regular, STSongti-TC-Bold, Songti TC, STSongti-TC-Bold, STSongti-TC-Light, Songti TC, STSongti-TC-Light, STSongti-TC-Regular, Songti TC, STSongti-TC-Regular, STXihei, STHeiti, STXihei, STXingkaiSC-Bold, Xingkai SC, STXingkaiSC-Bold, STXingkaiSC-Light, Xingkai SC, STXingkaiSC-Light, STXingkaiTC-Bold, Xingkai TC, STXingkaiTC-Bold, STXingkaiTC-Light, Xingkai TC, STXingkaiTC-Light, STYuanti-SC-Bold, Yuanti SC, STYuanti-SC-Bold, STYuanti-SC-Light, Yuanti SC, STYuanti-SC-Light, STYuanti-SC-Regular, Yuanti SC, STYuanti-SC-Regular, STYuanti-TC-Bold, Yuanti TC, STYuanti-TC-Bold, STYuanti-TC-Light, Yuanti TC, STYuanti-TC-Light, STYuanti-TC-Regular, Yuanti TC, STYuanti-TC-Regular, Sana, Sana, Sana, Sathu, Sathu, Sathu, SavoyeLetPlain, Savoye LET, SavoyeLetPlain, Seravek, Seravek, Seravek, Seravek-Bold, Seravek, Seravek-Bold, Seravek-BoldItalic, Seravek, Seravek-BoldItalic, Seravek-ExtraLight, Seravek, Seravek-ExtraLight, Seravek-ExtraLightItalic, Seravek, Seravek-ExtraLightItalic, Seravek-Italic, Seravek, Seravek-Italic, Seravek-Light, Seravek, Seravek-Light, Seravek-LightItalic, Seravek, Seravek-LightItalic, Seravek-Medium, Seravek, Seravek-Medium, Seravek-MediumItalic, Seravek, Seravek-MediumItalic, ShreeDev0714, Shree Devanagari 714, ShreeDev0714, ShreeDev0714-Bold, Shree Devanagari 714, ShreeDev0714-Bold, ShreeDev0714-BoldItalic, Shree Devanagari 714, ShreeDev0714-BoldItalic, ShreeDev0714-Italic, Shree Devanagari 714, ShreeDev0714-Italic, SignPainter-HouseScript, SignPainter, SignPainter-HouseScript, SignPainter-HouseScriptSemibold, SignPainter, SignPainter-HouseScriptSemibold, Silom, Silom, Silom, SinhalaMN, Sinhala MN, SinhalaMN, SinhalaMN-Bold, Sinhala MN, SinhalaMN-Bold, SinhalaSangamMN, Sinhala Sangam MN, SinhalaSangamMN, SinhalaSangamMN-Bold, Sinhala Sangam MN, SinhalaSangamMN-Bold, Skia-Regular, Skia, Skia-Regular, Skia-Regular_Black, Skia, Skia-Regular_Black, Skia-Regular_Black-Condensed, Skia, Skia-Regular_Black-Condensed, Skia-Regular_Black-Extended, Skia, Skia-Regular_Black-Extended, Skia-Regular_Bold, Skia, Skia-Regular_Bold, Skia-Regular_Condensed, Skia, Skia-Regular_Condensed, Skia-Regular_Extended, Skia, Skia-Regular_Extended, Skia-Regular_Light, Skia, Skia-Regular_Light, Skia-Regular_Light-Condensed, Skia, Skia-Regular_Light-Condensed, Skia-Regular_Light-Extended, Skia, Skia-Regular_Light-Extended, SnellRoundhand, Snell Roundhand, SnellRoundhand, SnellRoundhand-Black, Snell Roundhand, SnellRoundhand-Black, SnellRoundhand-Bold, Snell Roundhand, SnellRoundhand-Bold, SukhumvitSet-Bold, Sukhumvit Set, SukhumvitSet-Bold, SukhumvitSet-Light, Sukhumvit Set, SukhumvitSet-Light, SukhumvitSet-Medium, Sukhumvit Set, SukhumvitSet-Medium, SukhumvitSet-SemiBold, Sukhumvit Set, SukhumvitSet-SemiBold, SukhumvitSet-Text, Sukhumvit Set, SukhumvitSet-Text, SukhumvitSet-Thin, Sukhumvit Set, SukhumvitSet-Thin, Superclarendon-Black, Superclarendon, Superclarendon-Black, Superclarendon-BlackItalic, Superclarendon, Superclarendon-BlackItalic, Superclarendon-Bold, Superclarendon, Superclarendon-Bold, Superclarendon-BoldItalic, Superclarendon, Superclarendon-BoldItalic, Superclarendon-Italic, Superclarendon, Superclarendon-Italic, Superclarendon-Light, Superclarendon, Superclarendon-Light, Superclarendon-LightItalic, Superclarendon, Superclarendon-LightItalic, Superclarendon-Regular, Superclarendon, Superclarendon-Regular, Symbol, Symbol, Symbol, Tahoma, Tahoma, Tahoma, Tahoma-Bold, Tahoma, Tahoma-Bold, TamilMN, Tamil MN, TamilMN, TamilMN-Bold, Tamil MN, TamilMN-Bold, TamilSangamMN, Tamil Sangam MN, TamilSangamMN, TamilSangamMN-Bold, Tamil Sangam MN, TamilSangamMN-Bold, TeluguMN, Telugu MN, TeluguMN, TeluguMN-Bold, Telugu MN, TeluguMN-Bold, TeluguSangamMN, Telugu Sangam MN, TeluguSangamMN, TeluguSangamMN-Bold, Telugu Sangam MN, TeluguSangamMN-Bold, Thonburi, Thonburi, Thonburi, Thonburi-Bold, Thonburi, Thonburi-Bold, Thonburi-Light, Thonburi, Thonburi-Light, Times-Bold, Times, Times-Bold, Times-BoldItalic, Times, Times-BoldItalic, Times-Italic, Times, Times-Italic, Times-Roman, Times, Times-Roman, TimesNewRomanPS-BoldItalicMT, Times New Roman, TimesNewRomanPS-BoldItalicMT, TimesNewRomanPS-BoldMT, Times New Roman, TimesNewRomanPS-BoldMT, TimesNewRomanPS-ItalicMT, Times New Roman, TimesNewRomanPS-ItalicMT, TimesNewRomanPSMT, Times New Roman, TimesNewRomanPSMT, ToppanBunkyuGothicPr6N-DB, Toppan Bunkyu Gothic, ToppanBunkyuGothicPr6N-DB, ToppanBunkyuGothicPr6N-Regular, Toppan Bunkyu Gothic, ToppanBunkyuGothicPr6N-Regular, ToppanBunkyuMidashiGothicStdN-ExtraBold, Toppan Bunkyu Midashi Gothic, ToppanBunkyuMidashiGothicStdN-ExtraBold, ToppanBunkyuMidashiMinchoStdN-ExtraBold, Toppan Bunkyu Midashi Mincho, ToppanBunkyuMidashiMinchoStdN-ExtraBold, ToppanBunkyuMinchoPr6N-Regular, Toppan Bunkyu Mincho, ToppanBunkyuMinchoPr6N-Regular, Trattatello, Trattatello, Trattatello, Trebuchet-BoldItalic, Trebuchet MS, Trebuchet-BoldItalic, TrebuchetMS, Trebuchet MS, TrebuchetMS, TrebuchetMS-Bold, Trebuchet MS, TrebuchetMS-Bold, TrebuchetMS-Italic, Trebuchet MS, TrebuchetMS-Italic, TsukuARdGothic-Bold, Tsukushi A Round Gothic, TsukuARdGothic-Bold, TsukuARdGothic-Regular, Tsukushi A Round Gothic, TsukuARdGothic-Regular, TsukuBRdGothic-Bold, Tsukushi B Round Gothic, TsukuBRdGothic-Bold, TsukuBRdGothic-Regular, Tsukushi B Round Gothic, TsukuBRdGothic-Regular, Verdana, Verdana, Verdana, Verdana-Bold, Verdana, Verdana-Bold, Verdana-BoldItalic, Verdana, Verdana-BoldItalic, Verdana-Italic, Verdana, Verdana-Italic, Waseem, Waseem, Waseem, WaseemLight, Waseem, WaseemLight, Webdings, Webdings, Webdings, WeibeiSC-Bold, Weibei SC, WeibeiSC-Bold, WeibeiTC-Bold, Weibei TC, WeibeiTC-Bold, Wingdings-Regular, Wingdings, Wingdings-Regular, Wingdings2, Wingdings 2, Wingdings2, Wingdings3, Wingdings 3, Wingdings3, YuGo-Bold, YuGothic, YuGo-Bold, YuGo-Medium, YuGothic, YuGo-Medium, YuKyo-Bold, YuKyokasho, YuKyo-Bold, YuKyo-Medium, YuKyokasho, YuKyo-Medium, YuKyo_Yoko-Bold, YuKyokasho Yoko, YuKyo_Yoko-Bold, YuKyo_Yoko-Medium, YuKyokasho Yoko, YuKyo_Yoko-Medium, YuMin-Demibold, YuMincho, YuMin-Demibold, YuMin-Extrabold, YuMincho, YuMin-Extrabold, YuMin-Medium, YuMincho, YuMin-Medium, YuMin_36pKn-Demibold, YuMincho +36p Kana, YuMin_36pKn-Demibold, YuMin_36pKn-Extrabold, YuMincho +36p Kana, YuMin_36pKn-Extrabold, YuMin_36pKn-Medium, YuMincho +36p Kana, YuMin_36pKn-Medium, YuppySC-Regular, Yuppy SC, YuppySC-Regular, YuppyTC-Regular, Yuppy TC, YuppyTC-Regular, ZapfDingbatsITC, Zapf Dingbats, ZapfDingbatsITC, Zapfino, Zapfino, Zapfino, ","date":1532390400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532390400,"objectID":"4e9f6a48715866d509d6942b461e4a00","permalink":"https://yumauchiumi.com/post/2018-07-24-java-get-all-fonts/","publishdate":"2018-07-24T00:00:00Z","relpermalink":"/post/2018-07-24-java-get-all-fonts/","section":"post","summary":"TL;DR Use getLocalGraphicsEnvironment().getAllFonts(); method defined in java.awt.GraphicsEnvironment class.\nCf. https://docs.oracle.com/javase/jp/8/docs/api/java/awt/GraphicsEnvironment.html Code Sample (Java):\nimport java.awt.Font; import java.awt.GraphicsEnvironment; public class Main { public static void main(String[] args) throws Exception { Font[] fonts = GraphicsEnvironment.","tags":["Programming","Java"],"title":"Display All Physical Fonts That Can Be Used As Strings in Java","type":"post"},{"authors":null,"categories":["StatML"],"content":"KL-divergence frequently appears in many fields such as statistics and information theory. It is defined as the expected value of logarithmic transformation of likelihood ratio. Note that:\nexpected value: weighted integration with probability density. logarithmic transformation: conversion multiplication to linear combination that is suitable for convex optimization and function analysis. likelihood ratio: a measure of likelihood comparison 1. What is KL-divergence? 1.1 Definition For any probability distributions $P$ and $Q$, KL-divergence (Kullback-Leibler divergence)1 is defined as follows, using their probability density function $p(x)$ and $q(x)$.\n\\begin{align} D_{KL}( Q \\mid\\mid P ) \u0026amp;:= \\int q(x) \\log \\frac{q(x)}{p(x)} ~dx \\end{align}\n1.2 Basic properties KL-divergence has the following properties.\nï¼ˆnon-negativeï¼‰It has a non-negative range. \\begin{align} 0 \\leq D_{KL}( Q \\mid\\mid P ) \u0026amp;\\leq \\infty \\end{align}\nï¼ˆcompletenessï¼‰When it equals to $0$, $P$ and $Q$ are equivalent. \\begin{align} D_{KL}( Q \\mid\\mid P ) \u0026amp;= 0 ~~ \\Leftrightarrow ~~ P = Q \\end{align}\nï¼ˆassymmetryï¼‰It is not symmetric about $P$ and $Q$. \\begin{align} D_{KL}( Q \\mid\\mid P ) \u0026amp;\\neq D_{KL}( P \\mid\\mid Q ) \\end{align}\nï¼ˆabsolute continuityï¼‰Unless it diverges, $Q$ is absolutely continuous with respect to $P$. \\begin{align} D_{KL}( Q \\mid\\mid P ) \u0026amp;\\lt \\infty ~~ \\Rightarrow ~~ P \\gg Q \\end{align}\nâ€‹\tFor example, calculating KL-divergence2 between two Gaussian distributions gives the following results: It can be seen that the more the shapes between two distributions do not match, the more KL-divergence increases.\n1.3 Is KL-divergence a metrics? â€‹\tKL-divergence is so important measurement when considering probability and information that it is called by various names depending on the field and context.\n\u0026ldquo;KL-divergence\u0026rdquo; \u0026ldquo;KL-metrics\u0026rdquo; \u0026ldquo;KL-information\u0026rdquo; \u0026ldquo;Information divergence\u0026rdquo; \u0026ldquo;Information gain\u0026rdquo; \u0026ldquo;Relative entropy\u0026rdquo; Since KL-divergence is always non-negative, it might be interpreted as the metrics in the space where the probability distributions $P$ and $Q$ exist. However, KL-divergence is not strictly a metric because it only satisfies \u0026ldquo;non-negativity\u0026rdquo; and \u0026ldquo;completeness\u0026rdquo; among the following axioms of metrics.\nAxioms of metrics $d(~)$:\nnon-negativity $d(x, ~ y) \\geq 0$\ncompleteness $d(x, ~ y) = 0 ~~ \\Leftrightarrow ~~ x = y$\nsymmetry $d(x, ~ y) = d(y, ~ x)$\nThe triangle inequality $d(x, ~ y) + d(y, ~ z) \\geq d(x, ~ z)$\nNote that $d()$ is called the distance function or simply distance\nFor example, Euclidean distance, squared distance, Mahalanobis distance, and Hamming distance satisfy these conditions, and can be clearly considered as metrics. On the other hand, KL-divergence is a divergence, not metrics. In mathematics, \u0026ldquo;divergence\u0026rdquo; is an extended concept of \u0026ldquo;metrics\u0026rdquo; that satisfies only non-negativity and completeness among axioms of metrics. By introducing \u0026ldquo;divergence\u0026rdquo;, you can reduce the constraints of axioms of metrics and have a high level of abstraction.\nThe word \u0026ldquo;divergence\u0026rdquo; is generally interpreted as the process or state of diverging; for example, in physics it appears as a vector operator div. There is no Japanese words that corresponds to the meaning of divergence, but it seems that \u0026ldquo;ç›¸é•åº¦\u0026rdquo;, \u0026ldquo;åˆ†é›¢åº¦\u0026rdquo;, \u0026ldquo;é€¸è„±åº¦\u0026rdquo;, \u0026ldquo;ä¹–é›¢åº¦\u0026rdquo; etc. might be used.\nAs an example, let\u0026rsquo;s measure the KL-divergence between two Gaussian distributions $ N (0, 1) $ (blue) and $ N (1, 2) $ (red). In the figure, the left shows KL-divergence from red one as seen from blue one, and the right shows KL-divergence from blue one as seen from red one. Their value are surely different.\nNote that given two Gaussian distribution $p_1,p_2$ as\n$$ \\begin{align} p_1(x) \u0026amp;= \\mathcal{N}(\\mu_1, \\sigma_1^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma_1^2}} \\exp \\left\\{ - \\frac{ {(x - \\mu_1)}^2}{2 \\sigma_1^2} \\right\\} \\\\ p_2(x) \u0026amp;= \\mathcal{N}(\\mu_2, \\sigma_2^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma_2^2}} \\exp \\left\\{ - \\frac{ {(x - \\mu_2)}^2}{2 \\sigma_2^2} \\right\\} \\end{align} $$\nthe following holds.\n$$ \\begin{align} {D}_{KL}(p_1 \\mid\\mid p_2) \u0026amp;= \\int_{-\\infty}^{\\infty} p_1(x) \\log \\frac{p_1(x)}{p_2(x)} dx \\\\ \u0026amp;= \\log \\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + {( \\mu_1 - \\mu_2 )}^2}{2 \\sigma_2^2} - \\frac{1}{2} \\end{align} $$\nIncidentally, in addition to the KL-divergence, the following is known as a measure of the proximity (or closeness) between two probability distributions.\nThe metrics to measure closeness between $q(x)$ and $p(x)$\n$ {\\chi}^2(q ; p) := \\sum_{i=1}^{k} \\frac{ { { p_i - q_i } }^{2} }{p_i}$ ($\\chi^2$-statistics) $ L_1(q ; p) := \\int \\vert q(x) - p(x) \\vert ~ dx$ ($L_1$-norm) $ L_2(q ; p) := \\int { { q(x) - p(x) } }^{2} ~ dx$ ($L_2$-norm) $ I_K(q ; p) := \\int { \\{ \\sqrt{ q(x) } - \\sqrt{ p(x) } \\} }^{2} ~ dx $ (Herringer distance) $ \\mathbb{D}(q ; p) := \\int f \\left( {\\large \\frac{q(x)}{p(x)} } \\right) q(x) ~ dx$ ($f$-divergence) $ I_{\\lambda}(q ; p) := \\int \\left\\{ { \\left( {\\large \\frac{q(x)}{p(x)} } \\right) }^{\\lambda} - 1 \\right\\} q(x) ~ dx$ (Generalized information) $ {D}_{KL}(q ; p) := \\int \\log \\left( {\\large \\frac{q(x)}{p(x)} } \\right) q(x) ~ dx$ (KL-divergence) $ JSD(q \\mid\\mid p) := \\frac{1}{2} {D}_{KL}(q \\mid\\mid \\frac{q+p}{2}) + \\frac{1}{2} {D}_{KL}(p \\mid\\mid \\frac{q+p}{2})$ (JS-divergence) 2. Relatinoship to other measurements 2.1 KL-divergence vs Mutual information In information theory, entropy $H(X)$, join entropy $H(X,Y)$, conditional entropy $H(X \\vert Y)$, mutual information $MI(X,Y)$ are defined as follows by using probability density $Pr()$3.\n\\begin{align} H(X) \u0026amp;:= - \\int Pr(x) \\log Pr(x) ~dx \\\\ H(X,Y) \u0026amp;:= - \\int Pr(x,y) \\log Pr(x,y) ~dy~dx \\\\ H(X|Y) \u0026amp;:= - \\int Pr(x,y) \\log Pr(x|y) ~dx~dy \\\\ MI(X,Y) \u0026amp;:= \\int \\int Pr(x,y) \\log \\frac{Pr(x,y)}{Pr(x)Pr(y)} ~dxdy \\end{align}\nFor any two random variable $X$ and $Y$, mutual information $MI(X, Y)$ specifies the mutual (symmetric) dependence between them.\n\\begin{align} MI(X,Y) \u0026amp;= H(X) - H(X|Y) \\\\ \u0026amp;= H(Y) - H(Y|X) \\\\ \u0026amp;= H(X) + H(Y) - H(X,Y) \\end{align}\nHere, the following relationship holds between KL-divergence and mutual information.\n\\begin{align} MI(X, Y) \u0026amp;= D_{KL} \\bigl( Pr(x, y) \\mid\\mid Pr(x)Pr(y) \\bigr) \\\\ \u0026amp;= \\mathbb{E}{Y} \\bigl[ D{KL} \\bigl( Pr(x|y) \\mid\\mid Pr(x) \\bigr) \\bigr] \\\\ \u0026amp;= \\mathbb{E}{X} \\bigl[ D{KL} \\bigl( Pr(y|x) \\mid\\mid Pr(y) \\bigr) \\bigr] \\end{align}\nSo that, mutual information $MI (X, Y)$ is interpreted as the degree of difference (average degree of deviation) between the joint distribution $Pr (x, y)$ when the $X$ and $Y$ are not independent and the joint distribution $Pr (x) Pr (y)$ when $X$ and $Y$ are independent.\nï¼ˆcf.ï¼‰Formula transformation of mutual information:\n\\begin{align} MI(X,Y) \u0026amp;= \\int \\int Pr(x,y) \\log \\frac{Pr(x,y)}{Pr(x)Pr(y)} ~dxdy \\\\ \u0026amp;= \\int \\int Pr(x|y)Pr(y) \\log \\frac{Pr(x|y)Pr(y)}{Pr(x)Pr(y)} ~dxdy \\\\ \u0026amp;= \\int Pr(y) \\int Pr(x|y) \\log \\frac{Pr(x|y)}{Pr(x)} ~dx~dy \\\\ \u0026amp;= \\int Pr(y) \\cdot D_{KL} \\bigl( Pr(x|y) \\mid\\mid Pr(x) \\bigr) ~dy \\\\ \u0026amp;= \\mathbb{E}{Y} \\bigl[ D{KL} \\bigl( Pr(x|y) \\mid\\mid Pr(x) \\bigr) \\bigr] \\end{align}\n2.2 KL-divergence vs Log likelihood ratio In the field of Bayes inference and statistical modeling, you often face the problem of estimating the true distribution $q(x)$ by $p_{\\hat{\\theta}}(x)$ (that is the combination of stochastic model $p_{\\theta}(x)$ and estimated parameter $\\hat{\\theta}$ ) . Therefore, KL-divergence is used when you want to measure the difference between two distributions, or when you want to incorporate the estimation error into the loss function or risk function in order to solve the optimization problem for the parameter $\\theta$.\nAlso, KL-divergence is related to the log likelihood ratio so much that it has a deep connection to the model selection method 4 such as likelihood ratio test, Bayes factor, and AIC (Akaike\u0026rsquo;s information criterion).\nKL-divergence of estimated distribution $p_{\\theta}(x)$ for the true distribution $q(x)$ : $D_{KL}(q \\mid\\mid p_{\\theta})$ is considerd as the expected value of the log likelihood ratio $q(x)/p_{\\theta}(x)$ for tue true distribution $q(x)$. \\begin{align} \\left( \\text{Log likelihood ratio} \\right) \u0026amp;= \\log \\frac{q(x)}{p_{\\theta}(x)} \\\\ D_{KL}( q \\mid\\mid p_{\\theta} ) \u0026amp;:= \\int q(x) \\log \\frac{q(x)}{p_{\\theta}(x)} ~dx \\\\ \u0026amp;= \\mathbb{E}{X} \\left[ \\log \\frac{q(x)}{p{\\theta}(x)} \\right] \\left(\\text{Expected log likelihood ratio} \\right) \\end{align}\nWhen using KL-divergence as the evaluation/loss value in model selection/comparison, it is equivalent that minimizing KL-divergence: $D_{KL}( q \\mid\\mid p )$ and maximizing the log likelihood: $\\log p(x)$ as follows.\n\\begin{align} D_{KL}( q \\mid\\mid p_{\\theta} ) \u0026amp;= \\mathbb{E}{X} \\bigl[ \\log q(x) \\bigr] - \\mathbb{E}{X} \\bigl[ \\log p_{\\theta}(x) \\bigr] \\\\ \u0026amp;\\propto - \\mathbb{E}{X} \\bigl[ \\log p{\\theta}(x) \\bigr] \\left(-1 \\cdot \\text{ Expected log likelihood} \\right) \\end{align}\nFor any parametric stochastic model $f(x \\vert \\theta)$ (such as a linear regression model) which represents the estimated distribution as\n\\begin{align} p_{\\theta}(x) = f(x|\\theta) \\end{align}\n, if a certain loss function $L(\\theta)$ is given, the optimal parameter $\\theta^*$ exists as it satisfy the following.\n\\begin{align} q(x) \u0026amp;= f(x|\\theta^*) \\end{align}\nThen, for any estimated parameter $\\hat{\\theta}$ ,the estimated loss of the model $f(x \\vert \\hat{\\theta})$ is represented by KL-divergence. (Note that $\\ell( \\cdot \\vert x)$ means the log likelihood function.)\n\\begin{align} \\left( \\text{Log likelihood ratio} \\right) \u0026amp;= \\log \\frac{f(x|\\theta^{*})}{f(x|\\hat{\\theta})} \\end{align}\n\\begin{align} \\hat{\\theta} \u0026amp;:= \\underset{\\theta \\in \\Theta}{\\rm argmin} ~ L(\\theta) \\tag{7} \\\\ D_{KL}( q \\mid\\mid p_{\\hat{\\theta}} ) \u0026amp;= D_{KL}( p_{\\theta^{}} \\mid\\mid p_{\\hat{\\theta}} ) \\\\ \u0026amp;= D_{KL}( f_{\\theta^{}} \\mid\\mid f_{\\hat{\\theta}} ) \\\\ \u0026amp;= \\int f(x|\\theta^{}) \\log \\frac{f(x|\\theta^{})}{ f(x|\\hat{\\theta})} dx \\\\ \u0026amp;= \\mathbb{E}{X} \\left[ \\log \\frac{ f(x|\\theta^{*}) }{ f(x|\\hat{\\theta}) } \\right] \\\\ \u0026amp;= \\mathbb{E}{X} \\bigl[ \\ell( \\theta_{0}|x ) \\bigr] - \\mathbb{E}_{X} \\bigl[ \\ell( \\hat{\\theta} | x ) \\bigr] \\end{align}\n2.3 KL-divergence vs Fisher information Given a certain stochastic model $f(\\cdot \\vert \\theta)$, Fisher information $I(\\theta)$ for the parameter $\\theta$ is defined as follows. (Note that $ \\ell( \\cdot \\vert x) $ means the log likelihood function.)\n\\begin{align} I(\\theta) \u0026amp;:= \\mathbb{E}_{X} \\left[ { \\left\\{ \\frac{d}{dx} \\ell(\\theta \\vert x) \\right\\} }^{3} \\right] \\\\ \u0026amp;= \\mathbb{E}_{X} \\left[ { \\left\\{ \\frac{d}{dx} \\log f(x|\\theta) \\right\\} }^{2} \\right] \\end{align}\nAlso, between KL-divergence and Fisher information, the following holds.\n\\begin{align} \\lim_{h \\to 0} \\frac{1}{h^{2}} D_{KL} \\bigl( f(x|\\theta) \\mid\\mid f(x|\\theta+h) \\bigr) \u0026amp;= \\frac{1}{2} I(\\theta)\n\\end{align}\n(cf.) The following equation holds by using Taylor expansion of $\\ell( \\cdot \\vert x)$.\n\\begin{align} \\ell(\\theta + h) - \\ell(\\theta) \u0026amp;= {\\ell}^{\u0026rsquo;}(\\theta)h + \\frac{1}{2} {\\ell}^{\u0026rsquo;\u0026rsquo;}(\\theta) h^{2} + O(h^{3}) \\end{align}\nThis formula indicates that in parameter space $\\Theta$, for all point $ \\theta \\in \\Theta $ ant its neighborring point $ \\theta + h $, their KL-divergenceï¼š$ D_{KL} ( f(x \\vert \\theta) \\mid\\mid f(x \\vert \\theta+h) )$ is directly proportional to Fisher information $I(\\theta)$. After all, Fisher information $ I(\\theta)$ measures the local information that the stochastic model $f(\\cdot \\vert \\theta)$ has at the point $\\theta$.\n3. References Also, f-divergence is defined as its generalized class.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI used scipy.stats.entropy().\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAlthough thermodynamic entropy is originated in Boltzmann, the historical background of Shannon information is mentioned below link. There seems to be a reference flow: Hartley â†’ Nyquist â†’ Shannon. http://www.ieice.org/jpn/books/kaishikiji/200112/200112-9.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nArticle on gneralized information criterion(GIC): https://www.ism.ac.jp/editsec/toukei/pdf/47-2-375.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":1524096000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524096000,"objectID":"27a281007a11aade3bc5bd85329a41fd","permalink":"https://yumauchiumi.com/post/2018-04-19-kl-divergence/","publishdate":"2018-04-19T00:00:00Z","relpermalink":"/post/2018-04-19-kl-divergence/","section":"post","summary":"This is a basic notebook for KL-divergence which frequently appears in many fields such as statistics and information theory.","tags":["Bayes"],"title":"Kullback-Leibler Divergence","type":"post"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://yumauchiumi.com/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"}]