<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Yuma Uchiumi</title>
    <link>https://yumauchiumi.com/project/</link>
      <atom:link href="https://yumauchiumi.com/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© Yuma Uchiumi 2018-2024</copyright><lastBuildDate>Sat, 27 Feb 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://yumauchiumi.com/media/static/media/icon.png</url>
      <title>Projects</title>
      <link>https://yumauchiumi.com/project/</link>
    </image>
    
    <item>
      <title>Duck-Rabbit Illusion</title>
      <link>https://yumauchiumi.com/project/duck-rabbit-illusion/</link>
      <pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://yumauchiumi.com/project/duck-rabbit-illusion/</guid>
      <description>&lt;img src=&#34;duck-rabbit.png&#34; alt=&#34;drawing&#34; style=&#34;width:450px;&#34;/&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
&lt;i&gt;Which does this look like a duck or a rabbit?&lt;/i&gt;
&lt;/div&gt;
&lt;h5 id=&#34;overview&#34;&gt;Overview&lt;/h5&gt;
&lt;p&gt;In order to elucidate human perceptual functions, it is necessary to consider both bottom-up information processing, in which stimulus information received from the sensory organs is encoded into symbolic information, and top-down information processing, which is objective-oriented and based on memory, beliefs, and context. In this paper, we take the ResNet50 image classification problem as an example task, and conduct a basic study on the information processing when humans make judgments about visual information with ambiguities, and discuss the computation by which working memory during task execution penetrates the discrimination results of the model in a top-down manner.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-vim&#34;&gt;Yuma Uchiumi, Yosuke Fukuchi, Mitsuhiko Kimoto, Michita Imai, 
“A Top-down Penetration to the Visual Attention via Elimination of the Ambiguity”, 
The 35th Annual Conference of the Japanese Society for Artificial Intelligence (JSAI 2021), 
Jun. 8-11, 2021, Virtual Conference.
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;h3 id=&#34;measurement-of-the-ambiguity&#34;&gt;Measurement of the Ambiguity&lt;/h3&gt;
&lt;p&gt;When a single stochastic model $f:X \to Y$ get a data sample $\boldsymbol{x}^{*}$ of the input random variable $X$,
the ambiguity of the model&amp;rsquo;s inference result can be defined as the conditional entropy of the output random variable $Y$.&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
A(Y; f, \boldsymbol{x}^{*})
:= H^{(f)}(Y | X=\boldsymbol{x}^{*} )
= -\sum_{\boldsymbol{y}} f(\boldsymbol{y} | \boldsymbol{x}^{*}) \log f(\boldsymbol{y} | \boldsymbol{x}^{*})
\end{align}
$$&lt;/p&gt;
&lt;p&gt;Next, if the model has the prior belief for the output variable $Y$,
the surprise of the model&amp;rsquo;s inference result can be defined as the Kullback–Leibler divergence between
the prior distribution $p(\boldsymbol{y})$ and the predicted distribution $f(\boldsymbol{y} | \boldsymbol{x}^{*})$.&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
S(Y; f, \boldsymbol{x}^{*}, p)
:= D_{KL}( p || f )
= -\sum_{\boldsymbol{y}} p(\boldsymbol{y}) \log \frac{ p(\boldsymbol{y}) }{ f(\boldsymbol{y} | \boldsymbol{x}^{*}) }
\end{align}
$$&lt;/p&gt;
&lt;p&gt;So that, the optimization process of $f$ is obtained as&lt;/p&gt;
&lt;p&gt;$$
\underset{f}{\rm minimize} ~~
A(Y; f, \boldsymbol{x}^{*}) + \beta \cdot S(Y; f, \boldsymbol{x}^{*}, p)
$$&lt;/p&gt;
&lt;p&gt;where $\beta$ is a weighting factor in $[0,1]$. The overall computation is shown below.&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;training-the-model--deriving-the-ambiguity&#34;&gt;Training the Model &amp;amp; Deriving the Ambiguity&lt;/h3&gt;
&lt;p&gt;First of all, we trained the ResNet-50 as the image classification model $f:$&lt;/p&gt;
&lt;p&gt;$$
X \in \mathbb{R}^{H \times W \times C} \to Y \in \{\text{duck}, \text{rabbit}, \text{others}\}
$$&lt;/p&gt;
&lt;p&gt;on the Google Open Image Dataset.
Then,
when the model makes inferences sequentially for a given image
the ambiguity and the surprise in the model is calculated.
The overall computation is shown below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;fig2_architecture.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the experiment, initially 30 images were sampled from the prior categorical distribution of the class label $Y$ and then its posterior distribution $p(y)$ (belief) was formed. Afterwards, the model made inferences for the Duck-Rabbit illusion image $\boldsymbol{x}^{*}$.&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;plasticity-of-the-visual-attention&#34;&gt;Plasticity of the Visual Attention&lt;/h3&gt;
&lt;p&gt;Futhermore, we applied the elimination process of the ambiguity to the model.
Target parameters of the convolutional kernels &lt;code&gt;conv1&lt;/code&gt; in ResNet-50 was updated according to the defined metrics of the ambiguity and surprise.&lt;/p&gt;
&lt;p&gt;The attention maps that the model had for the input image during inference are shown as the following figures. As the belief distribution makes a difference between them,
the features that strongly affect the inference result are different
in the duck-rabbit illusion image.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;fig1_attention.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;As the result, the proposed computation explains the phenomenon in which the inference process of the model itself is penetrated by higher cognitive elements such as the belief in a top-down manner.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Project</title>
      <link>https://yumauchiumi.com/project/example/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://yumauchiumi.com/project/example/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
